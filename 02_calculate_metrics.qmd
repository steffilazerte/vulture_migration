---
title: "Calculate Metrics"
fig-width: 8
fig-asp: 0.7
---

## Background

Having explored the data ([Initial Exploration](01_initial_exploration.html)) and 
various ways of calculating metrics of migration timing, we will now calculate and
explore these metrics for the entire data set.

> ### We will use
> 
> - a [GAM approach](01_initial_exploration.html#gam)
> to model the pattern of vulture counts
> - percentiles based on cumulative modelled counts to assess dates of passage
> - [Option 3](01_initial_exploration.html#option-3) to account for
>   resident birds (subtract the predicted mean residents prior to calculating the cumulative counts)

## Load Data
```{r message = FALSE}
source("XX_functions.R")  # Custom functions and packages

set.seed(1234) # To make this reproducible

v <- read_csv("Data/Datasets/vultures_clean_2023.csv")
resident_date <- 240
```

## Metrics to assess

To answer these [questions](01_initial_exploration.html#questions) we will summarize the counts into specific metrics
representing the timing of migration.

Specifically, we would like to calculate the

- dates of 5%, 25%, 50%, 75%, and 95% of the kettle numbers
- duration of passage  - No. days between 5% and 95%
- duration of peak passage - No. days between 25% and 75%

Population size (no. vultures in aggregations)

- maximum
- cumulative
- number at peak passage (mean, median, range)
- number of locals (mean, median, range)

Of these, the most important starting metrics are the 
**dates of 5%, 25%, 50%, 75%, and 95% of the kettle numbers**. 
These dates will define migration phenology as well as local vs. migrating counts.
All other calculations can be performed using these values and the raw data.

## Proceedure

The steps for calculating these metrics are as follows. 

For each year we will calculate...

1. A GAM
2. The median number of residents, using day `r resident_date` as a cutoff
3. The cumulative migration counts
4. The dates of passage as percentiles of these cumulative counts (5%, 25%, 75%, 95%)
5. The duration of (peak) passage from these dates
6. The population size (max, cumulative, stats at peak passage, stats of locals)

We will also create figures outlining these metrics for each year and will
use these to assess whether anything needs to be tweaked 
(i.e. perhaps the date `r resident_date` cutoff)

## Calculate Metrics

### 0. Sample sizes

:::{.panel-tabset}

#### Calculate
```{r samples}
samples <- v |>
  group_by(year) |>
  filter(!is.na(count)) |> # Omit missing dates
  summarize(
    date_min = min(date), date_max = max(date),
    # number of dates with a count
    n_dates_obs = n(),           
    # number of dates in the range
    n_dates = as.numeric(difftime(date_max, date_min, units = "days")), 
    n_obs = sum(count))
```

#### Preview
```{r}
gt(samples)
```

:::

### 1. GAMs

As developed in our [Initial Exploration](01_initial_exploration.html#gam)
we will use:

- Negative binomial model to fit count data with overdispersion
- Use Restricted Maximum Likelihood ("Most likely to give you reliable, stable results"[^1])
- A smoother (`s()`) over `doy` (day of year) to account for non-linear migration patterns
- `k = 10` (up to 10 basis functions; we want enough to make sure we capture the patterns, but too many will slow things down).

[^1]: https://noamross.github.io/gams-in-r-course/chapter1


:::{.panel-tabset}

#### Models

**Run GAM on each year (except 2007)**
```{r gams}
#| cache: true
gams <- v |>
  mutate(count = as.integer(count)) |>
  filter(year != 2007) |> # Can't model 2007 because no data
  nest(counts = -year) |>
  mutate(models = map(counts, \(x) gam(count ~ s(doy, k = 20), data = x, 
                                      method = "REML", family = "nb")))
```

**Create model predictions**
```{r}
gams <- gams |>
  mutate(
    doy = map(counts, \(x) list(doy = min(x$doy):max(x$doy))),
    pred = map2(
      models, doy, 
      \(x, y) predict(x, newdata = y, type = "response", se.fit = TRUE)),
    pred = map2(
      pred, doy,
      \(x, y) data.frame(doy = y, count = x$fit, se = x$se) |>
        mutate(ci99_upper = count + se * 2.58,
               ci99_lower = count - se * 2.58)))

pred <- gams |>
  select(year, pred) |>
  unnest(pred)
```


#### Model Evaluation
Checks to ensure models are valid.

Here we look for two things

- first that there is full convergence
- second that there is not a significant non-random pattern in the residuals 
  around the smoothing term (p-value, but be aware this is an *approximation*[^2]) 

If we have low p-values, we want to check and see

- if the model doesn't look like it fits the data (see the model plots at the end of this script)
- if the `k` (number of basis functions) and `edf` (effective degrees of freedom)
  values are similar (if they are, this implies that we haven't picked a large enough `k`)

[^2]: https://noamross.github.io/gams-in-r-course/chapter2

```{r message = FALSE}
#| code-fold: true
#| cache: true
#| cache-globals: gams

checks <- gams |>
  mutate(checks = map2(models, year, gam_check)) |>
  invisible() |>
  mutate(plots = map(checks, \(x) pluck(x, "plot")),
         df = map(checks, \(x) pluck(x, "checks"))) |>
  unnest(df) |>
  mutate(low_k = p_value < 0.1)

c <- checks |>
  filter(low_k | !full_convergence) |>
  select(year, param, k, edf, k_index, p_value, convergence)

gt(c)
```


#### Model Check Plots

These plots are two different ways of presenting model diagnostics. 

`gam.check()` is the default check that produces both these plots as well as
the diagnostics in the Model Evaluation tab.

[DHARMa](https://cran.r-project.org/web/packages/DHARMa/vignettes/DHARMa.html)
is a package for simulating residuals to allow model checking for all types 
of models ([details](https://steffilazerte.ca/posts/dharma/index.html)).

Both these sets of plots can be interpreted similarly to general linear model
plots. We want roughly normal residuals and constant variance.

I tend to put more weight on DHARMa as it's plots are easier to interpret for
non-Gaussian model residuals, but I have included the `gam.check()` plots for completeness.

:::{.panel-tabset}

##### DHARMa

```{r}
#| echo: false
#| results: asis
#| message: false
#| fig-height: 12
#| fig-asp: 0.5
#| cache: true
#| cache-globals: checks
for(i in seq_len(nrow(checks))) {
  cat("**Year: ", checks$year[i], "**\n\n")
  cat("DHARMa's `simulateResiduals()` plot\n\n")
  p0 <- par(mar = c(2,2,2,2))
  DHARMa::simulateResiduals(checks$models[[i]], plot = TRUE)
  par(p0)
  cat("\n\n")
}
```


##### `gam.check()`

```{r}
#| echo: false
#| results: asis
#| message: false
#| fig-height: 12
#| fig-asp: 0.5
#| cache: true
#| cache-globals: checks
for(i in seq_len(nrow(checks))) {
  cat("**Year: ", checks$year[i], "**\n\n")
  cat("`gam.check()` plot\n\n")
  cat("![](", checks$plots[[i]], ")", sep = "")
  cat("\n\n")
}
```
:::

:::

#### Model Validity

Based on the **Model Evaluation**, there are years with lower k-indices
and a p-value < 0.1. It may be worth double checking that these models
don't look too unreasonable. 

On the whole, there seems to be quite a 
bit of variability, but nothing that seems especially problematic.

```{r}
#| code-fold: true
#| fig-width: 12
#| fig-align: center
g <- lapply(c$year, \(y) plot_model(v[v$year == y, ], pred[pred$year == y, ]) +
              labs(title = y))
wrap_plots(g)
```


Based on the **Model Check Plots**, particularly the ones with **Simulated DHARMa residuals**, 
we have good model fit (QQ plots of residuals) throughout, but a couple examples
of potential non-constant variance. However, I am 
not very concerned about this for several reasons.

1. Although DHARMa highlighted these plots as having significant quantile deviations, 
   visually, I don't find the deviations that concerning.
2. Heteroscedasticity can lead to issues with our Standard Errors, but since
   we're only really interested in the predicted value (based on the estimates),
   these problems don't really apply to us (we're not interpreting model error or significance of parameters).


> Therefore I suggest we proceed with these models and extract the metrics 
> we're interested in.

### 2. Residents

Using a resident date (`resident_date`) cutoff of DOY `r resident_date`. 
Here we calculate the min, max, mean, and median number of residents throughout
the 'resident period' (from the start to DOY `r resident_date`).

**Note:** Resident counts are fractional because they are based on the predicted
counts from the GAM models (i.e. the smoothed curve).


:::{.panel-tabset}

#### Calculate
```{r residents}
residents <- pred |>
  filter(doy < resident_date) |>
  group_by(year) |>
  # Calculate resident statistics by year
  summarize(res_pop_min = min(count), 
            res_pop_max = max(count), 
            res_pop_median = median(count), 
            res_pop_mean = mean(count)) |>
  # Round to 1 decimal place
  mutate(across(starts_with("res"), \(x) round(x, 1)))
```

#### Preview
```{r}
gt(residents)
```

:::


### 3. Cumulative counts

Here we calculate cumulative counts **after subtracting the median resident population**.

As noted in our [initial exploration](01_initial_exploration.html#option-3), 
if we calculate cumulative accounts across the whole year, we include cumulative
counts of residents which could bias the start of migration to an earlier date.

Therefore, we subtract the median number of resident birds from each daily count
and *then* calculate the cumulative number of birds seen
in kettles and use this cumulative curve to calculate our metrics of migration 
(calculated in the following sections).

Because we are subtracting a *median* value from the whole data range within a year
we can occasionally get some funny counts (i.e. if the number of resident vultures
fluctuated between 2 and 4, this would mean that subtracting a median of 3 would
sometimes result in a negative count which in turn would mean that the cumulative
count would sometimes go down, rather than up).

**However**, after reviewing the cumulative count figures for all years,
I've added one more rule: **we only start cumulative counts after DOY of `r resident_date`**.
This is still very much before the start of migration, but
avoids cumulative counts during the very clearly resident period. It helps avoid
some of the more extreme negative cumulative counts. 
Doing this doesn't appreciably alter the metrics calculated, but seems more reasonable.
So gives me more peace of mind.

**Overall:** I'm not concerned about minor negative cumulative count blips because 
a) they are minor, and 
b) they only occur during the resident phase. As soon as the birds start
accumulating for migration, the number of birds present is above the resident
number fluctuations and the cumulative count accumulates.

**Also Note:** Counts are fractional because they are based on the predicted
counts from the GAM models (i.e. the smoothed curve).

:::{.panel-tabset}

#### Calculate
```{r cum_counts}
cum_counts <- pred |>
  left_join(select(residents, year, res_pop_median), by = "year") |>
  group_by(year) |>
  mutate(count_init = count,
         count = count - res_pop_median, # Subtract residents from predicted counts
         count = if_else(doy < resident_date, 0, count),
         count_sum = cumsum(count)) |> # Calculate cumulative sum
  ungroup() |>
  select(year, doy, res_pop_median, count_init, count, count_sum)
```

#### Preview
Showing only a snapshot of the middle of the year 1999.

- `count_init` is the predicted daily kettle count (only included here for illustration)
- `count` is the predicted daily kettle count after residents are removed (this means that negative counts are possible before migration starts).
- `count_sum` is the *cumulative* sum of the `count` column (this means that it can decrease if counts are negative before migration starts).

**Note:** the first couple of `count`s are zero because they take place before
the resident date cutoff (`r resident_date`), and have been set to zero so we
start cumulative counts on `r resident_date`.

**The median number of residents for 1999 is `r cum_counts$res_pop_median[1]`**

```{r}
gt(slice(cum_counts, 35:60))
```

#### Visual

These figures show the early part of the migration season with **predicted daily counts (red)**
overlaid with the **adjusted cumulative predicted counts (grey)**.

- Red bars are predicted non-cumulative initial counts (no subtractions made)
- Grey bars are cumulative predicted counts created after subtracting resident birds from 
each daily count. Accumulating only after `r resident_date`.
- Note that there is a little negative cumulative blip in 2013 that evens out
relatively quickly.


```{r}
#| fig-width: 12
#| fig-height: 8

ggplot(data = filter(cum_counts, doy < 265), aes(x = doy, y = count_sum)) +
  theme_bw() +
  geom_bar(aes(y = count_init), stat = "identity", fill = "red", alpha = 0.8) +
  geom_bar(stat = "identity", alpha = 0.5) +
  #geom_bar(aes(y = count), stat = "identity", fill = "blue", alpha = 0.5) +
  facet_wrap(~ year, scales = "free_y")
```
:::


### 4. Dates of passage

Now that we have the cumulative predicted counts, we can calculate the dates at which
a particular proportion of the migrating population had passed through.

Here we look at the dates at which 5%, 25%, 50%, 75%, and 95% of the birds have
passed through. 

We also calculate the date at which the predicted count was at it's maximum.

The 5-95% and 25-75% date rages will then be used to calculate duration of passage,  population sizes, and migration patterns in the next sections.

:::{.panel-tabset}

#### Calculate
```{r passage_dates}
max_passage <- pred |>
  group_by(year) |> 
  # Keep first max count
  slice_max(count, with_ties = FALSE) |>
  mutate(measure = "max") |>
  select(year, measure, doy_passage = doy)

dts <- cum_counts |>
  select(year, doy, count, count_sum) |>
  group_by(year) |>
  calc_dates() |>
  rename("measure" = "perc") |>
  bind_rows(max_passage) |>
  arrange(year, measure)
```

#### Preview
Only showing part of the data.
```{r}
gt(slice(dts, 1:30))
```

:::


### 5. Duration of passage

Duration of passage is calculated as the number of days between the 5% and 95% (migration)
and the 25% and 75% (peak migration) dates.

We also organize the dates of passage into separate columns in this step.

:::{.panel-tabset}

#### Calculate
```{r passage}
passage <- dts |>
  group_by(year) |>
  summarize(mig_start_doy = doy_passage[measure == "p05"],
            mig_end_doy = doy_passage[measure == "p95"],
            peak_start_doy = doy_passage[measure == "p25"],
            peak_end_doy = doy_passage[measure == "p75"],
            p50_doy = doy_passage[measure == "p50"],
            max_doy = doy_passage[measure == "max"],
            mig_dur_days = mig_end_doy - mig_start_doy,
            peak_dur_days = peak_end_doy - peak_start_doy)
            
```

#### Preview
```{r}
gt(passage)
```
:::


### 6. Population size

Here we calculate population size metrics for different stages of the migration

- Residents (before resident date cutoff [DOY `r resident_date`])
- Migrants (between migration start [5%] and migration end [95%])
- Ambiguous (between resident date cutoff and migration start as well as after 
migration end)
- Peak migrants (between peak start [25%] and peak end [75%])
- Raw counts for migrants and peak migrants (min, mean, median, max, total)
  - These are the counts recorded by observers in the field
  - Remember that total is affected by missing dates

**Note:** I don't expect to use the ambiguous category, but have included it for
completeness and sanity checks as needed.

Because Migrants and Peak Migrants actually overlap (i.e a peak migrant is also a migrant).
They are calculated separately and then joined back in together.

We also calculate 'raw' counts, although we should be careful about interpreting
the 'totals' as those will depend quite a bit on how many days of observation
there were. I include total raw counts only for interest or for statements
along the lines of "Observers counted over X individual vultures over 26 years and
X days". Not for actual analysis.

We calculate min/max/mean/median/total, but do not expect to analyse all metrics. 
These are good for including in tables of descriptive stats and sanity checks.


:::{.panel-tabset}

#### Calculate
```{r pop_size}
# General stats on predicted counts of migrants, resident, and other
pop_size <- pred |>
  left_join(passage, by = "year") |>
  mutate(state = case_when(doy < resident_date ~ "res",
                           doy >= mig_start_doy & doy <= mig_end_doy ~ "mig",
                           TRUE ~ "ambig")) |>
  # Calculate migrant statistics by year and state
  group_by(year, state) |>
  summarize(pop_min = min(count), 
            pop_max = max(count), 
            pop_median = median(count), 
            pop_mean = mean(count),
            pop_total = sum(count), 
            .groups = "drop") |>
  pivot_longer(-c(year, state), names_to = "stat", values_to = "value") |>
  # Omit stats we don't care about
  filter(!(state %in% c("ambig", "res") & stat == "pop_total")) |>
  pivot_wider(names_from = c("state", "stat")) |>
  relocate(contains("ambig"), .after = last_col())

# General stats on raw migrant counts
raw_size <- v |>
  filter(year != "2007") |>
  left_join(passage, by = "year") |>
  mutate(state = case_when(doy < resident_date ~ "res",
                           doy >= mig_start_doy & doy <= mig_end_doy ~ "mig",
                           TRUE ~ "ambig")) |>
  group_by(year, state) |>
  summarize(raw_min = min(count, na.rm = TRUE), 
            raw_max = max(count, na.rm = TRUE), 
            raw_median = median(count, na.rm = TRUE), 
            raw_mean = mean(count, na.rm = TRUE), 
            raw_total = sum(count, na.rm = TRUE),
            .groups = "drop") |>
  pivot_longer(-c(year, state), names_to = "stat", values_to = "value") |>
  # Omit stats we don't care about
  filter(!(state %in% c("ambig", "res") & stat == "raw_total")) |>
  pivot_wider(names_from = c("state", "stat")) |>
  relocate(contains("ambig"), .after = last_col())

# General stats on peak-migration counts
peak_size <- pred |>
  left_join(passage, by = "year") |>
  filter(doy >= peak_start_doy & doy <= peak_end_doy) |>
  group_by(year) |>
  summarize(peak_pop_min = min(count), 
            peak_pop_max = max(count), 
            peak_pop_median = median(count), 
            peak_pop_mean = mean(count),
            peak_pop_total = sum(count), 
            .groups = "drop")

peak_raw_size <- v |>
  filter(year != "2007") |>
  left_join(passage, by = "year") |>
  mutate(abs_raw_max = max(count, na.rm = TRUE), .by = "year") |>
  filter(doy >= peak_start_doy & doy <= peak_end_doy) |>
  group_by(year, abs_raw_max) |>
  summarize(peak_raw_min = min(count, na.rm = TRUE), 
            peak_raw_max = max(count, na.rm = TRUE), 
            peak_raw_median = median(count, na.rm = TRUE), 
            peak_raw_mean = mean(count, na.rm = TRUE), 
            peak_raw_total = sum(count, na.rm = TRUE),
            .groups = "drop")

pop_size <- left_join(peak_size, pop_size, by = "year") |>
  left_join(raw_size, by = "year") |>
  left_join(peak_raw_size, by = "year") |>
  # Round to 1 decimal place
  mutate(across(matches("pop|raw"), \(x) round(x, 1)))

# Verify that max predicted populations are the same for peak and mig 
# Verify that max raw populations are the same for overall and mig (otherwise
#  we have missed the peak migration period...)
#
# Then remove the duplication - Keep one max count for predicted, one for raw

pop_size <- pop_size |>
  verify(mig_pop_max == peak_pop_max) |>
  verify(mig_raw_max == abs_raw_max) |>
  select(-peak_pop_max, -abs_raw_max, -peak_raw_max)
```

#### Preview
```{r}
gt(pop_size)
```

:::

### 7. Skewness & Kurtosis

To capture the pattern of migration, we calculate skewness (of particular interest)
and kurtosis (not necessarily of interest, but included for completeness). 

Note: We substract 3 from kurtosis to make it a measure of *Excess Kurtosis* 
which is centred on zero.
A normal distribution has a kurtosis 3, but an excess kurtosis of 0.

:::{.callout-tip}
Why the `skew_all`?

Skewness and kurtosis will differ depending on the range of data we include. 

If for example, we looked at the entire pattern of migration (including the 
very long tail of pre-migration data), we'd probably calculate that the distribution
was very left-skewed and the skew would be negative. 

So we need to look just at the migration period itself.

But if we use the 5-95% dates, which are good cutoffs, we might end up influencing
kurtosis which is affected by how thick or thin the tails are. Any truncating
will make those tails seem thicker.

So what do we do?

I suggest an additional period to look at. In this period, we will choose the 
date of the 50% of passage (i.e. when 50% of the birds have passed, `p50_doy`) as the
center of the distribution. Then we'll count symmetric dates out to either side
of that to 'cut' out the migration pattern. Because we're truncated on the fall
side, we'll count the number of days from `p50_doy` to the end (`p50_to_end`), and for each
year will include the dates between `p50_doy - p50_to_end` and the end of the 
date range (which is also `p50_doy + p50_to_end`).

This snap shot should give us a good look at skew, for sure, and possibly a
look at kurtosis. Although if we find negative kurtosis, it might be that the
date range isn't long enough. 
:::


```{r}
skew_all <- dts |>
  filter(measure %in% "p50") |>
  select(year, measure, doy_passage) |>
  pivot_wider(names_from = "measure", values_from = "doy_passage") |>
  left_join(x = pred, y = _, by = "year") |>
  mutate(p50_to_end = n_distinct(doy[doy >= p50]), .by = "year") |>
  filter(doy >= p50 - p50_to_end) |>
  summarize(doy = list(rep(doy, round(count))), .by = c("year", "p50_to_end")) |>
  mutate(all_skew = map_dbl(doy, skewness),
         all_kurt = map_dbl(doy, \(x) kurtosis(x) - 3)) |>
  select(-doy)


skew_mig <- dts |>
  filter(measure %in% c("p05", "p95")) |>
  select(year, measure, doy_passage) |>
  pivot_wider(names_from = "measure", values_from = "doy_passage") |>
  left_join(x = pred, y = _, by = "year") |>
  filter(doy >= p05 & doy <= p95) |>
  summarize(doy = list(rep(doy, round(count))), .by = "year") |>
  mutate(mig_skew = map_dbl(doy, skewness),
         mig_kurt = map_dbl(doy, \(x) kurtosis(x) - 3)) |>
  select(-doy)

skew_peak <- dts |>
  filter(measure %in% c("p25", "p75")) |>
  select(year, measure, doy_passage) |>
  pivot_wider(names_from = "measure", values_from = "doy_passage") |>
  left_join(x = pred, y = _, by = "year") |>
  filter(doy >= p25 & doy <= p75) |>
  summarize(doy = list(rep(doy, round(count))), .by = "year") |>
  mutate(peak_skew = map_dbl(doy, skewness),
         peak_kurt = map_dbl(doy, \(x) kurtosis(x) - 3)) |>
  select(-doy)
```


### Combine metrics

Join together the sample sizes, the passage dates/durations as well as the 
population sizes, and migration patterns (skew/kurtosis).


:::{.panel-tabset}

#### Calculate
```{r final}
final <- left_join(samples, passage, by = "year") |>
  left_join(pop_size, by = "year") |>
  left_join(skew_mig, by = "year") |>
  left_join(skew_peak, by = "year") |>
  left_join(skew_all, by = "year")
```

#### Preview

```{r}
gt(final)
```
:::


## Extra - Consider gaps

In the figures below, we can see that there is sometimes a significant gap in 
observation right before the 'start' of migration.

Although migration start is calculated from the GAM, it may be worth considering
any patterns in these gaps incase they influence the start of migration (i.e.
if there are more missing dates, is the start later?).

Therefore, let's calculate how many missing dates there are in the two weeks before
the predicted start of migration. We should be able to compare models with and without this
to see if it has an effect on our analysis.

:::{.panel-tabset}

### Calculate
```{r}
missing <- v |>
  left_join(select(final, year, mig_start_doy), by = "year") |>
  group_by(year, mig_start_doy) |>
  summarize(n_missing = sum(is.na(count[doy < mig_start_doy & doy >= mig_start_doy - 14])), 
            .groups = "drop")

final <- left_join(final, missing, by = c("year", "mig_start_doy"))
```

### Preview

```{r}
gt(missing)
```
:::


## Data

We'll save the models and calculated metrics for use later.

**Final** is our main data, **GAMS** and **Cumulative Counts** are for if we
need to refer to the intermediate steps.

```{r}
write_csv(final, "Data/Datasets/vultures_final.csv")
write_rds(gams, "Data/Datasets/vultures_gams.rds")
write_csv(pred, "Data/Datasets/vultures_gams_pred.csv")
write_csv(cum_counts, "Data/Datasets/vultures_cumulative_counts.csv")
```

### Details

Data are organized as observations per year.

**General**

- `year` - Year of the data
- `date_min` - First date with an observation
- `date_max` - Last date with an observation
- `n_dates_obs` - Number of dates with a count
- `n_dates` - Number of dates in the range (min to max)
- `n_obs` - Total number of vultures seen

**Migration**

- `mig`/`peak`/`res`/`amibig` - Migration period (`mig`, 5%-95%) or peak migration period (`peak`, 25%-75%), or for population counts (below), the resident period (`res`, DOY < `r resident_date`) or the ambiguous period (`ambig`) that occurs after the resident period
but before the start of migration and after the end of migration.
- `start_doy`/`end_doy` - Start/end day-of-year dates for a period (e.g., `mig_start_doy`).
- `dur_days` - Duration in days of a period (e.g., `peak_dur_days`).
- `skew`/`kurt` - Skewness and kurtosis of the counts for a period (e.g., `mig_skew`, `peak_kurt`)

**Population Counts**

- `pop`/`raw` - Type of count, either from model predictions (`pop`) or
`raw` data counts.
- `min`/`max`/`median`/`mean`/`total` - Population statistic calculated (e.g, `peak_pop_mean`, `mig_raw_min`, `res_pop_median`, or `ambig_raw_max`). `total` means the total sum of daily counts. *Note:* `total` isn't a sensible metric for raw data as it is dependent on the number of observation days.

**Extra**

- `n_missing` - Number of days missing an observation in the two weeks before the start of migration.


## Figures

These figures are meant to be sanity checks of the models and the Resident Date
cutoff (`r resident_date`). 

Each year has two figures showing the model, one overlaid with the predicted 
counts (determined from the model), and one overlaid with the raw counts.

This way we could double check the calculations as well as the models.

Note that we always expect raw counts to be greater and with more variability than
predicted counts. Further, I do not include the sum of counts for the raw data as this is dependent on the number of observations and somewhat misleading. 

> These are not particularly important plots, but we do want to have a visual 
> of what's going on, if only to catch mistakes in the calculations.

**Interpretations**


- Black line - Predicted model
- Grey ribbon - 99% CI around the model 
- Yellow line (box) - Period defined as containing only residents (defined by date cutoff)
- Purple box - Period defined as migration period (defined by 5%-95% cumulative predicted counts), showing the min, max and median counts
- Blue box - Period defined as peak migration period (defined by 25%-75% cumulative predicted counts), showing the min, max and median counts 
- Sum counts - Text indicating the cumulative total number of predicted counts
  expected in that period. 
- `n days` - the total number of days with observations for that year.

```{r}
#| code-fold: true
#| results: asis

for(y in unique(v$year)) {
  cat("### ", y, " {#", y, "}\n\n", sep = "")
  if(y != "2007") {
    v1 <- filter(v, year == y)
    p <- filter(pred, year == y)
    c <- filter(cum_counts, year == y)
    f <- filter(final, year == y)
    
    g1 <- plot_model(raw = v1, pred = p, final = f)
    
    pop1 <- select(f, mig_start_doy, mig_end_doy, peak_start_doy, peak_end_doy, 
                   contains("pop"), -contains("ambig")) |>
      pivot_longer(everything()) |>
      mutate(type = str_extract(name, "min|max|median|mean|total|start|end"),
             stage = str_extract(name, "mig|peak|res"),
             stage = str_replace_all(stage, c("mig" = "Migration", 
                                              "peak" = "Peak Migration",
                                              "res" = "Residents"))) |>
      select(-name) |>
      pivot_wider(names_from = type) |>
      mutate(start = replace_na(start, 204),
             end = replace_na(end, resident_date))
    
    pop2 <- pivot_longer(pop1, cols = c("start", "end"), values_to = "doy")
    
    
    g1 <- plot_model(raw = v1, pred = p, final = f) +
      geom_rect(data = pop1, aes(xmin = start, xmax = end, ymin = min, ymax = max,
                                 fill = stage, colour = stage),
                inherit.aes = FALSE) +
      geom_path(data = pop2, aes(x = doy, y = median, colour = stage), linewidth = 1) +
      geom_text(data = filter(pop1, stage != "Residents"),
                aes(x = (end - start)/2 + start, y = max, colour = stage,
                    label = paste("Sum", stage, "Counts\n", total)), 
                nudge_y = c(-60, 65), nudge_x = c(-30, 0)) +
      scale_fill_viridis_d(end = 0.9, alpha = 0.5) +
      scale_colour_viridis_d(end = 0.9) +
      labs(title = paste0(y, " - Check dates and predicted population sizes"),
           caption = "Boxes define the start date to end date (left/right), as well as population min, max, and median (bottom/top/middle)\n'Total' refers to cumulative predicted observations")
    
    
    pop1 <- select(f, mig_start_doy, mig_end_doy, peak_start_doy, peak_end_doy, 
                   contains("raw"), -contains("ambig")) |>
      pivot_longer(everything()) |>
      mutate(type = str_extract(name, "min|max|median|mean|total|start|end"),
             stage = str_extract(name, "mig|peak|res"),
             stage = str_replace_all(stage, c("mig" = "Migration", 
                                              "peak" = "Peak Migration",
                                              "res" = "Residents"))) |>
      select(-name) |>
      pivot_wider(names_from = type) |>
      mutate(start = replace_na(start, 204),
             end = replace_na(end, resident_date))
    
    pop2 <- pivot_longer(pop1, cols = c("start", "end"), values_to = "doy")
    
    g2 <- plot_model(raw = v1, pred = p, final = f) +
      geom_rect(data = pop1, aes(xmin = start, xmax = end, ymin = min, ymax = max,
                                 fill = stage, colour = stage),
                inherit.aes = FALSE) +
      geom_path(data = pop2, aes(x = doy, y = median, colour = stage), linewidth = 1) +
      scale_fill_viridis_d(end = 0.9, alpha = 0.5) +
      scale_colour_viridis_d(end = 0.9) +
      labs(title = paste0(y, " - Check dates and raw population sizes"),
           caption = "Boxes define the start date to end date (left/right), as well as population min, max, and median (bottom/top/middle)")
    
    cat(":::{.panel-tabset}\n\n")
    cat("#### Predicted Counts\n\n")
    print(g1)
    cat("\n\n")
    cat("#### Raw Counts\n\n")
    print(g2)
    cat("\n\n")
    cat(":::\n\n")
    
    cat("\n\n")
    
  } else cat("No Data for 2007\n\n")
}
```



{{< include _reproducibility.qmd >}}
