[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Vulture Migration",
    "section": "",
    "text": "Vulture Migration\n\n\nDirected by Don Kramer and Daniel Donnecke, coded by Steffi LaZerte\nThis is the data exploration and analysis of Turkey Vulture kettling and migration behaviour above Rocky Point on southern Vancouver Island.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "XX_notes.html",
    "href": "XX_notes.html",
    "title": "Notes",
    "section": "",
    "text": "GAM reference - https://noamross.github.io/gams-in-r-course/\nGAM plotting using mgcv functions:\n\nlibrary(readr)\nlibrary(mgcv)\nlibrary(dplyr)\nv &lt;- read_csv(\"Data/Datasets/vultures_clean_2023.csv\")\ng &lt;- gam(count ~ s(doy, k = 10), \n         data = filter(v, year == 2000) |&gt; mutate(l = 3), \n         method = \"REML\", family = \"nb\")\nsummary(g)\n\n\nFamily: Negative Binomial(1.129) \nLink function: log \n\nFormula:\ncount ~ s(doy, k = 10)\n\nParametric coefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   2.3393     0.1207   19.38   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n         edf Ref.df Chi.sq p-value    \ns(doy) 5.475  6.616  175.5  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.391   Deviance explained = 71.3%\n-REML =  271.2  Scale est. = 1         n = 76\n\nplot(g, shade = TRUE, trans = exp, residuals = TRUE, pch = 20, \n     shift = coef(g)[1])\n\n\n\n\n\n\n\n\n\nModel assessment\n\nLooks for significant patterns in residuals (check that we have given high enough basis k to estimate ‘wiggliness’)\n\n\np0 &lt;- par(mfrow = c(2,2))\ngam.check(g, pch = 19, cex = 0.5)\n\n\n\n\n\n\n\n\n\nMethod: REML   Optimizer: outer newton\nfull convergence after 4 iterations.\nGradient range [8.674563e-10,1.084888e-07]\n(score 271.1993 & scale 1).\nHessian positive definite, eigenvalue range [2.264941,28.62155].\nModel rank =  10 / 10 \n\nBasis dimension (k) checking results. Low p-value (k-index&lt;1) may\nindicate that k is too low, especially if edf is close to k'.\n\n         k'  edf k-index p-value\ns(doy) 9.00 5.48    0.96    0.61\n\npar(p0)\n\n\nCompare to DHARMa\n\n\ns &lt;- DHARMa::simulateResiduals(g, plot = TRUE)\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\n\nRegistered S3 method overwritten by 'mgcViz':\n  method from  \n  +.gg   GGally\n\n\n\n\n\n\n\n\n\nWe want\n\nFull convergence\nK &gt; edf\nHigh p-value\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Appendicies",
      "Notes"
    ]
  },
  {
    "objectID": "04_figures.html",
    "href": "04_figures.html",
    "title": "Manuscript Figure and Supplemental",
    "section": "",
    "text": "This is the final figure and Supplemental material for the manuscript.",
    "crumbs": [
      "Workflow",
      "Manuscript Figure and Supplemental"
    ]
  },
  {
    "objectID": "04_figures.html#setup",
    "href": "04_figures.html#setup",
    "title": "Manuscript Figure and Supplemental",
    "section": "Setup",
    "text": "Setup\n\nsource(\"XX_functions.R\")  # Custom functions and packages\n\n# Metrics\nv &lt;- read_csv(\"Data/Datasets/vultures_final.csv\") |&gt;\n  # Round non-integer values of population counts\n  mutate(across(c(contains(\"pop\"), contains(\"raw\")), round)) \n\n# Raw counts\nraw &lt;- read_csv(\"Data/Datasets/vultures_clean_2023.csv\")\n\n# Predicted GAM models\npred &lt;- read_csv(\"Data/Datasets/vultures_gams_pred.csv\")\n\n# Checking problematic years\nsupp &lt;- read_csv(\"Data/Datasets/table_supplemental.csv\")",
    "crumbs": [
      "Workflow",
      "Manuscript Figure and Supplemental"
    ]
  },
  {
    "objectID": "04_figures.html#main-figure",
    "href": "04_figures.html#main-figure",
    "title": "Manuscript Figure and Supplemental",
    "section": "Main figure",
    "text": "Main figure\n\n\nCode\nv &lt;- v |&gt; \n  mutate(date = as_date(p50_doy) - days(1))\n\ng1 &lt;- ggplot(v, aes(x = year, y = date)) +\n  theme_bw() +\n  theme(axis.title.x = element_blank()) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  labs(y = \"Date of peak migration\", x = \"\")\n\ng2 &lt;- ggplot(v, aes(x = year, y = mig_raw_max)) +\n  theme_bw() +\n  geom_point() +\n  stat_smooth(method = MASS::glm.nb) +\n  labs(y = \"Annual maximum\\nDaily Estimated Total\", x = \"Year\")\n\ng &lt;- g1 / g2 + plot_annotation(tag_levels = \"A\")\n#ggsave(\"fig1_quick.png\", dpi = 1000, width = 8, height = 7)\n\ngg &lt;- g1 / (g2 + labs(y = \"Annual maximum DET\")) + plot_annotation(tag_levels = \"A\")\n\n\nDouble checking growth and increases\n\n\nCode\nm &lt;- MASS::glm.nb(mig_raw_max ~ year, data = v)\n\nd &lt;- select(v, year) |&gt;\n  mutate(y = predict(m, v),\n         yexp = exp(y))\n\n# Compound growth (avg growth over the period)\n(1251 / 451)^ (1/25) - 1\n\n\n[1] 0.04165339\n\n\nCode\n# Compound interest (amount at the end)\n451 * (1 + 0.041647)^(25)\n\n\n[1] 1250.808\n\n\nCode\n# Factor of increase\n1251/451\n\n\n[1] 2.773836\n\n\n\nBig version\nWith two y-axis options for panel B\n\n\nCode\ng\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nCode\ngg\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nSmall version\nWith two y-axis options for panel B\n\n\nCode\ng\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nCode\ngg\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Workflow",
      "Manuscript Figure and Supplemental"
    ]
  },
  {
    "objectID": "04_figures.html#map",
    "href": "04_figures.html#map",
    "title": "Manuscript Figure and Supplemental",
    "section": "Map",
    "text": "Map\n\n\nCode\nlibrary(bcmaps)\n\n\nSupport for Spatial objects (`sp`) was removed in {bcmaps} v2.0.0. Please use `sf` objects with {bcmaps}.\n\n\nCode\nlibrary(ggrepel)\nne_download(type = \"populated_places\", scale = \"large\", load = FALSE)\n\n\n[1] \"ne_10m_populated_places\"\n\n\nCode\ncities &lt;- bcmaps::bc_cities(ask = FALSE) |&gt; \n  select(NAME) |&gt;\n  filter(NAME %in% c(\"Victoria\", \"Vancouver\", \"Nanaimo\"))\n\n\nbc_cities was updated on 2024-07-10\n\n\nCode\ncities &lt;- ne_load(file_name = \"ne_10m_populated_places\") |&gt; \n  filter(NAME %in% c(\"Seattle\", \"Portland\"), ADM1NAME %in% c(\"Oregon\", \"Washington\")) |&gt;\n  select(NAME) |&gt; \n  st_transform(st_crs(cities)) |&gt;\n  bind_rows(cities) |&gt;\n  rename(name = NAME)\n\n\nReading layer `ne_10m_populated_places' from data source \n  `/home/steffi/R_tmpdir/Rtmp1ic9bU/ne_10m_populated_places.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 7342 features and 137 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -179.59 ymin: -90 xmax: 179.3833 ymax: 82.48332\nGeodetic CRS:  WGS 84\n\n\nCode\nstn &lt;- c(-123.55082035835214, 48.31773308537152) |&gt;\n  st_point() |&gt;\n  st_sfc(crs = 4326) |&gt;\n  st_sf(name = \"Rocky Point\") |&gt;\n  st_transform(st_crs(cities))\n\nland &lt;- data.frame(lon = c(-124.3, -124.3, -123.5),\n                   lat = c(48.39, 48.75, 49.2),\n                   name = c(\"Juan de Fuca Strait\", \"Vancouver Island\", \"Salish Sea\")) |&gt;\n  st_as_sf(coords = c(\"lon\", \"lat\"), crs = 4326)\n\narea &lt;- ne_states(country = c(\"Canada\", \"United States of America\")) |&gt;\n  #st_crop(st_bbox(c(xmin = -140, xmax = -112, ymin = 41, ymax = 60))) |&gt;\n  #filter(name %in% c(\"British Columbia\", \"Alberta\", \"Washington\", \"Oregon\", \"Idaho\", \"Montana\")) |&gt;\n  mutate(\n    name = if_else(name %in% c(\"British Columbia\", \"Washington\", \"Oregon\"), toupper(name), NA),\n    name = str_replace(name, \" \", \"\\n\"),\n    postal =  if_else(postal %in% c(\"BC\", \"WA\", \"OR\", \"AB\"), postal, NA))\n\nbox &lt;- st_polygon(list(rbind(c(1050000, 330000), \n                             c(1050000, 490000),\n                             c(1230000, 490000),\n                             c(1230000, 330000),\n                             c(1050000, 330000)))) |&gt;\n  st_sfc(crs = 3005)\n\ng0 &lt;- ggplot(data = area, aes(label = name)) +\n  theme_map() +\n  theme(panel.border = element_rect(fill = NA), \n        plot.margin = unit(c(0,0,0,0), units = \"mm\"),\n        panel.spacing = unit(0, units = \"mm\")) +\n  scale_x_continuous(expand = c(0,0)) +\n  scale_y_continuous(expand = c(0,0)) +\n  geom_sf()\n  \ng &lt;- g0 +\n  geom_sf(data = stn, size = 3) +\n  geom_sf_text(data = stn, lineheight = 0.85, nudge_y = -8000) +\n  geom_sf(data = cities) +\n  geom_sf_text(data = cities, hjust = 1.1, nudge_y = 5000) +\n  geom_sf_text(data = land, angle = c(-24, 0, -45), colour = \"grey60\", size = c(6, 7, 7)) +\n  annotation_scale(location = \"bl\") + \n  annotation_north_arrow(\n    location = \"bl\", height = unit(0.5, \"cm\"), width = unit(0.5, \"cm\"),\n    style = north_arrow_orienteering(text_size = -Inf), pad_y = unit(0.75, \"cm\")) +\n  coord_sf(crs = 3005, xlim = c(1050000, 1230000), ylim = c(330000, 490000))\n\ng_inset &lt;- g0 +\n  theme(panel.background = element_rect(fill = \"white\")) +\n  geom_sf_text(aes(label = postal), colour = \"black\", size = 3) +\n  geom_sf(data = box, fill = NA, linewidth = 0.5, colour = \"black\", inherit.aes = FALSE) +\n  coord_sf(crs = 3005, xlim = c(500000, 2000000), ylim = c(-200000, 1370000))\n\n\n\nBig\n\n\nCode\ng + inset_element(g_inset, left = 0, top = 1, bottom = 0.6, right = 0.342, align_to = \"full\")\n\n\nWarning: Removed 60 rows containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\n\n\n\n\n\n\n\n\nSmall\n\n\nCode\ng + inset_element(g_inset, left = 0, top = 1, bottom = 0.6, right = 0.342, align_to = \"full\")\n\n\nWarning: Removed 60 rows containing missing values or values outside the scale range\n(`geom_text()`).",
    "crumbs": [
      "Workflow",
      "Manuscript Figure and Supplemental"
    ]
  },
  {
    "objectID": "04_figures.html#supplemental-figure",
    "href": "04_figures.html#supplemental-figure",
    "title": "Manuscript Figure and Supplemental",
    "section": "Supplemental Figure",
    "text": "Supplemental Figure\n\n\nCode\npred &lt;- mutate(pred, ci95_upper = count + se * 1.96, ci95_lower = count - se * 1.96)\nres &lt;- filter(v, year == 1999) |&gt;\n  mutate(xmin = 204, \n         xmax = 240, y = 0.25 * mig_raw_max, \n         xmid = (240-204)/2 + 204,\n         y = 0.25 * mig_raw_max,\n         height = 0.03 * mig_raw_max, label = \"Resident period\")\n\ng0 &lt;- ggplot() +\n  theme_void() +\n  annotate(geom = \"text\", label = c(\"Migration\\nstart\", \"Peak\\nstart\", \"Peak\", \"Peak\\nend\", \"Migration\\nend\"), \n           y = -0.25, x = c(0, 0.7, 1.3, 1.6, 2), size = 3, lineheight = 0.85) +\n  annotate(geom = \"text\", label = c(\"5%\", \"25%\", \"50%\", \"75%\", \"95%\"), y = 0.25, x = c(0, 0.7, 1.3, 1.6, 2), size = 3) +\n  annotate(geom = \"segment\", y = 0, x = 0, xend = 2, arrow = arrow(angle = 90, ends = \"both\", length = unit(1, \"mm\"))) +\n  annotate(geom = \"segment\", y = 0, x = 0.7, xend = 1.6, linewidth = 4) +\n  annotate(geom = \"segment\", x = 1.29, xend = 1.30, y = 0, linewidth = 8) +\n  ylim(c(-0.5, 0.5)) +\n  xlim(c(-0.1, 2.1))\n\ng1 &lt;- ggplot(data = pred, mapping = aes(x = doy, y = count)) +\n  theme_bw() +\n  \n  # GAM\n  geom_ribbon(aes(ymin = ci95_lower, ymax = ci95_upper), fill = \"grey50\", alpha = 0.5) +\n  geom_line() +\n  \n  # Raw points\n  geom_point(data = raw, na.rm = TRUE, size = 0.5) +\n  \n  # Metrics\n  geom_errorbarh(data = res, aes(xmin = xmin, xmax = xmax, y = y, height = height), \n                 colour = \"grey70\", inherit.aes = FALSE) +\n  geom_text(data = res, aes(x = xmid, y = y, label = label), vjust = -0.5, inherit.aes = FALSE,\n            size = 3, colour = \"grey30\") +\n  geom_segment(data = v, aes(x = p50_doy - 0.5, xend = p50_doy + 0.5, \n                             y = -(0.07 * mig_raw_max)), \n               linewidth = 4, inherit.aes = FALSE) +\n  geom_errorbarh(data = v, aes(y = -(0.07 * mig_raw_max), \n                               xmin = mig_start_doy, \n                               xmax = mig_end_doy, \n                               height = 0.07 * mig_raw_max), inherit.aes = FALSE) +\n  geom_segment(data = v, aes(y = -(0.07 * mig_raw_max), x = peak_start_doy, xend = peak_end_doy), linewidth = 2, inherit.aes = FALSE) +\n  scale_x_continuous(name = \"Date\", limits = c(203, 295), \n                     labels = \\(x) format(as_date(x) - days(1), \"%b %d\"), \n                     n.breaks = 7) +\n  labs(y = \"Daily Estimated Total\") + \n  facet_wrap(~ year, scales = \"free_y\", ncol = 4)\n\n\n\nBig\n\n\nCode\ng1 + inset_element(g0, left = 0.55, right = 0.95, bottom = 0, top = 0.1)\n\n\n\n\n\n\n\n\n\n\n\nSmall\n\n\nCode\ng1 + inset_element(g0, left = 0.55, right = 0.95, bottom = 0, top = 0.1)",
    "crumbs": [
      "Workflow",
      "Manuscript Figure and Supplemental"
    ]
  },
  {
    "objectID": "04_figures.html#supplemental-table",
    "href": "04_figures.html#supplemental-table",
    "title": "Manuscript Figure and Supplemental",
    "section": "Supplemental Table",
    "text": "Supplemental Table\nPresented here for the record, but use exported XLSX version for submission.\n\n\nCode\nt &lt;- supp |&gt; \n  filter(!str_detect(term, \"(I|i)ntercept\"),\n         !str_detect(model, \"(max_doy)|(pop_max)\")) |&gt;\n  mutate(across(where(is.numeric), \\(x) round(x, 3))) |&gt;\n  mutate(p_value = if_else(p_value &lt;= 0.05, \n                           paste0(\"**\", format(p_value, nsmall = 3), \"**\"),\n                           format(p_value, nsmall = 3)),\n         value = paste0(format(estimate, nsmall = 3), \" (P = \", p_value, \")\"),\n         value = str_trim(value)) |&gt;\n  select(Model = model, value, type) |&gt;\n  mutate(type = str_replace(type, \"_\", \" \"),\n         type = str_to_title(type)) |&gt;\n  pivot_wider(names_from = \"type\", values_from = \"value\")\n\ngt(t) |&gt;\n  gt_theme() |&gt;\n  fmt_markdown() |&gt;\n  fmt_number(columns = -1, decimals = 3)\n\n\n\n\n\n\n\n\nModel\nOriginal\nGaps Removed\nEnd Removed\nAll Removed\n\n\n\n\nmig_start_doy ~ year\n0.206 (P = 0.011)\n0.204 (P = 0.011)\n0.198 (P = 0.026)\n0.196 (P = 0.029)\n\n\npeak_start_doy ~ year\n0.178 (P = 0.003)\n0.176 (P = 0.003)\n0.178 (P = 0.011)\n0.176 (P = 0.010)\n\n\np50_doy ~ year\n0.164 (P = 0.008)\n0.161 (P = 0.004)\n0.167 (P = 0.023)\n0.165 (P = 0.015)\n\n\npeak_end_doy ~ year\n0.156 (P = 0.013)\n0.151 (P = 0.008)\n0.147 (P = 0.048)\n0.143 (P = 0.036)\n\n\nmig_end_doy ~ year\n0.119 (P = 0.171)\n0.115 (P = 0.192)\n0.077 (P = 0.445)\n0.073 (P = 0.480)\n\n\nmig_dur_days ~ year\n-0.087 (P = 0.373)\n-0.089 (P = 0.388)\n-0.121 (P = 0.297)\n-0.123 (P = 0.314)\n\n\npeak_dur_days ~ year\n-0.022 (P = 0.563)\n-0.024 (P = 0.531)\n-0.031 (P = 0.485)\n-0.034 (P = 0.457)\n\n\nmig_raw_max ~ year\n0.041 (P = 0.000)\n0.042 (P = 0.000)\n0.033 (P = 0.005)\n0.034 (P = 0.005)\n\n\n\n\n\n\n\nCode\nt &lt;- supp |&gt; \n  filter(!str_detect(term, \"(I|i)ntercept\")) |&gt;\n  mutate(across(where(is.numeric), \\(x) round(x, 3))) |&gt;\n  select(model, type, estimate, p_value) |&gt;\n  pivot_wider(names_from = \"type\", values_from = c(\"estimate\", \"p_value\"),\n              names_glue = \"{type}_{.value}\") |&gt;\n  select(model, contains(\"original\"), contains(\"gaps\"), contains(\"end\"), contains(\"all\")) |&gt;\n  rename(\"Model\" = \"model\",\n         \"Original\" = \"original_estimate\", \n         \"Removed Gaps\" = \"gaps_removed_estimate\",\n         \"Removed Ends \" = \"end_removed_estimate\", \n         \"Removed All\" = \"all_removed_estimate\")\n\nwb &lt;- createWorkbook()\naddWorksheet(wb, \"Supplemental Table 1\")\nwriteData(wb, 1, t)\nbold &lt;- createStyle(textDecoration = \"bold\")\npval &lt;- createStyle(halign = \"left\", numFmt = \"(P = 0.000)\")\nest &lt;- createStyle(halign = \"right\", numFmt = \"0.000\")\nfor(c in c(3, 5, 7, 9)) {\n  conditionalFormatting(wb, 1, cols = c, rows = 1:15, rule = \"&lt;=0.05\", style = bold)\n  addStyle(wb, 1, cols = c, rows = 1:15, style = pval, stack = TRUE)\n}\nfor(c in c(2, 4, 6, 8)) {\n  addStyle(wb, 1, cols = c, rows = 1:15, style = est, stack = TRUE)\n}\nmergeCells(wb, 1, cols = 2:3, rows = 1)\nmergeCells(wb, 1, cols = 4:5, rows = 1)\nmergeCells(wb, 1, cols = 6:7, rows = 1)\nmergeCells(wb, 1, cols = 8:9, rows = 1)\naddStyle(wb, 1, cols = 1:9, rows = 1, \n         style = createStyle(halign = \"center\", textDecoration = \"bold\"),\n         stack = TRUE)\nsetColWidths(wb, 1, cols = 1:9, widths = 10)\nsetColWidths(wb, 1, cols = 1, widths = 20)\n\nsetRowHeights(wb, 1, rows = nrow(t) + 3, heights = 100)\nwriteData(\n  wb, 1, startCol = 1, startRow = nrow(t) + 3, \n  x = paste0(\n    \"Table. Results [Estimate of Year (P-value)] for different models when removing selective years. \\n\",\n    \"'Original' represents the results for the original model;\\n\",\n    \"'Removed Gaps' represents the results for these models when removing years with large gaps around the migration peaks (2011, 2013);\\n\",\n    \"'Removed Ends' represents the results for these models when removing years where it looks like sampling ended earlier than the end of migration (2001, 2006, 2023);\\n\",\n    \"'Removed All' represents the results for these models when removing all questionable years (2001, 2006, 2011, 2013, 2023)\\n\",\n    \"Bold P values are significant at P &lt;= 0.05.\"\n    ))\n\nsaveWorkbook(wb, \"Data/table_supp.xlsx\", overwrite = TRUE)",
    "crumbs": [
      "Workflow",
      "Manuscript Figure and Supplemental"
    ]
  },
  {
    "objectID": "02_calculate_metrics.html",
    "href": "02_calculate_metrics.html",
    "title": "Calculate Metrics",
    "section": "",
    "text": "Having explored the data (Initial Exploration) and various ways of calculating metrics of migration timing, we will now calculate and explore these metrics for the entire data set.\n\n\n\na GAM approach to model the pattern of vulture counts\npercentiles based on cumulative modelled counts to assess dates of passage\nOption 3 to account for resident birds (subtract the predicted mean residents prior to calculating the cumulative counts)",
    "crumbs": [
      "Workflow",
      "Calculate Metrics"
    ]
  },
  {
    "objectID": "02_calculate_metrics.html#background",
    "href": "02_calculate_metrics.html#background",
    "title": "Calculate Metrics",
    "section": "",
    "text": "Having explored the data (Initial Exploration) and various ways of calculating metrics of migration timing, we will now calculate and explore these metrics for the entire data set.\n\n\n\na GAM approach to model the pattern of vulture counts\npercentiles based on cumulative modelled counts to assess dates of passage\nOption 3 to account for resident birds (subtract the predicted mean residents prior to calculating the cumulative counts)",
    "crumbs": [
      "Workflow",
      "Calculate Metrics"
    ]
  },
  {
    "objectID": "02_calculate_metrics.html#load-data",
    "href": "02_calculate_metrics.html#load-data",
    "title": "Calculate Metrics",
    "section": "Load Data",
    "text": "Load Data\n\nsource(\"XX_functions.R\")  # Custom functions and packages\n\nset.seed(1234) # To make this reproducible\n\nv &lt;- read_csv(\"Data/Datasets/vultures_clean_2023.csv\")\nresident_date &lt;- 240",
    "crumbs": [
      "Workflow",
      "Calculate Metrics"
    ]
  },
  {
    "objectID": "02_calculate_metrics.html#metrics-to-assess",
    "href": "02_calculate_metrics.html#metrics-to-assess",
    "title": "Calculate Metrics",
    "section": "Metrics to assess",
    "text": "Metrics to assess\nTo answer these questions we will summarize the counts into specific metrics representing the timing of migration.\nSpecifically, we would like to calculate the\n\ndates of 5%, 25%, 50%, 75%, and 95% of the kettle numbers\nduration of passage - No. days between 5% and 95%\nduration of peak passage - No. days between 25% and 75%\n\nPopulation size (no. vultures in aggregations)\n\nmaximum\ncumulative\nnumber at peak passage (mean, median, range)\nnumber of locals (mean, median, range)\n\nOf these, the most important starting metrics are the dates of 5%, 25%, 50%, 75%, and 95% of the kettle numbers. These dates will define migration phenology as well as local vs. migrating counts. All other calculations can be performed using these values and the raw data.",
    "crumbs": [
      "Workflow",
      "Calculate Metrics"
    ]
  },
  {
    "objectID": "02_calculate_metrics.html#proceedure",
    "href": "02_calculate_metrics.html#proceedure",
    "title": "Calculate Metrics",
    "section": "Proceedure",
    "text": "Proceedure\nThe steps for calculating these metrics are as follows.\nFor each year we will calculate…\n\nA GAM\nThe median number of residents, using day 240 as a cutoff\nThe cumulative migration counts\nThe dates of passage as percentiles of these cumulative counts (5%, 25%, 75%, 95%)\nThe duration of (peak) passage from these dates\nThe population size (max, cumulative, stats at peak passage, stats of locals)\n\nWe will also create figures outlining these metrics for each year and will use these to assess whether anything needs to be tweaked (i.e. perhaps the date 240 cutoff)",
    "crumbs": [
      "Workflow",
      "Calculate Metrics"
    ]
  },
  {
    "objectID": "02_calculate_metrics.html#calculate-metrics",
    "href": "02_calculate_metrics.html#calculate-metrics",
    "title": "Calculate Metrics",
    "section": "Calculate Metrics",
    "text": "Calculate Metrics\n\n0. Sample sizes\n\nCalculatePreview\n\n\n\nsamples &lt;- v |&gt;\n  group_by(year) |&gt;\n  filter(!is.na(count)) |&gt; # Omit missing dates\n  summarize(\n    date_min = min(date), date_max = max(date),\n    # number of dates with a count\n    n_dates_obs = n(),           \n    # number of dates in the range\n    n_dates = as.numeric(difftime(date_max, date_min, units = \"days\")), \n    n_obs = sum(count))\n\n\n\n\ngt(samples)\n\n\n\n\n\n\n\nyear\ndate_min\ndate_max\nn_dates_obs\nn_dates\nn_obs\n\n\n\n\n1999\n1999-07-25\n1999-10-20\n64\n87\n7397\n\n\n2000\n2000-07-23\n2000-10-18\n76\n87\n2623\n\n\n2001\n2001-07-23\n2001-10-07\n64\n76\n3366\n\n\n2002\n2002-07-23\n2002-10-21\n83\n90\n4454\n\n\n2003\n2003-07-23\n2003-10-18\n83\n87\n6229\n\n\n2004\n2004-07-23\n2004-10-18\n80\n87\n9052\n\n\n2005\n2005-07-23\n2005-10-18\n83\n87\n5267\n\n\n2006\n2006-07-23\n2006-10-17\n73\n86\n5317\n\n\n2008\n2008-07-23\n2008-10-17\n59\n86\n3761\n\n\n2009\n2009-07-23\n2009-10-18\n62\n87\n5257\n\n\n2010\n2010-07-23\n2010-10-18\n75\n87\n7956\n\n\n2011\n2011-07-24\n2011-10-20\n69\n88\n3032\n\n\n2012\n2012-07-23\n2012-10-18\n78\n87\n5327\n\n\n2013\n2013-07-23\n2013-10-18\n69\n87\n4006\n\n\n2014\n2014-07-23\n2014-10-18\n74\n87\n6021\n\n\n2015\n2015-07-23\n2015-10-18\n77\n87\n5428\n\n\n2016\n2016-07-23\n2016-10-18\n68\n87\n8137\n\n\n2017\n2017-07-23\n2017-10-17\n62\n86\n4827\n\n\n2018\n2018-07-23\n2018-10-18\n75\n87\n6672\n\n\n2019\n2019-07-23\n2019-10-18\n87\n87\n6476\n\n\n2020\n2020-07-23\n2020-10-18\n81\n87\n12595\n\n\n2021\n2021-07-23\n2021-10-18\n82\n87\n9652\n\n\n2022\n2022-07-23\n2022-10-18\n86\n87\n19826\n\n\n2023\n2023-07-23\n2023-10-15\n84\n84\n12749\n\n\n\n\n\n\n\n\n\n\n\n\n1. GAMs\nAs developed in our Initial Exploration we will use:\n\nNegative binomial model to fit count data with overdispersion\nUse Restricted Maximum Likelihood (“Most likely to give you reliable, stable results”1)\nA smoother (s()) over doy (day of year) to account for non-linear migration patterns\nk = 10 (up to 10 basis functions; we want enough to make sure we capture the patterns, but too many will slow things down).\n\n\nModelsModel EvaluationModel Check Plots\n\n\nRun GAM on each year (except 2007)\n\ngams &lt;- v |&gt;\n  mutate(count = as.integer(count)) |&gt;\n  filter(year != 2007) |&gt; # Can't model 2007 because no data\n  nest(counts = -year) |&gt;\n  mutate(models = map(counts, \\(x) gam(count ~ s(doy, k = 20), data = x, \n                                      method = \"REML\", family = \"nb\")))\n\nCreate model predictions\n\ngams &lt;- gams |&gt;\n  mutate(\n    doy = map(counts, \\(x) list(doy = min(x$doy):max(x$doy))),\n    pred = map2(\n      models, doy, \n      \\(x, y) predict(x, newdata = y, type = \"response\", se.fit = TRUE)),\n    pred = map2(\n      pred, doy,\n      \\(x, y) data.frame(doy = y, count = x$fit, se = x$se) |&gt;\n        mutate(ci99_upper = count + se * 2.58,\n               ci99_lower = count - se * 2.58)))\n\npred &lt;- gams |&gt;\n  select(year, pred) |&gt;\n  unnest(pred)\n\n\n\nChecks to ensure models are valid.\nHere we look for two things\n\nfirst that there is full convergence\nsecond that there is not a significant non-random pattern in the residuals around the smoothing term (p-value, but be aware this is an approximation2)\n\nIf we have low p-values, we want to check and see\n\nif the model doesn’t look like it fits the data (see the model plots at the end of this script)\nif the k (number of basis functions) and edf (effective degrees of freedom) values are similar (if they are, this implies that we haven’t picked a large enough k)\n\n\n\nCode\nchecks &lt;- gams |&gt;\n  mutate(checks = map2(models, year, gam_check)) |&gt;\n  invisible() |&gt;\n  mutate(plots = map(checks, \\(x) pluck(x, \"plot\")),\n         df = map(checks, \\(x) pluck(x, \"checks\"))) |&gt;\n  unnest(df) |&gt;\n  mutate(low_k = p_value &lt; 0.1)\n\nc &lt;- checks |&gt;\n  filter(low_k | !full_convergence) |&gt;\n  select(year, param, k, edf, k_index, p_value, convergence)\n\ngt(c)\n\n\n\n\n\n\n\n\nyear\nparam\nk\nedf\nk_index\np_value\nconvergence\n\n\n\n\n2010\ns(doy)\n19.00\n8.57\n0.76\n0.027\nfull convergence after 4 iterations.\n\n\n2012\ns(doy)\n19.00\n7.68\n0.76\n0.021\nfull convergence after 4 iterations.\n\n\n\n\n\n\n\n\n\nThese plots are two different ways of presenting model diagnostics.\ngam.check() is the default check that produces both these plots as well as the diagnostics in the Model Evaluation tab.\nDHARMa is a package for simulating residuals to allow model checking for all types of models (details).\nBoth these sets of plots can be interpreted similarly to general linear model plots. We want roughly normal residuals and constant variance.\nI tend to put more weight on DHARMa as it’s plots are easier to interpret for non-Gaussian model residuals, but I have included the gam.check() plots for completeness.\n\nDHARMagam.check()\n\n\nYear: 1999 \nDHARMa’s simulateResiduals() plot\n\nYear: 2000 \nDHARMa’s simulateResiduals() plot\n\nYear: 2001 \nDHARMa’s simulateResiduals() plot\n\nYear: 2002 \nDHARMa’s simulateResiduals() plot\n\nYear: 2003 \nDHARMa’s simulateResiduals() plot\n\nYear: 2004 \nDHARMa’s simulateResiduals() plot\n\nYear: 2005 \nDHARMa’s simulateResiduals() plot\n\nYear: 2006 \nDHARMa’s simulateResiduals() plot\n\nYear: 2008 \nDHARMa’s simulateResiduals() plot\n\nYear: 2009 \nDHARMa’s simulateResiduals() plot\n\nYear: 2010 \nDHARMa’s simulateResiduals() plot\n\nYear: 2011 \nDHARMa’s simulateResiduals() plot\n\nYear: 2012 \nDHARMa’s simulateResiduals() plot\n\nYear: 2013 \nDHARMa’s simulateResiduals() plot\n\nYear: 2014 \nDHARMa’s simulateResiduals() plot\n\nYear: 2015 \nDHARMa’s simulateResiduals() plot\n\nYear: 2016 \nDHARMa’s simulateResiduals() plot\n\nYear: 2017 \nDHARMa’s simulateResiduals() plot\n\nYear: 2018 \nDHARMa’s simulateResiduals() plot\n\nYear: 2019 \nDHARMa’s simulateResiduals() plot\n\nYear: 2020 \nDHARMa’s simulateResiduals() plot\n\nYear: 2021 \nDHARMa’s simulateResiduals() plot\n\nYear: 2022 \nDHARMa’s simulateResiduals() plot\n\nYear: 2023 \nDHARMa’s simulateResiduals() plot\n\n\n\nYear: 1999 \ngam.check() plot\n\nYear: 2000 \ngam.check() plot\n\nYear: 2001 \ngam.check() plot\n\nYear: 2002 \ngam.check() plot\n\nYear: 2003 \ngam.check() plot\n\nYear: 2004 \ngam.check() plot\n\nYear: 2005 \ngam.check() plot\n\nYear: 2006 \ngam.check() plot\n\nYear: 2008 \ngam.check() plot\n\nYear: 2009 \ngam.check() plot\n\nYear: 2010 \ngam.check() plot\n\nYear: 2011 \ngam.check() plot\n\nYear: 2012 \ngam.check() plot\n\nYear: 2013 \ngam.check() plot\n\nYear: 2014 \ngam.check() plot\n\nYear: 2015 \ngam.check() plot\n\nYear: 2016 \ngam.check() plot\n\nYear: 2017 \ngam.check() plot\n\nYear: 2018 \ngam.check() plot\n\nYear: 2019 \ngam.check() plot\n\nYear: 2020 \ngam.check() plot\n\nYear: 2021 \ngam.check() plot\n\nYear: 2022 \ngam.check() plot\n\nYear: 2023 \ngam.check() plot\n\n\n\n\n\n\n\n\nModel Validity\nBased on the Model Evaluation, there are years with lower k-indices and a p-value &lt; 0.1. It may be worth double checking that these models don’t look too unreasonable.\nOn the whole, there seems to be quite a bit of variability, but nothing that seems especially problematic.\n\n\nCode\ng &lt;- lapply(c$year, \\(y) plot_model(v[v$year == y, ], pred[pred$year == y, ]) +\n              labs(title = y))\nwrap_plots(g)\n\n\n\n\n\n\n\n\n\nBased on the Model Check Plots, particularly the ones with Simulated DHARMa residuals, we have good model fit (QQ plots of residuals) throughout, but a couple examples of potential non-constant variance. However, I am not very concerned about this for several reasons.\n\nAlthough DHARMa highlighted these plots as having significant quantile deviations, visually, I don’t find the deviations that concerning.\nHeteroscedasticity can lead to issues with our Standard Errors, but since we’re only really interested in the predicted value (based on the estimates), these problems don’t really apply to us (we’re not interpreting model error or significance of parameters).\n\n\nTherefore I suggest we proceed with these models and extract the metrics we’re interested in.\n\n\n\n\n2. Residents\nUsing a resident date (resident_date) cutoff of DOY 240. Here we calculate the min, max, mean, and median number of residents throughout the ‘resident period’ (from the start to DOY 240).\nNote: Resident counts are fractional because they are based on the predicted counts from the GAM models (i.e. the smoothed curve).\n\nCalculatePreview\n\n\n\nresidents &lt;- pred |&gt;\n  filter(doy &lt; resident_date) |&gt;\n  group_by(year) |&gt;\n  # Calculate resident statistics by year\n  summarize(res_pop_min = min(count), \n            res_pop_max = max(count), \n            res_pop_median = median(count), \n            res_pop_mean = mean(count)) |&gt;\n  # Round to 1 decimal place\n  mutate(across(starts_with(\"res\"), \\(x) round(x, 1)))\n\n\n\n\ngt(residents)\n\n\n\n\n\n\n\nyear\nres_pop_min\nres_pop_max\nres_pop_median\nres_pop_mean\n\n\n\n\n1999\n3.7\n8.4\n4.6\n5.5\n\n\n2000\n1.1\n4.6\n2.5\n2.5\n\n\n2001\n3.7\n14.1\n7.4\n8.1\n\n\n2002\n3.1\n5.7\n4.4\n4.3\n\n\n2003\n3.8\n5.8\n4.7\n4.7\n\n\n2004\n2.9\n5.1\n4.3\n4.0\n\n\n2005\n3.8\n5.9\n4.3\n4.6\n\n\n2006\n2.2\n4.4\n3.2\n3.3\n\n\n2008\n0.9\n3.4\n1.5\n1.8\n\n\n2009\n2.6\n4.4\n3.4\n3.4\n\n\n2010\n1.5\n7.6\n6.3\n5.7\n\n\n2011\n3.2\n5.3\n3.7\n3.8\n\n\n2012\n3.0\n5.5\n4.2\n4.2\n\n\n2013\n3.4\n8.5\n5.5\n6.0\n\n\n2014\n2.5\n3.6\n2.7\n2.9\n\n\n2015\n3.5\n5.3\n4.7\n4.6\n\n\n2016\n6.1\n11.8\n9.4\n9.2\n\n\n2017\n3.3\n8.0\n5.2\n5.1\n\n\n2018\n5.7\n13.4\n11.3\n10.6\n\n\n2019\n6.6\n11.1\n8.0\n8.4\n\n\n2020\n4.8\n13.4\n11.8\n10.8\n\n\n2021\n6.8\n13.2\n9.1\n9.3\n\n\n2022\n3.0\n8.5\n7.2\n6.4\n\n\n2023\n4.4\n8.9\n7.1\n7.2\n\n\n\n\n\n\n\n\n\n\n\n\n3. Cumulative counts\nHere we calculate cumulative counts after subtracting the median resident population.\nAs noted in our initial exploration, if we calculate cumulative accounts across the whole year, we include cumulative counts of residents which could bias the start of migration to an earlier date.\nTherefore, we subtract the median number of resident birds from each daily count and then calculate the cumulative number of birds seen in kettles and use this cumulative curve to calculate our metrics of migration (calculated in the following sections).\nBecause we are subtracting a median value from the whole data range within a year we can occasionally get some funny counts (i.e. if the number of resident vultures fluctuated between 2 and 4, this would mean that subtracting a median of 3 would sometimes result in a negative count which in turn would mean that the cumulative count would sometimes go down, rather than up).\nHowever, after reviewing the cumulative count figures for all years, I’ve added one more rule: we only start cumulative counts after DOY of 240. This is still very much before the start of migration, but avoids cumulative counts during the very clearly resident period. It helps avoid some of the more extreme negative cumulative counts. Doing this doesn’t appreciably alter the metrics calculated, but seems more reasonable. So gives me more peace of mind.\nOverall: I’m not concerned about minor negative cumulative count blips because a) they are minor, and b) they only occur during the resident phase. As soon as the birds start accumulating for migration, the number of birds present is above the resident number fluctuations and the cumulative count accumulates.\nAlso Note: Counts are fractional because they are based on the predicted counts from the GAM models (i.e. the smoothed curve).\n\nCalculatePreviewVisual\n\n\n\ncum_counts &lt;- pred |&gt;\n  left_join(select(residents, year, res_pop_median), by = \"year\") |&gt;\n  group_by(year) |&gt;\n  mutate(count_init = count,\n         count = count - res_pop_median, # Subtract residents from predicted counts\n         count = if_else(doy &lt; resident_date, 0, count),\n         count_sum = cumsum(count)) |&gt; # Calculate cumulative sum\n  ungroup() |&gt;\n  select(year, doy, res_pop_median, count_init, count, count_sum)\n\n\n\nShowing only a snapshot of the middle of the year 1999.\n\ncount_init is the predicted daily kettle count (only included here for illustration)\ncount is the predicted daily kettle count after residents are removed (this means that negative counts are possible before migration starts).\ncount_sum is the cumulative sum of the count column (this means that it can decrease if counts are negative before migration starts).\n\nNote: the first couple of counts are zero because they take place before the resident date cutoff (240), and have been set to zero so we start cumulative counts on 240.\nThe median number of residents for 1999 is 4.6\n\ngt(slice(cum_counts, 35:60))\n\n\n\n\n\n\n\nyear\ndoy\nres_pop_median\ncount_init\ncount\ncount_sum\n\n\n\n\n1999\n238\n4.6\n4.457062\n0.0000000\n0.0000000\n\n\n1999\n239\n4.6\n4.624695\n0.0000000\n0.0000000\n\n\n1999\n240\n4.6\n4.818115\n0.2181154\n0.2181154\n\n\n1999\n241\n4.6\n5.035076\n0.4350758\n0.6531912\n\n\n1999\n242\n4.6\n5.272257\n0.6722571\n1.3254484\n\n\n1999\n243\n4.6\n5.527974\n0.9279740\n2.2534224\n\n\n1999\n244\n4.6\n5.812629\n1.2126292\n3.4660516\n\n\n1999\n245\n4.6\n6.153038\n1.5530377\n5.0190893\n\n\n1999\n246\n4.6\n6.590825\n1.9908247\n7.0099139\n\n\n1999\n247\n4.6\n7.177578\n2.5775779\n9.5874918\n\n\n1999\n248\n4.6\n7.968728\n3.3687283\n12.9562201\n\n\n1999\n249\n4.6\n9.021757\n4.4217568\n17.3779769\n\n\n1999\n250\n4.6\n10.412951\n5.8129509\n23.1909278\n\n\n1999\n251\n4.6\n12.246878\n7.6468781\n30.8378059\n\n\n1999\n252\n4.6\n14.656202\n10.0562023\n40.8940082\n\n\n1999\n253\n4.6\n17.815874\n13.2158736\n54.1098818\n\n\n1999\n254\n4.6\n21.964218\n17.3642181\n71.4740999\n\n\n1999\n255\n4.6\n27.448787\n22.8487866\n94.3228865\n\n\n1999\n256\n4.6\n34.779072\n30.1790719\n124.5019584\n\n\n1999\n257\n4.6\n44.671167\n40.0711665\n164.5731249\n\n\n1999\n258\n4.6\n58.061065\n53.4610653\n218.0341902\n\n\n1999\n259\n4.6\n76.051163\n71.4511634\n289.4853536\n\n\n1999\n260\n4.6\n99.777668\n95.1776678\n384.6630214\n\n\n1999\n261\n4.6\n130.271456\n125.6714561\n510.3344775\n\n\n1999\n262\n4.6\n168.225334\n163.6253336\n673.9598111\n\n\n1999\n263\n4.6\n213.860507\n209.2605071\n883.2203182\n\n\n\n\n\n\n\n\n\nThese figures show the early part of the migration season with predicted daily counts (red) overlaid with the adjusted cumulative predicted counts (grey).\n\nRed bars are predicted non-cumulative initial counts (no subtractions made)\nGrey bars are cumulative predicted counts created after subtracting resident birds from each daily count. Accumulating only after 240.\nNote that there is a little negative cumulative blip in 2013 that evens out relatively quickly.\n\n\nggplot(data = filter(cum_counts, doy &lt; 265), aes(x = doy, y = count_sum)) +\n  theme_bw() +\n  geom_bar(aes(y = count_init), stat = \"identity\", fill = \"red\", alpha = 0.8) +\n  geom_bar(stat = \"identity\", alpha = 0.5) +\n  #geom_bar(aes(y = count), stat = \"identity\", fill = \"blue\", alpha = 0.5) +\n  facet_wrap(~ year, scales = \"free_y\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n4. Dates of passage\nNow that we have the cumulative predicted counts, we can calculate the dates at which a particular proportion of the migrating population had passed through.\nHere we look at the dates at which 5%, 25%, 50%, 75%, and 95% of the birds have passed through.\nWe also calculate the date at which the predicted count was at it’s maximum.\nThe 5-95% and 25-75% date rages will then be used to calculate duration of passage, population sizes, and migration patterns in the next sections.\n\nCalculatePreview\n\n\n\nmax_passage &lt;- pred |&gt;\n  group_by(year) |&gt; \n  # Keep first max count\n  slice_max(count, with_ties = FALSE) |&gt;\n  mutate(measure = \"max\") |&gt;\n  select(year, measure, doy_passage = doy)\n\ndts &lt;- cum_counts |&gt;\n  select(year, doy, count, count_sum) |&gt;\n  group_by(year) |&gt;\n  calc_dates() |&gt;\n  rename(\"measure\" = \"perc\") |&gt;\n  bind_rows(max_passage) |&gt;\n  arrange(year, measure)\n\n\n\nOnly showing part of the data.\n\ngt(slice(dts, 1:30))\n\n\n\n\n\n\n\nyear\nmeasure\ncount_thresh\ndoy_passage\ncount_pred\n\n\n\n\n1999\nmax\nNA\n271\nNA\n\n\n1999\np05\n431.3002\n261\n125.67146\n\n\n1999\np25\n2156.5009\n267\n448.50778\n\n\n1999\np50\n4313.0019\n271\n603.35750\n\n\n1999\np75\n6469.5028\n275\n446.10741\n\n\n1999\np95\n8194.7035\n282\n109.70742\n\n\n2000\nmax\nNA\n269\nNA\n\n\n2000\np05\n136.1183\n253\n25.80135\n\n\n2000\np25\n680.5916\n262\n95.98933\n\n\n2000\np50\n1361.1831\n268\n138.06548\n\n\n2000\np75\n2041.7747\n273\n116.72027\n\n\n2000\np95\n2586.2479\n281\n32.13846\n\n\n2001\nmax\nNA\n271\nNA\n\n\n2001\np05\n161.7092\n254\n27.60036\n\n\n2001\np25\n808.5459\n265\n126.37183\n\n\n2001\np50\n1617.0917\n270\n197.69425\n\n\n2001\np75\n2425.6376\n274\n172.39983\n\n\n2001\np95\n3072.4743\n280\n52.30889\n\n\n2002\nmax\nNA\n266\nNA\n\n\n2002\np05\n223.0923\n257\n60.39372\n\n\n2002\np25\n1115.4614\n264\n222.46137\n\n\n2002\np50\n2230.9229\n268\n224.84705\n\n\n2002\np75\n3346.3843\n275\n119.12477\n\n\n2002\np95\n4238.7535\n285\n60.99240\n\n\n2003\nmax\nNA\n269\nNA\n\n\n2003\np05\n295.8550\n259\n85.83200\n\n\n2003\np25\n1479.2748\n266\n277.49744\n\n\n2003\np50\n2958.5497\n271\n300.54305\n\n\n2003\np75\n4437.8245\n276\n227.34231\n\n\n2003\np95\n5621.2443\n284\n78.15856\n\n\n\n\n\n\n\n\n\n\n\n\n5. Duration of passage\nDuration of passage is calculated as the number of days between the 5% and 95% (migration) and the 25% and 75% (peak migration) dates.\nWe also organize the dates of passage into separate columns in this step.\n\nCalculatePreview\n\n\n\npassage &lt;- dts |&gt;\n  group_by(year) |&gt;\n  summarize(mig_start_doy = doy_passage[measure == \"p05\"],\n            mig_end_doy = doy_passage[measure == \"p95\"],\n            peak_start_doy = doy_passage[measure == \"p25\"],\n            peak_end_doy = doy_passage[measure == \"p75\"],\n            p50_doy = doy_passage[measure == \"p50\"],\n            max_doy = doy_passage[measure == \"max\"],\n            mig_dur_days = mig_end_doy - mig_start_doy,\n            peak_dur_days = peak_end_doy - peak_start_doy)\n\n\n\n\ngt(passage)\n\n\n\n\n\n\n\nyear\nmig_start_doy\nmig_end_doy\npeak_start_doy\npeak_end_doy\np50_doy\nmax_doy\nmig_dur_days\npeak_dur_days\n\n\n\n\n1999\n261\n282\n267\n275\n271\n271\n21\n8\n\n\n2000\n253\n281\n262\n273\n268\n269\n28\n11\n\n\n2001\n254\n280\n265\n274\n270\n271\n26\n9\n\n\n2002\n257\n285\n264\n275\n268\n266\n28\n11\n\n\n2003\n259\n284\n266\n276\n271\n269\n25\n10\n\n\n2004\n262\n288\n268\n278\n273\n272\n26\n10\n\n\n2005\n256\n283\n265\n275\n270\n270\n27\n10\n\n\n2006\n257\n282\n264\n275\n270\n270\n25\n11\n\n\n2008\n256\n281\n265\n275\n270\n271\n25\n10\n\n\n2009\n261\n277\n266\n272\n269\n269\n16\n6\n\n\n2010\n262\n282\n268\n276\n272\n273\n20\n8\n\n\n2011\n263\n286\n270\n280\n276\n277\n23\n10\n\n\n2012\n259\n284\n267\n276\n272\n272\n25\n9\n\n\n2013\n262\n287\n269\n280\n274\n275\n25\n11\n\n\n2014\n260\n288\n269\n280\n274\n274\n28\n11\n\n\n2015\n259\n289\n267\n276\n271\n270\n30\n9\n\n\n2016\n258\n285\n265\n276\n270\n271\n27\n11\n\n\n2017\n261\n280\n267\n275\n271\n272\n19\n8\n\n\n2018\n260\n285\n267\n278\n272\n270\n25\n11\n\n\n2019\n262\n282\n267\n276\n271\n271\n20\n9\n\n\n2020\n264\n288\n272\n281\n277\n277\n24\n9\n\n\n2021\n258\n281\n266\n275\n271\n271\n23\n9\n\n\n2022\n265\n285\n271\n279\n275\n276\n20\n8\n\n\n2023\n259\n286\n268\n278\n273\n274\n27\n10\n\n\n\n\n\n\n\n\n\n\n\n\n6. Population size\nHere we calculate population size metrics for different stages of the migration\n\nResidents (before resident date cutoff [DOY 240])\nMigrants (between migration start [5%] and migration end [95%])\nAmbiguous (between resident date cutoff and migration start as well as after migration end)\nPeak migrants (between peak start [25%] and peak end [75%])\nRaw counts for migrants and peak migrants (min, mean, median, max, total)\n\nThese are the counts recorded by observers in the field\nRemember that total is affected by missing dates\n\n\nNote: I don’t expect to use the ambiguous category, but have included it for completeness and sanity checks as needed.\nBecause Migrants and Peak Migrants actually overlap (i.e a peak migrant is also a migrant). They are calculated separately and then joined back in together.\nWe also calculate ‘raw’ counts, although we should be careful about interpreting the ‘totals’ as those will depend quite a bit on how many days of observation there were. I include total raw counts only for interest or for statements along the lines of “Observers counted over X individual vultures over 26 years and X days”. Not for actual analysis.\nWe calculate min/max/mean/median/total, but do not expect to analyse all metrics. These are good for including in tables of descriptive stats and sanity checks.\n\nCalculatePreview\n\n\n\n# General stats on predicted counts of migrants, resident, and other\npop_size &lt;- pred |&gt;\n  left_join(passage, by = \"year\") |&gt;\n  mutate(state = case_when(doy &lt; resident_date ~ \"res\",\n                           doy &gt;= mig_start_doy & doy &lt;= mig_end_doy ~ \"mig\",\n                           TRUE ~ \"ambig\")) |&gt;\n  # Calculate migrant statistics by year and state\n  group_by(year, state) |&gt;\n  summarize(pop_min = min(count), \n            pop_max = max(count), \n            pop_median = median(count), \n            pop_mean = mean(count),\n            pop_total = sum(count), \n            .groups = \"drop\") |&gt;\n  pivot_longer(-c(year, state), names_to = \"stat\", values_to = \"value\") |&gt;\n  # Omit stats we don't care about\n  filter(!(state %in% c(\"ambig\", \"res\") & stat == \"pop_total\")) |&gt;\n  pivot_wider(names_from = c(\"state\", \"stat\")) |&gt;\n  relocate(contains(\"ambig\"), .after = last_col())\n\n# General stats on raw migrant counts\nraw_size &lt;- v |&gt;\n  filter(year != \"2007\") |&gt;\n  left_join(passage, by = \"year\") |&gt;\n  mutate(state = case_when(doy &lt; resident_date ~ \"res\",\n                           doy &gt;= mig_start_doy & doy &lt;= mig_end_doy ~ \"mig\",\n                           TRUE ~ \"ambig\")) |&gt;\n  group_by(year, state) |&gt;\n  summarize(raw_min = min(count, na.rm = TRUE), \n            raw_max = max(count, na.rm = TRUE), \n            raw_median = median(count, na.rm = TRUE), \n            raw_mean = mean(count, na.rm = TRUE), \n            raw_total = sum(count, na.rm = TRUE),\n            .groups = \"drop\") |&gt;\n  pivot_longer(-c(year, state), names_to = \"stat\", values_to = \"value\") |&gt;\n  # Omit stats we don't care about\n  filter(!(state %in% c(\"ambig\", \"res\") & stat == \"raw_total\")) |&gt;\n  pivot_wider(names_from = c(\"state\", \"stat\")) |&gt;\n  relocate(contains(\"ambig\"), .after = last_col())\n\n# General stats on peak-migration counts\npeak_size &lt;- pred |&gt;\n  left_join(passage, by = \"year\") |&gt;\n  filter(doy &gt;= peak_start_doy & doy &lt;= peak_end_doy) |&gt;\n  group_by(year) |&gt;\n  summarize(peak_pop_min = min(count), \n            peak_pop_max = max(count), \n            peak_pop_median = median(count), \n            peak_pop_mean = mean(count),\n            peak_pop_total = sum(count), \n            .groups = \"drop\")\n\npeak_raw_size &lt;- v |&gt;\n  filter(year != \"2007\") |&gt;\n  left_join(passage, by = \"year\") |&gt;\n  mutate(abs_raw_max = max(count, na.rm = TRUE), .by = \"year\") |&gt;\n  filter(doy &gt;= peak_start_doy & doy &lt;= peak_end_doy) |&gt;\n  group_by(year, abs_raw_max) |&gt;\n  summarize(peak_raw_min = min(count, na.rm = TRUE), \n            peak_raw_max = max(count, na.rm = TRUE), \n            peak_raw_median = median(count, na.rm = TRUE), \n            peak_raw_mean = mean(count, na.rm = TRUE), \n            peak_raw_total = sum(count, na.rm = TRUE),\n            .groups = \"drop\")\n\npop_size &lt;- left_join(peak_size, pop_size, by = \"year\") |&gt;\n  left_join(raw_size, by = \"year\") |&gt;\n  left_join(peak_raw_size, by = \"year\") |&gt;\n  # Round to 1 decimal place\n  mutate(across(matches(\"pop|raw\"), \\(x) round(x, 1)))\n\n# Verify that max predicted populations are the same for peak and mig \n# Verify that max raw populations are the same for overall and mig (otherwise\n#  we have missed the peak migration period...)\n#\n# Then remove the duplication - Keep one max count for predicted, one for raw\n\npop_size &lt;- pop_size |&gt;\n  verify(mig_pop_max == peak_pop_max) |&gt;\n  verify(mig_raw_max == abs_raw_max) |&gt;\n  select(-peak_pop_max, -abs_raw_max, -peak_raw_max)\n\n\n\n\ngt(pop_size)\n\n\n\n\n\n\n\nyear\npeak_pop_min\npeak_pop_median\npeak_pop_mean\npeak_pop_total\nmig_pop_min\nmig_pop_max\nmig_pop_median\nmig_pop_mean\nmig_pop_total\nres_pop_min\nres_pop_max\nres_pop_median\nres_pop_mean\nambig_pop_min\nambig_pop_max\nambig_pop_median\nambig_pop_mean\nmig_raw_min\nmig_raw_max\nmig_raw_median\nmig_raw_mean\nmig_raw_total\nres_raw_min\nres_raw_max\nres_raw_median\nres_raw_mean\nambig_raw_min\nambig_raw_max\nambig_raw_median\nambig_raw_mean\npeak_raw_min\npeak_raw_median\npeak_raw_mean\npeak_raw_total\n\n\n\n\n1999\n450.7\n561.9\n539.4\n4854.2\n114.3\n608.0\n356.2\n362.9\n7984.7\n3.7\n8.4\n4.6\n5.5\n4.8\n99.8\n15.2\n27.1\n65\n1100\n370.0\n452.2\n6783\n0\n34\n5.0\n5.8\n3\n89\n10.0\n21.6\n120\n635.0\n661.6\n5293\n\n\n2000\n98.5\n130.4\n127.0\n1523.5\n28.3\n141.1\n88.5\n88.7\n2572.6\n1.1\n4.6\n2.5\n2.5\n1.8\n28.3\n8.2\n10.7\n1\n350\n46.0\n102.4\n2355\n0\n7\n2.0\n2.7\n0\n32\n4.0\n8.7\n22\n105.0\n128.7\n1287\n\n\n2001\n133.8\n190.9\n183.5\n1835.3\n35.0\n205.9\n113.6\n116.7\n3149.6\n3.7\n14.1\n7.4\n8.1\n2.1\n47.0\n14.0\n16.8\n2\n310\n90.0\n125.2\n2879\n0\n29\n8.0\n8.2\n2\n50\n14.0\n21.9\n57\n180.0\n193.6\n1742\n\n\n2002\n123.5\n202.4\n193.7\n2324.1\n64.8\n246.7\n123.5\n143.1\n4149.5\n3.1\n5.7\n4.4\n4.3\n5.9\n56.6\n15.6\n21.3\n45\n475\n122.0\n164.9\n3793\n0\n16\n4.0\n4.3\n4\n64\n15.5\n21.1\n70\n200.0\n223.9\n2687\n\n\n2003\n232.0\n293.9\n286.2\n3147.7\n82.9\n315.5\n221.8\n212.6\n5528.4\n3.8\n5.8\n4.7\n4.7\n5.2\n71.8\n12.9\n22.3\n1\n520\n208.0\n222.7\n5567\n0\n20\n4.0\n4.9\n2\n105\n8.0\n22.1\n1\n280.0\n265.9\n2925\n\n\n2004\n324.2\n496.4\n473.6\n5209.2\n92.4\n549.3\n291.5\n313.1\n8453.3\n2.9\n5.1\n4.3\n4.0\n5.2\n117.2\n16.7\n33.4\n0\n760\n300.0\n336.9\n8086\n0\n12\n4.0\n3.9\n2\n300\n9.5\n35.0\n200\n600.0\n523.6\n5760\n\n\n2005\n206.1\n256.1\n253.4\n2787.4\n58.5\n286.1\n164.2\n170.2\n4766.8\n3.8\n5.9\n4.3\n4.6\n4.4\n53.2\n16.3\n20.6\n1\n450\n150.0\n178.6\n4643\n0\n23\n4.0\n4.7\n1\n95\n16.0\n21.7\n150\n330.0\n299.5\n2995\n\n\n2006\n246.6\n312.6\n306.0\n3671.9\n98.0\n342.1\n233.3\n229.8\n5974.8\n2.2\n4.4\n3.2\n3.3\n2.6\n79.0\n11.9\n22.7\n21\n458\n240.0\n225.8\n4741\n0\n11\n2.0\n3.3\n0\n270\n8.0\n27.2\n69\n337.5\n318.2\n3182\n\n\n2008\n174.4\n232.5\n227.0\n2497.4\n43.2\n255.0\n156.7\n156.5\n4068.7\n0.9\n3.4\n1.5\n1.8\n0.8\n59.0\n8.9\n14.4\n1\n650\n73.5\n161.9\n3238\n0\n11\n1.0\n1.8\n0\n244\n6.5\n24.4\n25\n250.0\n281.3\n2532\n\n\n2009\n340.9\n408.5\n402.0\n2814.0\n86.8\n442.0\n281.1\n279.0\n4743.6\n2.6\n4.4\n3.4\n3.4\n3.0\n76.3\n5.1\n13.6\n2\n600\n306.0\n324.8\n4872\n0\n7\n3.0\n3.5\n1\n150\n5.0\n13.5\n150\n450.0\n409.4\n2866\n\n\n2010\n437.6\n562.8\n542.7\n4884.5\n138.9\n608.8\n391.9\n386.4\n8114.9\n1.5\n7.6\n6.3\n5.7\n2.9\n106.1\n12.1\n26.4\n69\n1334\n290.0\n443.3\n7093\n0\n18\n5.0\n5.8\n0\n185\n14.0\n26.6\n101\n255.0\n540.4\n4864\n\n\n2011\n233.2\n326.7\n316.2\n3478.1\n92.1\n359.8\n241.3\n237.1\n5691.0\n3.2\n5.3\n3.7\n3.8\n4.8\n75.3\n8.7\n19.4\n10\n634\n162.0\n217.0\n2387\n1\n8\n4.0\n3.8\n0\n90\n9.0\n19.0\n255\n337.5\n337.5\n675\n\n\n2012\n239.9\n300.4\n293.9\n2939.1\n57.5\n330.3\n185.6\n192.3\n4999.2\n3.0\n5.5\n4.2\n4.2\n4.2\n54.0\n14.6\n18.7\n13\n466\n142.5\n200.1\n4803\n0\n12\n4.0\n4.4\n1\n64\n8.0\n17.4\n127\n380.0\n356.8\n3211\n\n\n2013\n238.5\n288.8\n283.1\n3397.3\n87.7\n314.2\n215.5\n211.9\n5509.0\n3.4\n8.5\n5.5\n6.0\n3.3\n80.0\n14.3\n25.0\n1\n588\n170.0\n190.5\n3239\n0\n16\n5.0\n6.1\n0\n138\n10.0\n29.8\n113\n256.0\n311.6\n1558\n\n\n2014\n226.8\n293.1\n286.7\n3440.2\n63.0\n326.8\n187.9\n195.2\n5659.4\n2.5\n3.6\n2.7\n2.9\n3.3\n66.6\n15.3\n23.0\n2\n950\n164.5\n229.0\n5497\n0\n6\n3.0\n2.9\n0\n87\n6.0\n26.6\n7\n350.0\n383.5\n4218\n\n\n2015\n192.8\n301.2\n288.9\n2888.9\n59.7\n339.5\n135.6\n165.4\n5128.2\n3.5\n5.3\n4.7\n4.6\n5.0\n59.3\n14.6\n24.9\n2\n850\n75.0\n197.1\n4927\n0\n18\n4.0\n4.8\n1\n110\n9.5\n20.6\n5\n250.0\n390.2\n3512\n\n\n2016\n369.8\n474.0\n462.6\n5551.5\n98.8\n515.8\n325.9\n322.6\n9032.5\n6.1\n11.8\n9.4\n9.2\n11.9\n111.1\n33.8\n40.3\n39\n1050\n328.0\n340.7\n7495\n1\n22\n8.0\n8.9\n5\n80\n13.0\n23.5\n210\n450.0\n507.1\n4564\n\n\n2017\n362.3\n488.2\n468.5\n4216.4\n110.7\n524.0\n344.5\n337.6\n6751.6\n3.3\n8.0\n5.2\n5.1\n2.4\n112.0\n10.9\n24.8\n75\n594\n339.0\n365.1\n4016\n1\n15\n5.0\n5.4\n2\n100\n17.0\n29.7\n253\n500.0\n463.6\n2318\n\n\n2018\n280.2\n318.2\n313.7\n3764.8\n110.1\n329.0\n273.8\n249.9\n6497.2\n5.7\n13.4\n11.3\n10.6\n12.8\n90.3\n21.0\n32.2\n1\n680\n245.5\n259.6\n5712\n2\n32\n8.0\n10.3\n5\n85\n17.5\n26.2\n1\n245.0\n248.2\n2234\n\n\n2019\n288.2\n350.5\n343.3\n3432.8\n103.8\n380.1\n262.3\n258.6\n5430.3\n6.6\n11.1\n8.0\n8.4\n1.2\n86.4\n11.0\n21.4\n1\n800\n236.5\n271.6\n5433\n1\n30\n8.0\n8.5\n0\n147\n12.0\n23.8\n185\n265.0\n365.8\n3658\n\n\n2020\n521.8\n657.9\n643.9\n6439.0\n144.0\n711.7\n463.8\n450.9\n11271.8\n4.8\n13.4\n11.8\n10.8\n12.6\n154.7\n24.8\n43.3\n29\n1050\n400.0\n502.0\n11546\n0\n36\n8.5\n10.5\n3\n138\n19.5\n28.8\n240\n850.0\n710.5\n7105\n\n\n2021\n428.4\n512.2\n502.6\n5026.1\n118.3\n539.5\n387.0\n367.1\n8810.3\n6.8\n13.2\n9.1\n9.3\n4.1\n144.0\n20.4\n36.0\n7\n1200\n329.0\n417.0\n8340\n0\n21\n8.0\n9.2\n0\n269\n16.0\n37.7\n15\n442.0\n462.2\n4622\n\n\n2022\n955.3\n1224.4\n1184.4\n10659.7\n304.2\n1304.2\n880.4\n857.4\n18005.7\n3.0\n8.5\n7.2\n6.4\n7.4\n233.4\n21.8\n52.4\n175\n2375\n860.0\n914.0\n18280\n0\n17\n6.0\n6.3\n3\n250\n20.5\n43.9\n520\n1100.0\n1158.3\n10425\n\n\n2023\n478.4\n595.4\n577.3\n6350.6\n141.5\n639.5\n401.1\n401.7\n11247.8\n4.4\n8.9\n7.1\n7.2\n6.7\n134.5\n41.1\n47.6\n1\n1700\n257.0\n437.7\n11818\n1\n18\n5.5\n7.2\n0\n131\n14.0\n32.0\n7\n261.0\n648.1\n7129\n\n\n\n\n\n\n\n\n\n\n\n\n7. Skewness & Kurtosis\nTo capture the pattern of migration, we calculate skewness (of particular interest) and kurtosis (not necessarily of interest, but included for completeness).\nNote: We substract 3 from kurtosis to make it a measure of Excess Kurtosis which is centred on zero. A normal distribution has a kurtosis 3, but an excess kurtosis of 0.\n\n\n\n\n\n\nWhy the skew_all?\nSkewness and kurtosis will differ depending on the range of data we include.\nIf for example, we looked at the entire pattern of migration (including the very long tail of pre-migration data), we’d probably calculate that the distribution was very left-skewed and the skew would be negative.\nSo we need to look just at the migration period itself.\nBut if we use the 5-95% dates, which are good cutoffs, we might end up influencing kurtosis which is affected by how thick or thin the tails are. Any truncating will make those tails seem thicker.\nSo what do we do?\nI suggest an additional period to look at. In this period, we will choose the date of the 50% of passage (i.e. when 50% of the birds have passed, p50_doy) as the center of the distribution. Then we’ll count symmetric dates out to either side of that to ‘cut’ out the migration pattern. Because we’re truncated on the fall side, we’ll count the number of days from p50_doy to the end (p50_to_end), and for each year will include the dates between p50_doy - p50_to_end and the end of the date range (which is also p50_doy + p50_to_end).\nThis snap shot should give us a good look at skew, for sure, and possibly a look at kurtosis. Although if we find negative kurtosis, it might be that the date range isn’t long enough.\n\n\n\n\nskew_all &lt;- dts |&gt;\n  filter(measure %in% \"p50\") |&gt;\n  select(year, measure, doy_passage) |&gt;\n  pivot_wider(names_from = \"measure\", values_from = \"doy_passage\") |&gt;\n  left_join(x = pred, y = _, by = \"year\") |&gt;\n  mutate(p50_to_end = n_distinct(doy[doy &gt;= p50]), .by = \"year\") |&gt;\n  filter(doy &gt;= p50 - p50_to_end) |&gt;\n  summarize(doy = list(rep(doy, round(count))), .by = c(\"year\", \"p50_to_end\")) |&gt;\n  mutate(all_skew = map_dbl(doy, skewness),\n         all_kurt = map_dbl(doy, \\(x) kurtosis(x) - 3)) |&gt;\n  select(-doy)\n\n\nskew_mig &lt;- dts |&gt;\n  filter(measure %in% c(\"p05\", \"p95\")) |&gt;\n  select(year, measure, doy_passage) |&gt;\n  pivot_wider(names_from = \"measure\", values_from = \"doy_passage\") |&gt;\n  left_join(x = pred, y = _, by = \"year\") |&gt;\n  filter(doy &gt;= p05 & doy &lt;= p95) |&gt;\n  summarize(doy = list(rep(doy, round(count))), .by = \"year\") |&gt;\n  mutate(mig_skew = map_dbl(doy, skewness),\n         mig_kurt = map_dbl(doy, \\(x) kurtosis(x) - 3)) |&gt;\n  select(-doy)\n\nskew_peak &lt;- dts |&gt;\n  filter(measure %in% c(\"p25\", \"p75\")) |&gt;\n  select(year, measure, doy_passage) |&gt;\n  pivot_wider(names_from = \"measure\", values_from = \"doy_passage\") |&gt;\n  left_join(x = pred, y = _, by = \"year\") |&gt;\n  filter(doy &gt;= p25 & doy &lt;= p75) |&gt;\n  summarize(doy = list(rep(doy, round(count))), .by = \"year\") |&gt;\n  mutate(peak_skew = map_dbl(doy, skewness),\n         peak_kurt = map_dbl(doy, \\(x) kurtosis(x) - 3)) |&gt;\n  select(-doy)\n\n\n\nCombine metrics\nJoin together the sample sizes, the passage dates/durations as well as the population sizes, and migration patterns (skew/kurtosis).\n\nCalculatePreview\n\n\n\nfinal &lt;- left_join(samples, passage, by = \"year\") |&gt;\n  left_join(pop_size, by = \"year\") |&gt;\n  left_join(skew_mig, by = \"year\") |&gt;\n  left_join(skew_peak, by = \"year\") |&gt;\n  left_join(skew_all, by = \"year\")\n\n\n\n\ngt(final)\n\n\n\n\n\n\n\nyear\ndate_min\ndate_max\nn_dates_obs\nn_dates\nn_obs\nmig_start_doy\nmig_end_doy\npeak_start_doy\npeak_end_doy\np50_doy\nmax_doy\nmig_dur_days\npeak_dur_days\npeak_pop_min\npeak_pop_median\npeak_pop_mean\npeak_pop_total\nmig_pop_min\nmig_pop_max\nmig_pop_median\nmig_pop_mean\nmig_pop_total\nres_pop_min\nres_pop_max\nres_pop_median\nres_pop_mean\nambig_pop_min\nambig_pop_max\nambig_pop_median\nambig_pop_mean\nmig_raw_min\nmig_raw_max\nmig_raw_median\nmig_raw_mean\nmig_raw_total\nres_raw_min\nres_raw_max\nres_raw_median\nres_raw_mean\nambig_raw_min\nambig_raw_max\nambig_raw_median\nambig_raw_mean\npeak_raw_min\npeak_raw_median\npeak_raw_mean\npeak_raw_total\nmig_skew\nmig_kurt\npeak_skew\npeak_kurt\np50_to_end\nall_skew\nall_kurt\n\n\n\n\n1999\n1999-07-25\n1999-10-20\n64\n87\n7397\n261\n282\n267\n275\n271\n271\n21\n8\n450.7\n561.9\n539.4\n4854.2\n114.3\n608.0\n356.2\n362.9\n7984.7\n3.7\n8.4\n4.6\n5.5\n4.8\n99.8\n15.2\n27.1\n65\n1100\n370.0\n452.2\n6783\n0\n34\n5.0\n5.8\n3\n89\n10.0\n21.6\n120\n635.0\n661.6\n5293\n0.0668830493\n-0.6284030\n-0.0006279026\n-1.123637\n24\n0.018359875\n0.77951212\n\n\n2000\n2000-07-23\n2000-10-18\n76\n87\n2623\n253\n281\n262\n273\n268\n269\n28\n11\n98.5\n130.4\n127.0\n1523.5\n28.3\n141.1\n88.5\n88.7\n2572.6\n1.1\n4.6\n2.5\n2.5\n1.8\n28.3\n8.2\n10.7\n1\n350\n46.0\n102.4\n2355\n0\n7\n2.0\n2.7\n0\n32\n4.0\n8.7\n22\n105.0\n128.7\n1287\n-0.1341947086\n-0.6873657\n-0.0717510756\n-1.124343\n28\n-0.207383405\n0.33588771\n\n\n2001\n2001-07-23\n2001-10-07\n64\n76\n3366\n254\n280\n265\n274\n270\n271\n26\n9\n133.8\n190.9\n183.5\n1835.3\n35.0\n205.9\n113.6\n116.7\n3149.6\n3.7\n14.1\n7.4\n8.1\n2.1\n47.0\n14.0\n16.8\n2\n310\n90.0\n125.2\n2879\n0\n29\n8.0\n8.2\n2\n50\n14.0\n21.9\n57\n180.0\n193.6\n1742\n-0.4442868130\n-0.3458484\n-0.0975276812\n-1.118906\n25\n-0.465411859\n0.40488924\n\n\n2002\n2002-07-23\n2002-10-21\n83\n90\n4454\n257\n285\n264\n275\n268\n266\n28\n11\n123.5\n202.4\n193.7\n2324.1\n64.8\n246.7\n123.5\n143.1\n4149.5\n3.1\n5.7\n4.4\n4.3\n5.9\n56.6\n15.6\n21.3\n45\n475\n122.0\n164.9\n3793\n0\n16\n4.0\n4.3\n4\n64\n15.5\n21.1\n70\n200.0\n223.9\n2687\n0.3820434010\n-0.7249736\n0.2880288071\n-1.033750\n27\n0.152286748\n-0.01868869\n\n\n2003\n2003-07-23\n2003-10-18\n83\n87\n6229\n259\n284\n266\n276\n271\n269\n25\n10\n232.0\n293.9\n286.2\n3147.7\n82.9\n315.5\n221.8\n212.6\n5528.4\n3.8\n5.8\n4.7\n4.7\n5.2\n71.8\n12.9\n22.3\n1\n520\n208.0\n222.7\n5567\n0\n20\n4.0\n4.9\n2\n105\n8.0\n22.1\n1\n280.0\n265.9\n2925\n0.1312336371\n-0.8131903\n0.0910576817\n-1.149585\n24\n0.076608106\n-0.05455030\n\n\n2004\n2004-07-23\n2004-10-18\n80\n87\n9052\n262\n288\n268\n278\n273\n272\n26\n10\n324.2\n496.4\n473.6\n5209.2\n92.4\n549.3\n291.5\n313.1\n8453.3\n2.9\n5.1\n4.3\n4.0\n5.2\n117.2\n16.7\n33.4\n0\n760\n300.0\n336.9\n8086\n0\n12\n4.0\n3.9\n2\n300\n9.5\n35.0\n200\n600.0\n523.6\n5760\n0.3783186957\n-0.4418543\n0.1210075333\n-1.082930\n23\n0.371925923\n0.21956496\n\n\n2005\n2005-07-23\n2005-10-18\n83\n87\n5267\n256\n283\n265\n275\n270\n270\n27\n10\n206.1\n256.1\n253.4\n2787.4\n58.5\n286.1\n164.2\n170.2\n4766.8\n3.8\n5.9\n4.3\n4.6\n4.4\n53.2\n16.3\n20.6\n1\n450\n150.0\n178.6\n4643\n0\n23\n4.0\n4.7\n1\n95\n16.0\n21.7\n150\n330.0\n299.5\n2995\n-0.0292359035\n-0.6328078\n0.0091185677\n-1.112292\n25\n-0.042300644\n0.19578706\n\n\n2006\n2006-07-23\n2006-10-17\n73\n86\n5317\n257\n282\n264\n275\n270\n270\n25\n11\n246.6\n312.6\n306.0\n3671.9\n98.0\n342.1\n233.3\n229.8\n5974.8\n2.2\n4.4\n3.2\n3.3\n2.6\n79.0\n11.9\n22.7\n21\n458\n240.0\n225.8\n4741\n0\n11\n2.0\n3.3\n0\n270\n8.0\n27.2\n69\n337.5\n318.2\n3182\n-0.0378821764\n-0.8003038\n-0.0289715867\n-1.119078\n25\n-0.065892687\n-0.08516749\n\n\n2008\n2008-07-23\n2008-10-17\n59\n86\n3761\n256\n281\n265\n275\n270\n271\n25\n10\n174.4\n232.5\n227.0\n2497.4\n43.2\n255.0\n156.7\n156.5\n4068.7\n0.9\n3.4\n1.5\n1.8\n0.8\n59.0\n8.9\n14.4\n1\n650\n73.5\n161.9\n3238\n0\n11\n1.0\n1.8\n0\n244\n6.5\n24.4\n25\n250.0\n281.3\n2532\n-0.2290201422\n-0.6202011\n-0.0631823253\n-1.117553\n26\n-0.344749439\n0.36177342\n\n\n2009\n2009-07-23\n2009-10-18\n62\n87\n5257\n261\n277\n266\n272\n269\n269\n16\n6\n340.9\n408.5\n402.0\n2814.0\n86.8\n442.0\n281.1\n279.0\n4743.6\n2.6\n4.4\n3.4\n3.4\n3.0\n76.3\n5.1\n13.6\n2\n600\n306.0\n324.8\n4872\n0\n7\n3.0\n3.5\n1\n150\n5.0\n13.5\n150\n450.0\n409.4\n2866\n-0.0390772469\n-0.7064636\n-0.0241616303\n-1.165297\n26\n-0.183855706\n2.55542917\n\n\n2010\n2010-07-23\n2010-10-18\n75\n87\n7956\n262\n282\n268\n276\n272\n273\n20\n8\n437.6\n562.8\n542.7\n4884.5\n138.9\n608.8\n391.9\n386.4\n8114.9\n1.5\n7.6\n6.3\n5.7\n2.9\n106.1\n12.1\n26.4\n69\n1334\n290.0\n443.3\n7093\n0\n18\n5.0\n5.8\n0\n185\n14.0\n26.6\n101\n255.0\n540.4\n4864\n-0.0646430642\n-0.7398209\n-0.0827335380\n-1.138708\n23\n-0.172021480\n0.54001217\n\n\n2011\n2011-07-24\n2011-10-20\n69\n88\n3032\n263\n286\n270\n280\n276\n277\n23\n10\n233.2\n326.7\n316.2\n3478.1\n92.1\n359.8\n241.3\n237.1\n5691.0\n3.2\n5.3\n3.7\n3.8\n4.8\n75.3\n8.7\n19.4\n10\n634\n162.0\n217.0\n2387\n1\n8\n4.0\n3.8\n0\n90\n9.0\n19.0\n255\n337.5\n337.5\n675\n-0.1925099346\n-0.7804020\n-0.1296424247\n-1.116217\n19\n-0.171830608\n-0.33197475\n\n\n2012\n2012-07-23\n2012-10-18\n78\n87\n5327\n259\n284\n267\n276\n272\n272\n25\n9\n239.9\n300.4\n293.9\n2939.1\n57.5\n330.3\n185.6\n192.3\n4999.2\n3.0\n5.5\n4.2\n4.2\n4.2\n54.0\n14.6\n18.7\n13\n466\n142.5\n200.1\n4803\n0\n12\n4.0\n4.4\n1\n64\n8.0\n17.4\n127\n380.0\n356.8\n3211\n-0.0299617741\n-0.5927384\n-0.0217424582\n-1.121322\n24\n0.041036319\n0.45519209\n\n\n2013\n2013-07-23\n2013-10-18\n69\n87\n4006\n262\n287\n269\n280\n274\n275\n25\n11\n238.5\n288.8\n283.1\n3397.3\n87.7\n314.2\n215.5\n211.9\n5509.0\n3.4\n8.5\n5.5\n6.0\n3.3\n80.0\n14.3\n25.0\n1\n588\n170.0\n190.5\n3239\n0\n16\n5.0\n6.1\n0\n138\n10.0\n29.8\n113\n256.0\n311.6\n1558\n0.0008979828\n-0.7952244\n-0.0135491805\n-1.126270\n21\n0.002994512\n-0.26920320\n\n\n2014\n2014-07-23\n2014-10-18\n74\n87\n6021\n260\n288\n269\n280\n274\n274\n28\n11\n226.8\n293.1\n286.7\n3440.2\n63.0\n326.8\n187.9\n195.2\n5659.4\n2.5\n3.6\n2.7\n2.9\n3.3\n66.6\n15.3\n23.0\n2\n950\n164.5\n229.0\n5497\n0\n6\n3.0\n2.9\n0\n87\n6.0\n26.6\n7\n350.0\n383.5\n4218\n-0.0183349776\n-0.6354133\n0.0226728004\n-1.100426\n21\n-0.011381679\n-0.17246145\n\n\n2015\n2015-07-23\n2015-10-18\n77\n87\n5428\n259\n289\n267\n276\n271\n270\n30\n9\n192.8\n301.2\n288.9\n2888.9\n59.7\n339.5\n135.6\n165.4\n5128.2\n3.5\n5.3\n4.7\n4.6\n5.0\n59.3\n14.6\n24.9\n2\n850\n75.0\n197.1\n4927\n0\n18\n4.0\n4.8\n1\n110\n9.5\n20.6\n5\n250.0\n390.2\n3512\n0.5067799293\n-0.1280714\n0.1303439237\n-1.071614\n24\n0.381640565\n0.23945515\n\n\n2016\n2016-07-23\n2016-10-18\n68\n87\n8137\n258\n285\n265\n276\n270\n271\n27\n11\n369.8\n474.0\n462.6\n5551.5\n98.8\n515.8\n325.9\n322.6\n9032.5\n6.1\n11.8\n9.4\n9.2\n11.9\n111.1\n33.8\n40.3\n39\n1050\n328.0\n340.7\n7495\n1\n22\n8.0\n8.9\n5\n80\n13.0\n23.5\n210\n450.0\n507.1\n4564\n0.1346850652\n-0.6650986\n0.0115481450\n-1.118977\n26\n0.104842051\n0.29293811\n\n\n2017\n2017-07-23\n2017-10-17\n62\n86\n4827\n261\n280\n267\n275\n271\n272\n19\n8\n362.3\n488.2\n468.5\n4216.4\n110.7\n524.0\n344.5\n337.6\n6751.6\n3.3\n8.0\n5.2\n5.1\n2.4\n112.0\n10.9\n24.8\n75\n594\n339.0\n365.1\n4016\n1\n15\n5.0\n5.4\n2\n100\n17.0\n29.7\n253\n500.0\n463.6\n2318\n-0.1592480069\n-0.7158155\n-0.0795803763\n-1.132266\n24\n-0.273144664\n0.73703862\n\n\n2018\n2018-07-23\n2018-10-18\n75\n87\n6672\n260\n285\n267\n278\n272\n270\n25\n11\n280.2\n318.2\n313.7\n3764.8\n110.1\n329.0\n273.8\n249.9\n6497.2\n5.7\n13.4\n11.3\n10.6\n12.8\n90.3\n21.0\n32.2\n1\n680\n245.5\n259.6\n5712\n2\n32\n8.0\n10.3\n5\n85\n17.5\n26.2\n1\n245.0\n248.2\n2234\n0.0535846199\n-0.9329652\n0.0419922170\n-1.188355\n23\n0.006998443\n-0.24641922\n\n\n2019\n2019-07-23\n2019-10-18\n87\n87\n6476\n262\n282\n267\n276\n271\n271\n20\n9\n288.2\n350.5\n343.3\n3432.8\n103.8\n380.1\n262.3\n258.6\n5430.3\n6.6\n11.1\n8.0\n8.4\n1.2\n86.4\n11.0\n21.4\n1\n800\n236.5\n271.6\n5433\n1\n30\n8.0\n8.5\n0\n147\n12.0\n23.8\n185\n265.0\n365.8\n3658\n0.0723129060\n-0.8107342\n0.0269621604\n-1.136540\n24\n-0.254477296\n0.55165823\n\n\n2020\n2020-07-23\n2020-10-18\n81\n87\n12595\n264\n288\n272\n281\n277\n277\n24\n9\n521.8\n657.9\n643.9\n6439.0\n144.0\n711.7\n463.8\n450.9\n11271.8\n4.8\n13.4\n11.8\n10.8\n12.6\n154.7\n24.8\n43.3\n29\n1050\n400.0\n502.0\n11546\n0\n36\n8.5\n10.5\n3\n138\n19.5\n28.8\n240\n850.0\n710.5\n7105\n-0.1108202356\n-0.7095091\n-0.0474589870\n-1.136934\n19\n-0.064047515\n-0.20573570\n\n\n2021\n2021-07-23\n2021-10-18\n82\n87\n9652\n258\n281\n266\n275\n271\n271\n23\n9\n428.4\n512.2\n502.6\n5026.1\n118.3\n539.5\n387.0\n367.1\n8810.3\n6.8\n13.2\n9.1\n9.3\n4.1\n144.0\n20.4\n36.0\n7\n1200\n329.0\n417.0\n8340\n0\n21\n8.0\n9.2\n0\n269\n16.0\n37.7\n15\n442.0\n462.2\n4622\n-0.1449912074\n-0.7812370\n-0.0393816072\n-1.162953\n24\n-0.192544659\n0.15413079\n\n\n2022\n2022-07-23\n2022-10-18\n86\n87\n19826\n265\n285\n271\n279\n275\n276\n20\n8\n955.3\n1224.4\n1184.4\n10659.7\n304.2\n1304.2\n880.4\n857.4\n18005.7\n3.0\n8.5\n7.2\n6.4\n7.4\n233.4\n21.8\n52.4\n175\n2375\n860.0\n914.0\n18280\n0\n17\n6.0\n6.3\n3\n250\n20.5\n43.9\n520\n1100.0\n1158.3\n10425\n-0.0926814829\n-0.7666577\n-0.0800418285\n-1.151993\n20\n-0.204737359\n0.10719914\n\n\n2023\n2023-07-23\n2023-10-15\n84\n84\n12749\n259\n286\n268\n278\n273\n274\n27\n10\n478.4\n595.4\n577.3\n6350.6\n141.5\n639.5\n401.1\n401.7\n11247.8\n4.4\n8.9\n7.1\n7.2\n6.7\n134.5\n41.1\n47.6\n1\n1700\n257.0\n437.7\n11818\n1\n18\n5.5\n7.2\n0\n131\n14.0\n32.0\n7\n261.0\n648.1\n7129\n-0.0673564649\n-0.7132596\n-0.0361381179\n-1.130459\n22\n-0.012238180\n-0.14218174",
    "crumbs": [
      "Workflow",
      "Calculate Metrics"
    ]
  },
  {
    "objectID": "02_calculate_metrics.html#extra---consider-gaps",
    "href": "02_calculate_metrics.html#extra---consider-gaps",
    "title": "Calculate Metrics",
    "section": "Extra - Consider gaps",
    "text": "Extra - Consider gaps\nIn the figures below, we can see that there is sometimes a significant gap in observation right before the ‘start’ of migration.\nAlthough migration start is calculated from the GAM, it may be worth considering any patterns in these gaps incase they influence the start of migration (i.e. if there are more missing dates, is the start later?).\nTherefore, let’s calculate how many missing dates there are in the two weeks before the predicted start of migration. We should be able to compare models with and without this to see if it has an effect on our analysis.\n\nCalculatePreview\n\n\n\nmissing &lt;- v |&gt;\n  left_join(select(final, year, mig_start_doy), by = \"year\") |&gt;\n  group_by(year, mig_start_doy) |&gt;\n  summarize(n_missing = sum(is.na(count[doy &lt; mig_start_doy & doy &gt;= mig_start_doy - 14])), \n            .groups = \"drop\")\n\nfinal &lt;- left_join(final, missing, by = c(\"year\", \"mig_start_doy\"))\n\n\n\n\ngt(missing)\n\n\n\n\n\n\n\nyear\nmig_start_doy\nn_missing\n\n\n\n\n1999\n261\n4\n\n\n2000\n253\n1\n\n\n2001\n254\n3\n\n\n2002\n257\n2\n\n\n2003\n259\n0\n\n\n2004\n262\n1\n\n\n2005\n256\n0\n\n\n2006\n257\n7\n\n\n2007\nNA\n91\n\n\n2008\n256\n2\n\n\n2009\n261\n4\n\n\n2010\n262\n3\n\n\n2011\n263\n1\n\n\n2012\n259\n2\n\n\n2013\n262\n5\n\n\n2014\n260\n4\n\n\n2015\n259\n4\n\n\n2016\n258\n7\n\n\n2017\n261\n5\n\n\n2018\n260\n0\n\n\n2019\n262\n0\n\n\n2020\n264\n0\n\n\n2021\n258\n0\n\n\n2022\n265\n1\n\n\n2023\n259\n0",
    "crumbs": [
      "Workflow",
      "Calculate Metrics"
    ]
  },
  {
    "objectID": "02_calculate_metrics.html#data",
    "href": "02_calculate_metrics.html#data",
    "title": "Calculate Metrics",
    "section": "Data",
    "text": "Data\nWe’ll save the models and calculated metrics for use later.\nFinal is our main data, GAMS and Cumulative Counts are for if we need to refer to the intermediate steps.\n\nwrite_csv(final, \"Data/Datasets/vultures_final.csv\")\nwrite_rds(gams, \"Data/Datasets/vultures_gams.rds\")\nwrite_csv(pred, \"Data/Datasets/vultures_gams_pred.csv\")\nwrite_csv(cum_counts, \"Data/Datasets/vultures_cumulative_counts.csv\")\n\n\nDetails\nData are organized as observations per year.\nGeneral\n\nyear - Year of the data\ndate_min - First date with an observation\ndate_max - Last date with an observation\nn_dates_obs - Number of dates with a count\nn_dates - Number of dates in the range (min to max)\nn_obs - Total number of vultures seen\n\nMigration\n\nmig/peak/res/amibig - Migration period (mig, 5%-95%) or peak migration period (peak, 25%-75%), or for population counts (below), the resident period (res, DOY &lt; 240) or the ambiguous period (ambig) that occurs after the resident period but before the start of migration and after the end of migration.\nstart_doy/end_doy - Start/end day-of-year dates for a period (e.g., mig_start_doy).\ndur_days - Duration in days of a period (e.g., peak_dur_days).\nskew/kurt - Skewness and kurtosis of the counts for a period (e.g., mig_skew, peak_kurt)\n\nPopulation Counts\n\npop/raw - Type of count, either from model predictions (pop) or raw data counts.\nmin/max/median/mean/total - Population statistic calculated (e.g, peak_pop_mean, mig_raw_min, res_pop_median, or ambig_raw_max). total means the total sum of daily counts. Note: total isn’t a sensible metric for raw data as it is dependent on the number of observation days.\n\nExtra\n\nn_missing - Number of days missing an observation in the two weeks before the start of migration.",
    "crumbs": [
      "Workflow",
      "Calculate Metrics"
    ]
  },
  {
    "objectID": "02_calculate_metrics.html#figures",
    "href": "02_calculate_metrics.html#figures",
    "title": "Calculate Metrics",
    "section": "Figures",
    "text": "Figures\nThese figures are meant to be sanity checks of the models and the Resident Date cutoff (240).\nEach year has two figures showing the model, one overlaid with the predicted counts (determined from the model), and one overlaid with the raw counts.\nThis way we could double check the calculations as well as the models.\nNote that we always expect raw counts to be greater and with more variability than predicted counts. Further, I do not include the sum of counts for the raw data as this is dependent on the number of observations and somewhat misleading.\n\nThese are not particularly important plots, but we do want to have a visual of what’s going on, if only to catch mistakes in the calculations.\n\nInterpretations\n\nBlack line - Predicted model\nGrey ribbon - 99% CI around the model\nYellow line (box) - Period defined as containing only residents (defined by date cutoff)\nPurple box - Period defined as migration period (defined by 5%-95% cumulative predicted counts), showing the min, max and median counts\nBlue box - Period defined as peak migration period (defined by 25%-75% cumulative predicted counts), showing the min, max and median counts\nSum counts - Text indicating the cumulative total number of predicted counts expected in that period.\nn days - the total number of days with observations for that year.\n\n\nCode\nfor(y in unique(v$year)) {\n  cat(\"### \", y, \" {#\", y, \"}\\n\\n\", sep = \"\")\n  if(y != \"2007\") {\n    v1 &lt;- filter(v, year == y)\n    p &lt;- filter(pred, year == y)\n    c &lt;- filter(cum_counts, year == y)\n    f &lt;- filter(final, year == y)\n    \n    g1 &lt;- plot_model(raw = v1, pred = p, final = f)\n    \n    pop1 &lt;- select(f, mig_start_doy, mig_end_doy, peak_start_doy, peak_end_doy, \n                   contains(\"pop\"), -contains(\"ambig\")) |&gt;\n      pivot_longer(everything()) |&gt;\n      mutate(type = str_extract(name, \"min|max|median|mean|total|start|end\"),\n             stage = str_extract(name, \"mig|peak|res\"),\n             stage = str_replace_all(stage, c(\"mig\" = \"Migration\", \n                                              \"peak\" = \"Peak Migration\",\n                                              \"res\" = \"Residents\"))) |&gt;\n      select(-name) |&gt;\n      pivot_wider(names_from = type) |&gt;\n      mutate(start = replace_na(start, 204),\n             end = replace_na(end, resident_date))\n    \n    pop2 &lt;- pivot_longer(pop1, cols = c(\"start\", \"end\"), values_to = \"doy\")\n    \n    \n    g1 &lt;- plot_model(raw = v1, pred = p, final = f) +\n      geom_rect(data = pop1, aes(xmin = start, xmax = end, ymin = min, ymax = max,\n                                 fill = stage, colour = stage),\n                inherit.aes = FALSE) +\n      geom_path(data = pop2, aes(x = doy, y = median, colour = stage), linewidth = 1) +\n      geom_text(data = filter(pop1, stage != \"Residents\"),\n                aes(x = (end - start)/2 + start, y = max, colour = stage,\n                    label = paste(\"Sum\", stage, \"Counts\\n\", total)), \n                nudge_y = c(-60, 65), nudge_x = c(-30, 0)) +\n      scale_fill_viridis_d(end = 0.9, alpha = 0.5) +\n      scale_colour_viridis_d(end = 0.9) +\n      labs(title = paste0(y, \" - Check dates and predicted population sizes\"),\n           caption = \"Boxes define the start date to end date (left/right), as well as population min, max, and median (bottom/top/middle)\\n'Total' refers to cumulative predicted observations\")\n    \n    \n    pop1 &lt;- select(f, mig_start_doy, mig_end_doy, peak_start_doy, peak_end_doy, \n                   contains(\"raw\"), -contains(\"ambig\")) |&gt;\n      pivot_longer(everything()) |&gt;\n      mutate(type = str_extract(name, \"min|max|median|mean|total|start|end\"),\n             stage = str_extract(name, \"mig|peak|res\"),\n             stage = str_replace_all(stage, c(\"mig\" = \"Migration\", \n                                              \"peak\" = \"Peak Migration\",\n                                              \"res\" = \"Residents\"))) |&gt;\n      select(-name) |&gt;\n      pivot_wider(names_from = type) |&gt;\n      mutate(start = replace_na(start, 204),\n             end = replace_na(end, resident_date))\n    \n    pop2 &lt;- pivot_longer(pop1, cols = c(\"start\", \"end\"), values_to = \"doy\")\n    \n    g2 &lt;- plot_model(raw = v1, pred = p, final = f) +\n      geom_rect(data = pop1, aes(xmin = start, xmax = end, ymin = min, ymax = max,\n                                 fill = stage, colour = stage),\n                inherit.aes = FALSE) +\n      geom_path(data = pop2, aes(x = doy, y = median, colour = stage), linewidth = 1) +\n      scale_fill_viridis_d(end = 0.9, alpha = 0.5) +\n      scale_colour_viridis_d(end = 0.9) +\n      labs(title = paste0(y, \" - Check dates and raw population sizes\"),\n           caption = \"Boxes define the start date to end date (left/right), as well as population min, max, and median (bottom/top/middle)\")\n    \n    cat(\":::{.panel-tabset}\\n\\n\")\n    cat(\"#### Predicted Counts\\n\\n\")\n    print(g1)\n    cat(\"\\n\\n\")\n    cat(\"#### Raw Counts\\n\\n\")\n    print(g2)\n    cat(\"\\n\\n\")\n    cat(\":::\\n\\n\")\n    \n    cat(\"\\n\\n\")\n    \n  } else cat(\"No Data for 2007\\n\\n\")\n}\n\n\n1999\n\nPredicted CountsRaw Counts\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_rect()`).\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_rect()`).\n\n\n\n\n\n\n2000\n\nPredicted CountsRaw Counts\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_rect()`).\nRemoved 1 row containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_rect()`).\n\n\n\n\n\n\n2001\n\nPredicted CountsRaw Counts\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_rect()`).\nRemoved 1 row containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_rect()`).\n\n\n\n\n\n\n2002\n\nPredicted CountsRaw Counts\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_rect()`).\nRemoved 1 row containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_rect()`).\n\n\n\n\n\n\n2003\n\nPredicted CountsRaw Counts\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_rect()`).\nRemoved 1 row containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_rect()`).\n\n\n\n\n\n\n2004\n\nPredicted CountsRaw Counts\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_rect()`).\nRemoved 1 row containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_rect()`).\n\n\n\n\n\n\n2005\n\nPredicted CountsRaw Counts\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_rect()`).\nRemoved 1 row containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_rect()`).\n\n\n\n\n\n\n2006\n\nPredicted CountsRaw Counts\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_rect()`).\nRemoved 1 row containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_rect()`).\n\n\n\n\n\n\n2007\nNo Data for 2007\n\n\n2008\n\nPredicted CountsRaw Counts\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_rect()`).\nRemoved 1 row containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_rect()`).\n\n\n\n\n\n\n2009\n\nPredicted CountsRaw Counts\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_rect()`).\nRemoved 1 row containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_rect()`).\n\n\n\n\n\n\n2010\n\nPredicted CountsRaw Counts\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_rect()`).\nRemoved 1 row containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_rect()`).\n\n\n\n\n\n\n2011\n\nPredicted CountsRaw Counts\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_rect()`).\nRemoved 1 row containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_rect()`).\n\n\n\n\n\n\n2012\n\nPredicted CountsRaw Counts\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_rect()`).\nRemoved 1 row containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_rect()`).\n\n\n\n\n\n\n2013\n\nPredicted CountsRaw Counts\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_rect()`).\nRemoved 1 row containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_rect()`).\n\n\n\n\n\n\n2014\n\nPredicted CountsRaw Counts\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_rect()`).\nRemoved 1 row containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_rect()`).\n\n\n\n\n\n\n2015\n\nPredicted CountsRaw Counts\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_rect()`).\nRemoved 1 row containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_rect()`).\n\n\n\n\n\n\n2016\n\nPredicted CountsRaw Counts\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_rect()`).\nRemoved 1 row containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_rect()`).\n\n\n\n\n\n\n2017\n\nPredicted CountsRaw Counts\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_rect()`).\nRemoved 1 row containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_rect()`).\n\n\n\n\n\n\n2018\n\nPredicted CountsRaw Counts\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_rect()`).\nRemoved 1 row containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_rect()`).\n\n\n\n\n\n\n2019\n\nPredicted CountsRaw Counts\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_rect()`).\nRemoved 1 row containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_rect()`).\n\n\n\n\n\n\n2020\n\nPredicted CountsRaw Counts\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_rect()`).\nRemoved 1 row containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_rect()`).\n\n\n\n\n\n\n2021\n\nPredicted CountsRaw Counts\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_rect()`).\nRemoved 1 row containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_rect()`).\n\n\n\n\n\n\n2022\n\nPredicted CountsRaw Counts\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_rect()`).\nRemoved 1 row containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_rect()`).\n\n\n\n\n\n\n2023\n\nPredicted CountsRaw Counts\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_rect()`).\nRemoved 1 row containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_rect()`).",
    "crumbs": [
      "Workflow",
      "Calculate Metrics"
    ]
  },
  {
    "objectID": "02_calculate_metrics.html#reproducibility",
    "href": "02_calculate_metrics.html#reproducibility",
    "title": "Calculate Metrics",
    "section": "Reproducibility",
    "text": "Reproducibility\n\n\n\n\n\n\nSession Info\n\n\n\n\n\n\ndevtools::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.5.0 (2025-04-11)\n os       Ubuntu 24.04.2 LTS\n system   x86_64, linux-gnu\n ui       X11\n language en_CA:en_US:en\n collate  en_CA.UTF-8\n ctype    en_CA.UTF-8\n tz       America/Winnipeg\n date     2025-06-06\n pandoc   3.4 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/x86_64/ (via rmarkdown)\n quarto   1.6.39 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package       * version date (UTC) lib source\n P assertr       * 3.0.1   2023-11-23 [?] CRAN (R 4.5.0)\n P backports       1.5.0   2024-05-23 [?] CRAN (R 4.5.0)\n P bit             4.6.0   2025-03-06 [?] CRAN (R 4.5.0)\n P bit64           4.6.0-1 2025-01-16 [?] CRAN (R 4.5.0)\n P boot            1.3-31  2024-08-28 [?] CRAN (R 4.4.2)\n P broom         * 1.0.8   2025-03-28 [?] CRAN (R 4.5.0)\n P cachem          1.1.0   2024-05-16 [?] CRAN (R 4.5.0)\n P cellranger      1.1.0   2016-07-27 [?] CRAN (R 4.5.0)\n P class           7.3-23  2025-01-01 [?] CRAN (R 4.4.2)\n P classInt        0.4-11  2025-01-08 [?] CRAN (R 4.5.0)\n P cli             3.6.5   2025-04-23 [?] CRAN (R 4.5.0)\n P codetools       0.2-20  2024-03-31 [?] CRAN (R 4.4.0)\n P crayon          1.5.3   2024-06-20 [?] CRAN (R 4.5.0)\n P DBI             1.2.3   2024-06-02 [?] CRAN (R 4.5.0)\n P devtools        2.4.5   2022-10-11 [?] CRAN (R 4.5.0)\n P DHARMa        * 0.4.7   2024-10-18 [?] CRAN (R 4.5.0)\n P digest          0.6.37  2024-08-19 [?] CRAN (R 4.5.0)\n P doParallel      1.0.17  2022-02-07 [?] CRAN (R 4.5.0)\n P dplyr         * 1.1.4   2023-11-17 [?] CRAN (R 4.5.0)\n P e1071           1.7-16  2024-09-16 [?] CRAN (R 4.5.0)\n P ellipsis        0.3.2   2021-04-29 [?] CRAN (R 4.5.0)\n P evaluate        1.0.3   2025-01-10 [?] CRAN (R 4.5.0)\n P farver          2.1.2   2024-05-13 [?] CRAN (R 4.5.0)\n P fastmap         1.2.0   2024-05-15 [?] CRAN (R 4.5.0)\n P foreach         1.5.2   2022-02-02 [?] CRAN (R 4.5.0)\n P fs              1.6.6   2025-04-12 [?] CRAN (R 4.5.0)\n P gamm4           0.2-7   2025-04-22 [?] CRAN (R 4.5.0)\n P generics        0.1.4   2025-05-09 [?] CRAN (R 4.5.0)\n P GGally          2.2.1   2024-02-14 [?] CRAN (R 4.5.0)\n P ggplot2       * 3.5.2   2025-04-09 [?] CRAN (R 4.5.0)\n P ggspatial     * 1.1.9   2023-08-17 [?] CRAN (R 4.5.0)\n P ggstats         0.9.0   2025-03-10 [?] CRAN (R 4.5.0)\n P ggthemes      * 5.1.0   2024-02-10 [?] CRAN (R 4.5.0)\n P glue            1.8.0   2024-09-30 [?] CRAN (R 4.5.0)\n P gridExtra       2.3     2017-09-09 [?] CRAN (R 4.5.0)\n P gt            * 1.0.0   2025-04-05 [?] CRAN (R 4.5.0)\n P gtable          0.3.6   2024-10-25 [?] CRAN (R 4.5.0)\n P hms             1.1.3   2023-03-21 [?] CRAN (R 4.5.0)\n P htmltools       0.5.8.1 2024-04-04 [?] CRAN (R 4.5.0)\n P htmlwidgets     1.6.4   2023-12-06 [?] CRAN (R 4.5.0)\n P httpuv          1.6.16  2025-04-16 [?] CRAN (R 4.5.0)\n P httr            1.4.7   2023-08-15 [?] CRAN (R 4.5.0)\n P iterators       1.0.14  2022-02-05 [?] CRAN (R 4.5.0)\n P jsonlite        2.0.0   2025-03-27 [?] CRAN (R 4.5.0)\n P KernSmooth      2.23-26 2025-01-01 [?] CRAN (R 4.4.2)\n P knitr           1.50    2025-03-16 [?] CRAN (R 4.5.0)\n P labeling        0.4.3   2023-08-29 [?] CRAN (R 4.5.0)\n P later           1.4.2   2025-04-08 [?] CRAN (R 4.5.0)\n P lattice         0.22-7  2025-04-02 [?] CRAN (R 4.5.0)\n P lifecycle       1.0.4   2023-11-07 [?] CRAN (R 4.5.0)\n P lme4            1.1-37  2025-03-26 [?] CRAN (R 4.5.0)\n P lubridate     * 1.9.4   2024-12-08 [?] CRAN (R 4.5.0)\n P magrittr        2.0.3   2022-03-30 [?] CRAN (R 4.5.0)\n P MASS            7.3-65  2025-02-28 [?] CRAN (R 4.4.3)\n P Matrix          1.7-3   2025-03-11 [?] CRAN (R 4.4.3)\n P matrixStats     1.5.0   2025-01-07 [?] CRAN (R 4.5.0)\n P memoise         2.0.1   2021-11-26 [?] CRAN (R 4.5.0)\n P mgcv          * 1.9-3   2025-04-04 [?] CRAN (R 4.5.0)\n P mgcViz        * 0.2.0   2025-04-11 [?] CRAN (R 4.5.0)\n P mime            0.13    2025-03-17 [?] CRAN (R 4.5.0)\n P miniUI          0.1.2   2025-04-17 [?] CRAN (R 4.5.0)\n P minqa           1.2.8   2024-08-17 [?] CRAN (R 4.5.0)\n P moments       * 0.14.1  2022-05-02 [?] CRAN (R 4.5.0)\n P nlme          * 3.1-168 2025-03-31 [?] CRAN (R 4.4.3)\n P nloptr          2.2.1   2025-03-17 [?] CRAN (R 4.5.0)\n P openxlsx      * 4.2.8   2025-01-25 [?] CRAN (R 4.5.0)\n P patchwork     * 1.3.0   2024-09-16 [?] CRAN (R 4.5.0)\n P pillar          1.10.2  2025-04-05 [?] CRAN (R 4.5.0)\n P pkgbuild        1.4.8   2025-05-26 [?] CRAN (R 4.5.0)\n P pkgconfig       2.0.3   2019-09-22 [?] CRAN (R 4.5.0)\n P pkgload         1.4.0   2024-06-28 [?] CRAN (R 4.5.0)\n P plyr            1.8.9   2023-10-02 [?] CRAN (R 4.5.0)\n P profvis         0.4.0   2024-09-20 [?] CRAN (R 4.5.0)\n P promises        1.3.3   2025-05-29 [?] CRAN (R 4.5.0)\n P proxy           0.4-27  2022-06-09 [?] CRAN (R 4.5.0)\n P purrr         * 1.0.4   2025-02-05 [?] CRAN (R 4.5.0)\n P qgam          * 2.0.0   2025-04-10 [?] CRAN (R 4.5.0)\n P R6              2.6.1   2025-02-15 [?] CRAN (R 4.5.0)\n P rbibutils       2.3     2024-10-04 [?] CRAN (R 4.5.0)\n P RColorBrewer    1.1-3   2022-04-03 [?] CRAN (R 4.5.0)\n P Rcpp            1.0.14  2025-01-12 [?] CRAN (R 4.5.0)\n P Rdpack          2.6.4   2025-04-09 [?] CRAN (R 4.5.0)\n P readr         * 2.1.5   2024-01-10 [?] CRAN (R 4.5.0)\n P readxl        * 1.4.5   2025-03-07 [?] CRAN (R 4.5.0)\n P reformulas      0.4.1   2025-04-30 [?] CRAN (R 4.5.0)\n P remotes         2.5.0   2024-03-17 [?] CRAN (R 4.5.0)\n P renv            1.1.4   2025-03-20 [?] CRAN (R 4.5.0)\n P rlang           1.1.6   2025-04-11 [?] CRAN (R 4.5.0)\n P rmarkdown       2.29    2024-11-04 [?] CRAN (R 4.5.0)\n P rnaturalearth * 1.0.1   2023-12-15 [?] CRAN (R 4.5.0)\n P rstudioapi      0.17.1  2024-10-22 [?] CRAN (R 4.5.0)\n P sass            0.4.10  2025-04-11 [?] CRAN (R 4.5.0)\n P scales          1.4.0   2025-04-24 [?] CRAN (R 4.5.0)\n P sessioninfo     1.2.3   2025-02-05 [?] CRAN (R 4.5.0)\n P sf            * 1.0-21  2025-05-15 [?] CRAN (R 4.5.0)\n P shiny           1.10.0  2024-12-14 [?] CRAN (R 4.5.0)\n P stringi         1.8.7   2025-03-27 [?] CRAN (R 4.5.0)\n P stringr       * 1.5.1   2023-11-14 [?] CRAN (R 4.5.0)\n P terra           1.8-54  2025-06-01 [?] CRAN (R 4.5.0)\n P tibble          3.2.1   2023-03-20 [?] CRAN (R 4.5.0)\n P tidyr         * 1.3.1   2024-01-24 [?] CRAN (R 4.5.0)\n P tidyselect      1.2.1   2024-03-11 [?] CRAN (R 4.5.0)\n P timechange      0.3.0   2024-01-18 [?] CRAN (R 4.5.0)\n P tzdb            0.5.0   2025-03-15 [?] CRAN (R 4.5.0)\n P units           0.8-7   2025-03-11 [?] CRAN (R 4.5.0)\n P urlchecker      1.0.1   2021-11-30 [?] CRAN (R 4.5.0)\n P usethis         3.1.0   2024-11-26 [?] CRAN (R 4.5.0)\n P vctrs           0.6.5   2023-12-01 [?] CRAN (R 4.5.0)\n P viridis         0.6.5   2024-01-29 [?] CRAN (R 4.5.0)\n P viridisLite     0.4.2   2023-05-02 [?] CRAN (R 4.5.0)\n P vroom           1.6.5   2023-12-05 [?] CRAN (R 4.5.0)\n P withr           3.0.2   2024-10-28 [?] CRAN (R 4.5.0)\n P xfun            0.52    2025-04-02 [?] CRAN (R 4.5.0)\n P xml2            1.3.8   2025-03-14 [?] CRAN (R 4.5.0)\n P xtable          1.8-4   2019-04-21 [?] CRAN (R 4.5.0)\n P yaml            2.3.10  2024-07-26 [?] CRAN (R 4.5.0)\n P zip             2.3.3   2025-05-13 [?] CRAN (R 4.5.0)\n\n [1] /home/steffi/Projects/vulture_migration/renv/library/linux-ubuntu-noble/R-4.5/x86_64-pc-linux-gnu\n [2] /home/steffi/.cache/R/renv/sandbox/linux-ubuntu-noble/R-4.5/x86_64-pc-linux-gnu/9a444a72\n\n * ── Packages attached to the search path.\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────",
    "crumbs": [
      "Workflow",
      "Calculate Metrics"
    ]
  },
  {
    "objectID": "02_calculate_metrics.html#footnotes",
    "href": "02_calculate_metrics.html#footnotes",
    "title": "Calculate Metrics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://noamross.github.io/gams-in-r-course/chapter1↩︎\nhttps://noamross.github.io/gams-in-r-course/chapter2↩︎",
    "crumbs": [
      "Workflow",
      "Calculate Metrics"
    ]
  },
  {
    "objectID": "01_initial_exploration.html",
    "href": "01_initial_exploration.html",
    "title": "Initial Exploration",
    "section": "",
    "text": "This is the initial exploration of Turkey Vulture kettling and migration behaviour above Rocky Point on southern Vancouver Island.\n\nBanding and observations start 1 hour before sunrise and commence for 6 hours. Blanks and ****** indicate no data (NA), often because the banding station was not open at all due to rainy weather or closed early for similar reasons. In addition, the station is on Department of National Defence land, and blanks can arise when banders are excluded by DND. Unfortunately, this applies to the entire year of 2007. Zero values represent true zeros, the count was made but no vultures seen.\n\nData values are daily estimates “of the greatest aggregation of vultures over Rocky Point that day during the station hours”.",
    "crumbs": [
      "Workflow",
      "Initial Exploration"
    ]
  },
  {
    "objectID": "01_initial_exploration.html#background",
    "href": "01_initial_exploration.html#background",
    "title": "Initial Exploration",
    "section": "",
    "text": "This is the initial exploration of Turkey Vulture kettling and migration behaviour above Rocky Point on southern Vancouver Island.\n\nBanding and observations start 1 hour before sunrise and commence for 6 hours. Blanks and ****** indicate no data (NA), often because the banding station was not open at all due to rainy weather or closed early for similar reasons. In addition, the station is on Department of National Defence land, and blanks can arise when banders are excluded by DND. Unfortunately, this applies to the entire year of 2007. Zero values represent true zeros, the count was made but no vultures seen.\n\nData values are daily estimates “of the greatest aggregation of vultures over Rocky Point that day during the station hours”.",
    "crumbs": [
      "Workflow",
      "Initial Exploration"
    ]
  },
  {
    "objectID": "01_initial_exploration.html#load-clean-data",
    "href": "01_initial_exploration.html#load-clean-data",
    "title": "Initial Exploration",
    "section": "Load & Clean Data",
    "text": "Load & Clean Data\n\nsource(\"XX_functions.R\")  # Custom functions and packages\n\n\nLoad DataQuick Check\n\n\n\npeek &lt;- read_excel(\"Data/Raw/TUVU DET 1998-2023 days excluded FINAL_DLK_2025-05-28.xlsx\")\n\nNew names:\n• `` -&gt; `...1`\n• `` -&gt; `...28`\n• `` -&gt; `...29`\n• `` -&gt; `...30`\n• `` -&gt; `...31`\n• `` -&gt; `...32`\n• `` -&gt; `...33`\n• `` -&gt; `...34`\n• `` -&gt; `...35`\n\nend &lt;- which(peek$`day/year` == \"year\") - 1\n\nv &lt;- read_excel(\"Data/Raw/TUVU DET 1998-2023 days excluded FINAL_DLK_2025-05-28.xlsx\", \n                na = c(\"\", \"******\"), n_max = end) \n\nNew names:\n• `` -&gt; `...27`\n• `` -&gt; `...28`\n• `` -&gt; `...29`\n• `` -&gt; `...30`\n• `` -&gt; `...31`\n\n\n\n\n\nhead(v)\n\n# A tibble: 6 × 31\n  `day/year`          `1998` `1999` `2000` `2001` `2002` `2003` `2004` `2005`\n  &lt;dttm&gt;               &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 2023-07-23 00:00:00      4     NA      2      4      3      4      3      3\n2 2023-07-24 00:00:00      7     NA      0      3      4      5      3      4\n3 2023-07-25 00:00:00      4      6      2      3      7      4      5      6\n4 2023-07-26 00:00:00      0      4      0      2      6      5      4      6\n5 2023-07-27 00:00:00      0      7     NA      9      5      1      1      1\n6 2023-07-28 00:00:00      1      4     NA     10      7      6      1      8\n# ℹ 22 more variables: `2006` &lt;dbl&gt;, `2008` &lt;dbl&gt;, `2009` &lt;dbl&gt;, `2010` &lt;dbl&gt;,\n#   `2011` &lt;dbl&gt;, `2012` &lt;dbl&gt;, `2013` &lt;dbl&gt;, `2014` &lt;dbl&gt;, `2015` &lt;dbl&gt;,\n#   `2016` &lt;dbl&gt;, `2017` &lt;dbl&gt;, `2018` &lt;dbl&gt;, `2019` &lt;dbl&gt;, `2020` &lt;dbl&gt;,\n#   `2021` &lt;dbl&gt;, `2022` &lt;dbl&gt;, `2023` &lt;dbl&gt;, ...27 &lt;lgl&gt;, ...28 &lt;lgl&gt;,\n#   ...29 &lt;lgl&gt;, ...30 &lt;lgl&gt;, ...31 &lt;chr&gt;\n\ntail(v)\n\n# A tibble: 6 × 31\n  `day/year`          `1998` `1999` `2000` `2001` `2002` `2003` `2004` `2005`\n  &lt;dttm&gt;               &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 2023-10-16 00:00:00      0     NA      0     NA     25     NA    120      5\n2 2023-10-17 00:00:00     NA     NA      4     NA     26     NA     NA     NA\n3 2023-10-18 00:00:00     12     NA      4     NA     14      6     26     10\n4 2023-10-19 00:00:00     NA     18     NA     NA      8     NA     NA     NA\n5 2023-10-20 00:00:00     NA     10     NA     NA     17     NA     NA     NA\n6 2023-10-21 00:00:00     NA     NA     NA     NA      9     NA     NA     NA\n# ℹ 22 more variables: `2006` &lt;dbl&gt;, `2008` &lt;dbl&gt;, `2009` &lt;dbl&gt;, `2010` &lt;dbl&gt;,\n#   `2011` &lt;dbl&gt;, `2012` &lt;dbl&gt;, `2013` &lt;dbl&gt;, `2014` &lt;dbl&gt;, `2015` &lt;dbl&gt;,\n#   `2016` &lt;dbl&gt;, `2017` &lt;dbl&gt;, `2018` &lt;dbl&gt;, `2019` &lt;dbl&gt;, `2020` &lt;dbl&gt;,\n#   `2021` &lt;dbl&gt;, `2022` &lt;dbl&gt;, `2023` &lt;dbl&gt;, ...27 &lt;lgl&gt;, ...28 &lt;lgl&gt;,\n#   ...29 &lt;lgl&gt;, ...30 &lt;lgl&gt;, ...31 &lt;chr&gt;\n\n\n\n\n\n\nQuick CleanQuick Check\n\n\n\nv &lt;- rename(v, date = \"day/year\") %&gt;%\n  select(matches(\"date|\\\\d{4}\")) %&gt;%\n  assert(is.numeric, -\"date\") # make sure we get only numeric data, not summaries\n\n\n\nExpect 2023 for all years, as original data doesn’t include year and R must have a year (this is corrected below).\n\nv\n\n# A tibble: 91 × 26\n   date                `1998` `1999` `2000` `2001` `2002` `2003` `2004` `2005`\n   &lt;dttm&gt;               &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 2023-07-23 00:00:00      4     NA      2      4      3      4      3      3\n 2 2023-07-24 00:00:00      7     NA      0      3      4      5      3      4\n 3 2023-07-25 00:00:00      4      6      2      3      7      4      5      6\n 4 2023-07-26 00:00:00      0      4      0      2      6      5      4      6\n 5 2023-07-27 00:00:00      0      7     NA      9      5      1      1      1\n 6 2023-07-28 00:00:00      1      4     NA     10      7      6      1      8\n 7 2023-07-29 00:00:00      0      5      0      2      6      1      3      1\n 8 2023-07-30 00:00:00      3     34      3      0      2      1      3      4\n 9 2023-07-31 00:00:00      4     11      4      8      2      7      2      3\n10 2023-08-01 00:00:00      6      5      0      2      6      5      3      4\n# ℹ 81 more rows\n# ℹ 17 more variables: `2006` &lt;dbl&gt;, `2008` &lt;dbl&gt;, `2009` &lt;dbl&gt;, `2010` &lt;dbl&gt;,\n#   `2011` &lt;dbl&gt;, `2012` &lt;dbl&gt;, `2013` &lt;dbl&gt;, `2014` &lt;dbl&gt;, `2015` &lt;dbl&gt;,\n#   `2016` &lt;dbl&gt;, `2017` &lt;dbl&gt;, `2018` &lt;dbl&gt;, `2019` &lt;dbl&gt;, `2020` &lt;dbl&gt;,\n#   `2021` &lt;dbl&gt;, `2022` &lt;dbl&gt;, `2023` &lt;dbl&gt;\n\n\n\n\n\n\nRe-arrangeQuick Check\n\n\n\nv &lt;- v |&gt;\n  mutate(`2007` = NA) |&gt;  # Add missing year for completeness\n  relocate(\"2007\", .before = \"2008\") |&gt;\n  pivot_longer(-date, names_to = \"year\", values_to = \"count\")\n\nyear(v$date) &lt;- as.integer(v$year)  # Fix years for each date\n\nv &lt;- mutate(v, date = as_date(date), doy = yday(date))\n\n\n\nCorrect year on all dates now, and verify a couple of random dates.\n\nv %&gt;%\n  verify(is.na(count[date == \"1999-07-23\"])) %&gt;%\n  verify(count[date == \"2006-08-06\"] == 7) %&gt;%\n  verify(count[date == \"2020-09-26\"] == 50) %&gt;%\n  verify(count[date == \"2023-10-14\"] == 54)\n\n# A tibble: 2,366 × 4\n   date       year  count   doy\n   &lt;date&gt;     &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 1998-07-23 1998      4   204\n 2 1999-07-23 1999     NA   204\n 3 2000-07-23 2000      2   205\n 4 2001-07-23 2001      4   204\n 5 2002-07-23 2002      3   204\n 6 2003-07-23 2003      4   204\n 7 2004-07-23 2004      3   205\n 8 2005-07-23 2005      3   204\n 9 2006-07-23 2006      4   204\n10 2007-07-23 2007     NA   204\n# ℹ 2,356 more rows",
    "crumbs": [
      "Workflow",
      "Initial Exploration"
    ]
  },
  {
    "objectID": "01_initial_exploration.html#quick-look-at-the-data",
    "href": "01_initial_exploration.html#quick-look-at-the-data",
    "title": "Initial Exploration",
    "section": "Quick look at the data",
    "text": "Quick look at the data\nToo see full screen: Right-click and select “Open Image in New Tab” (or similar)\n\nggplot(v, aes(x = doy, y = count, group = year, colour = year)) +\n  theme_bw() +\n  geom_point(na.rm = TRUE) +\n  stat_smooth(method = \"gam\", formula = y ~ s(x, k = 20), \n              method.args = list(method = \"REML\", family = \"nb\"), na.rm = TRUE, \n              level = 0.95) +\n  facet_wrap(~year, scales = \"free\") +\n  scale_colour_viridis_d()\n\n\n\n\n\n\n\n\nOmit 1998 because missing almost all of the second half of the migration period.\n\nv &lt;- filter(v, year != 1998)\n\nSave this formatted data for later use\n\nwrite_csv(v, \"Data/Datasets/vultures_clean_2023.csv\")",
    "crumbs": [
      "Workflow",
      "Initial Exploration"
    ]
  },
  {
    "objectID": "01_initial_exploration.html#questions",
    "href": "01_initial_exploration.html#questions",
    "title": "Initial Exploration",
    "section": "Questions",
    "text": "Questions\n\nHas the timing of kettle formation and migration has changed over the years?\n\nIf so, what is the pattern of change? (Direction and magnitude of change)\nIf not, document temporal distribution of numbers\n\nHas the number of birds in the kettles changed over time?\n\nmay indicate population trends\ncomplicated by accumulating birds over days when the weather conditions are not suitable for passing over the strait\npotentially look at weather effects…",
    "crumbs": [
      "Workflow",
      "Initial Exploration"
    ]
  },
  {
    "objectID": "01_initial_exploration.html#metrics-to-assess",
    "href": "01_initial_exploration.html#metrics-to-assess",
    "title": "Initial Exploration",
    "section": "Metrics to assess",
    "text": "Metrics to assess\nTo answer these questions we need to summarize the counts into specific metrics representing the timing of migration.\nSpecifically, we would like to calculate the\n\ndates of 5%, 25%, 50%, 75%, and 95% of the kettle numbers\nduration of passage - No. days between 5% and 95%\nduration of peak passage - No. days between 25% and 75%\n\nPopulation size (no. vultures in aggregations)\n\nmaximum\ncumulative\nnumber at peak passage (max? range? median?)\nmean/median number of locals\n\nOf these, the most important starting metrics are the dates of 5%, 25%, 50%, 75%, and 95% of the kettle numbers. These dates will define migration phenology as well as local vs. migrating counts. All other calculations can be performed using these values and the raw data.",
    "crumbs": [
      "Workflow",
      "Initial Exploration"
    ]
  },
  {
    "objectID": "01_initial_exploration.html#how-to-calculate-dates-of-passage",
    "href": "01_initial_exploration.html#how-to-calculate-dates-of-passage",
    "title": "Initial Exploration",
    "section": "How to calculate dates of passage?",
    "text": "How to calculate dates of passage?\n\nDon suggested following methodology from Allcock et al 2022\n\n“fit a curve to the data for each year and use it to estimate …”\n\nAllcock et al 2022 “modelled optimal Gaussian functions to describe the migration phenology for each species – year combination in our dataset using Markhov Chain Monte Carlo (MCMC) techniques”.\nThey say that “Gaussian functions … often outperform General Additive Models (Linden et al., 2016)”\n\nI like this approach in general, but I’m not convinced that we can’t/shouldn’t use GAMs.\nIn Linden et al., they restricted the GAM models’ effective degrees of freedom (which isn’t common, usually they are determined by the modelling procedure) and note that if they didn’t restrict them, “GAMs would have been preferred in 73% of the cases”.\nThe argument for restricting GAMS was to make the comparison among models possible as the “estimation of [effective] degrees of freedom [in a GAM] is similar to a model selection procedure”. This would have invalidated their use of the information theoretic approach for model comparison.\nI still think we should use GAMs, because\n\nWe are using this to calculate metrics and as long as the model fits the data, is consistent and replicable, it doesn’t really matter how we get there (i.e. there’s no reason to not use the best GAM method with built-in model selection).\nWe are looking at a single species and so can assess each year to make sure it looks right.\nI am familiar with GAMs, but have no experience with MCMC techniques to model Gaussian functions.\nI don’t think that Linden et al. really demonstrated that GAMs are ‘bad’ to use.\n\n\nSteffi’s suggested approach\nWe use GAMs to model each year individually. From the predicted data we calculate the cumulative counts and the points at which these counts hit 5%, 25%, 75%, 95% of the total (I think this is what Allcock et al mean by ‘bird-days’?).\nHowever, we will need to think about how to handle the resident birds, as they may artificially inflate the cumulative counts prior to migration.",
    "crumbs": [
      "Workflow",
      "Initial Exploration"
    ]
  },
  {
    "objectID": "01_initial_exploration.html#gam",
    "href": "01_initial_exploration.html#gam",
    "title": "Initial Exploration",
    "section": "Using GAM based appraoch",
    "text": "Using GAM based appraoch\nFor illustration and exploration of this approach, we’ll look at 2000.\n\nExample GAM - 2000Model summaryModel evaluation\n\n\n\nNegative binomial model fits the count data with overdispersion\nUse Restricted Maximum Likelihood (“Most likely to give you reliable, stable results”1)\nWe smooth (s()) over doy (day of year) to account for non-linear migration patterns\nWe set k = 10 (up to 10 basis functions; we want enough to make sure we capture the patterns, but too many will slow things down).\n\n\ng &lt;- gam(count ~ s(doy, k = 10), data = filter(v, year == 2000), \n         method = \"REML\", family = \"nb\")\nplot(g, shade = TRUE, trans = exp, residuals = TRUE, pch = 20, \n     shift = coef(g)[1])\n\n\n\n\n\n\n\n\n\n\nNot really necessary for us to evaluate, but the s(doy) value indicates that we have a signifcant pattern, and the fact that the edf value is less than our k = 10, is good.\n\nsummary(g)\n\n\nFamily: Negative Binomial(1.129) \nLink function: log \n\nFormula:\ncount ~ s(doy, k = 10)\n\nParametric coefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   2.3393     0.1207   19.38   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n         edf Ref.df Chi.sq p-value    \ns(doy) 5.475  6.616  175.5  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.391   Deviance explained = 71.3%\n-REML =  271.2  Scale est. = 1         n = 76\n\n\n\n\nQuick checks to ensure model is valid.\n\n\nCode\np0 &lt;- par(mfrow = c(2,2))\ngam.check(g, pch = 19, cex = 0.5)\n\n\n\n\n\n\n\n\n\n\nMethod: REML   Optimizer: outer newton\nfull convergence after 4 iterations.\nGradient range [8.674563e-10,1.084888e-07]\n(score 271.1993 & scale 1).\nHessian positive definite, eigenvalue range [2.264941,28.62155].\nModel rank =  10 / 10 \n\nBasis dimension (k) checking results. Low p-value (k-index&lt;1) may\nindicate that k is too low, especially if edf is close to k'.\n\n         k'  edf k-index p-value\ns(doy) 9.00 5.48    0.96    0.62\n\n\nCode\npar(p0)\n\ns &lt;- DHARMa::simulateResiduals(g, plot = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalculating dates of passage\nFirst create data set of predicted GAM model outputs across the entire date range.\n\ndoy &lt;- min(g$model$doy):max(g$model$doy)\n\np &lt;- predict(g, newdata = list(doy = doy), type = \"response\", se.fit = TRUE)\nd &lt;- data.frame(doy = doy, count = p$fit, se = p$se) |&gt;\n  mutate(ci99_upper = count + se * 2.58,\n         ci99_lower = count - se * 2.58)\n\nNext we’ll calculate percentiles based on cumulative counts. But there are several ways we can do this, depending how we want to account for resident vultures.\n\nOption 1: We do nothing\nOption 2: We use the model’s CI 99% to find a threshold date before which we assume all observations are of local, resident vultures, after which is migration. We would then calculate cumulative counts only on data after this threshold)2\nOption 3: We calculate the median number of residents and simply subtract that from all counts before calculating our cumulative counts.\n\n\n\nOption 1: Do not omit resident vultures\n\nUse entire date range\nNo threshold cutoff\nNo subtraction of local counts\nCalculate local counts from predicted data before Day 240\n\n\n\nCode\nd_sum &lt;- mutate(d, count_sum = cumsum(count))\n\ndts &lt;- calc_dates(d_sum)\ndts_overall &lt;- mutate(dts, type = \"Option 1: No Adjustments\")\n\nresidents &lt;- d |&gt;\n  filter(doy &lt; 240) |&gt;\n  summarize(res_pop_min = min(count), res_pop_max = max(count), \n            res_pop_median = median(count), res_pop_mean = mean(count)) |&gt;\n  mutate(across(everything(), \\(x) round(x, 1)))\n\ng1 &lt;- plot_cum_explore(d_sum = d_sum, dts = dts)\ng2 &lt;- plot_model_explore(d_raw = g$model, d_pred = d, dts = dts, residents, resident_date = 240) +\n  labs(caption = \"Local count stats calculated up to Day 240\")\ng_opt1 &lt;- g1 / g2 + plot_annotation(title = \"Option 1: No adjustments\")\n\n\n\n\nOption 2: Use a count threshold to omit dates with resident vultures\n\nUse the minimum value of the upper limit of the CI 99% to calculate a cutoff threshold (only consider dates &lt; 270 to avoid the end of migration tailing off)\nThe first date prior to migration which crosses this threshold into migration (i.e. avoids little blips up and down early in the year) is used as the threshold\nThe data is filtered to include only dates on or after this threshold\nThen the percentiles are calculated based on cumulative counts in this subset\n\n\nthresh &lt;- filter(d, doy &lt; 270) |&gt; # Only consider pre-migration\n  arrange(desc(doy)) |&gt;\n  filter(count &lt;= min(ci99_upper)) |&gt;\n  slice(1) |&gt;\n  pull(doy)\nthresh\n\n[1] 228\n\n\n\nWhat is the min of the upper limit of the CI 99%?!?!?\nIn the image below…\n\nthe Grey ribbon represents the 99% Confidence Interval around the GAM model (grey line)\nthe upper limit of this ribbon is the upper limit of the 99% CI\nthe minimum of this value is the area on the figure where the upper edge of the ribbon is at it’s lowest value (large black point)\nin this year/model, it’s occurs on the first day\n\nHow do we use this value?\n\nthe first value in the model (grey line) to cross this limit (dashed black line), identifies our threshold date (red dashed line)\nwe then use only the dates after this threshold to calculate migration metrics\nwe then use only the dates before this threshold to calculate local counts\n\n\n\n\nCode\n# g1 &lt;- ggplot(data = d, mapping = aes(x = doy, y = count)) +\n#   theme_bw() +\n#   geom_ribbon(aes(ymin = ci99_lower, ymax = ci99_upper), fill = \"grey50\", alpha = 0.5) +\n#   geom_point(data = g$model) +\n#   geom_line() +\n#   coord_cartesian(xlim = c(204, 270)) +\n#   labs(title = \"Look only at beginning of migration\")\n\ng2 &lt;- ggplot(data = d, mapping = aes(x = doy, y = count)) +\n  theme_bw() +\n  theme(legend.position = c(0.3, 0.85), legend.title = element_blank()) +\n  geom_ribbon(aes(ymin = ci99_lower, ymax = ci99_upper), fill = \"grey50\", alpha = 0.3) +\n  geom_point(data = g$model, colour = \"grey40\") +\n  geom_line(colour = \"grey60\") +\n  coord_cartesian(xlim = c(204, 245), ylim = c(-5, 40)) +\n  geom_point(x = d$doy[d$ci99_upper == min(d$ci99_upper)], \n             y = min(d$ci99_upper), size = 4, aes(colour = \"Day with min 99% CI\"),\n             inherit.aes = FALSE) +\n  geom_hline(linetype = 2,\n             aes(yintercept = min(d$ci99_upper), colour = \"Min of upper 99% CI\")) +\n  geom_vline(linetype = 2,\n             aes(xintercept = thresh, colour = \"Threshold\\n(first date consistently\\nabove min of upper 99% CI)\")) +\n  scale_colour_manual(values = c(\"black\", \"black\", \"red\")) +\n  guides(colour = guide_legend(override.aes = list(shape = c(19, 0, 0), size = c(3, 0, 0), linewidth = c(0, 0.3, 0.3), linetype = c(\"solid\", \"dashed\", \"dashed\")))) +\n  labs(title = \"Find thresholds based on local counts\", \n       subtitle = \"Figure is model plot 'zoomed' into early season\")\n\n\nWarning: A numeric `legend.position` argument in `theme()` was deprecated in ggplot2\n3.5.0.\nℹ Please use the `legend.position.inside` argument of `theme()` instead.\n\n\nCode\ng2\n\n\nWarning in geom_point(x = d$doy[d$ci99_upper == min(d$ci99_upper)], y = min(d$ci99_upper), : All aesthetics have length 1, but the data has 88 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning: Use of `d$ci99_upper` is discouraged.\nℹ Use `ci99_upper` instead.\n\n\n\n\n\n\n\n\n\n\n\nCode\nd_sum &lt;- filter(d, doy &gt;= thresh) |&gt;\n  mutate(count_sum = cumsum(count))\n\ndts &lt;- calc_dates(d_sum)\ndts_overall &lt;- bind_rows(dts_overall, \n                         mutate(dts, type = \"Option 2: Threshold\"))\n\nresidents &lt;- d |&gt;\n  filter(doy &lt; thresh) |&gt;\n  summarize(res_pop_min = min(count), res_pop_max = max(count), \n            res_pop_median = median(count), res_pop_mean = mean(count)) |&gt;\n  mutate(across(everything(), \\(x) round(x, 1)))\n\n\ng1 &lt;- plot_cum_explore(d_sum, dts) +\n  geom_vline(xintercept = thresh, colour = \"red\", linetype = \"dotted\") +\n  annotate(\"text\", label = \"Threshold\", x = thresh, y = 1000)\ng2 &lt;- plot_model_explore(d_raw = g$model, d_pred = d, dts = dts, residents, resident_date = thresh) +\n  labs(caption = paste0(\"Resident count stats calculated up to threshold date, \", thresh))\n  \ng_opt2 &lt;- g1 / g2 + plot_annotation(title = \"Option 2: Use only dates after threshold\")\n\n\n\n\nOption 3: Subtract the median resident count from all observations\n\nUse entire date range\nNo threshold cutoff\nSubtract the median resident count from the predicted observations, prior to calculating the cumulative counts\nCalculate resident counts from predicted data before Day 240\n\n\n\nCode\nresidents &lt;- d |&gt;\n  filter(doy &lt; 240) |&gt;\n  summarize(res_pop_min = min(count), res_pop_max = max(count), \n            res_pop_median = median(count), res_pop_mean = mean(count)) |&gt;\n  mutate(across(everything(), \\(x) round(x, 1)))\n\nd_sum &lt;- mutate(d, \n                count = count - residents$res_pop_median,\n                count_sum = cumsum(count))\n\ndts &lt;- calc_dates(d_sum)\ndts_overall &lt;- bind_rows(dts_overall, \n                         mutate(dts, type = \"Option 3: Subtraction\"))\n\ng1 &lt;- plot_cum_explore(d_sum = d_sum, dts = dts) +\n  annotate(\"text\", x = 225, y = 1000,\n           label = paste0(\"Remove \", \n                          residents$res_pop_median, \n                          \" from all counts\\nbefore calcuating cumuative sum\"))\ng2 &lt;- plot_model_explore(d_raw = g$model, d_pred = d, dts = dts, residents, resident_date = 240) +\n  labs(caption = \"Local count stats calculated up to Day 240\")\ng_opt3 &lt;- g1 / g2 + plot_annotation(title = \"Option 3: Subtract median first\")\n\n\n\n\nComparing our options\nIn each figure the red dots represent the 5%, 25%, 50%, 75% and 95% dates calculated based on the cumulative counts (top figure, dotted lines show the percentiles).\nThe bottom figure shows the raw counts, with the GAM model overlaid, the pink windows indicate 5%-95% and 25%-75% date ranges. The arrow on the left indicates which predicted values were included in the local count statistics.\nNote that the only two things that really change in this example are the initial 5% date (becomes increasingly later with each option) and the calculation of the “local” population statistics (because in Option 1 and Option 2, they’re based on the same set of data, but in Option 2, we use a threshold to define before and after the expected start of migration.\n\ng_opt1\n\n\n\n\n\n\n\ng_opt2\n\n\n\n\n\n\n\ng_opt3\n\n\n\n\n\n\n\n\nHowever in this example (and in many that I’ve looked at), this only really affects the first, 5%, Date value.\nHere are the dates calculated for each percentiles using the different methods in this example.\n\ndts_overall |&gt;\n  select(-contains(\"count\")) |&gt;\n  pivot_wider(names_from = \"perc\", values_from = \"doy_passage\")\n\n# A tibble: 3 × 6\n  type                       p05   p25   p50   p75   p95\n  &lt;chr&gt;                    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Option 1: No Adjustments   247   262   268   273   281\n2 Option 2: Threshold        250   262   268   273   281\n3 Option 3: Subtraction      253   263   268   273   281\n\n\n\n\nSteffi’s suggested approach\nI suggest we use Option 3: Subtracting the median local numbers.\nI think that Option 1 gives us an artifically early start to migration as it starts cumulating counts over resident bird observations.\nI think that Option 2 works, but is potentially overly complicated. I’m also not convinced that it completely avoids the issue of resident birds.\nOn the other hand I find Option 3\n\nReasonable\nEasy to explain\nVisually, it looks the most ‘right’ (to me at any rate)\nIt seems to result in more conservative numbers\n\nOther options\n\nWe could also just not use the 5% and 95% metrics\nI could run all three methods on the full data set and we could check them all together.",
    "crumbs": [
      "Workflow",
      "Initial Exploration"
    ]
  },
  {
    "objectID": "01_initial_exploration.html#reproducibility",
    "href": "01_initial_exploration.html#reproducibility",
    "title": "Initial Exploration",
    "section": "Reproducibility",
    "text": "Reproducibility\n\n\n\n\n\n\nSession Info\n\n\n\n\n\n\ndevtools::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.5.0 (2025-04-11)\n os       Ubuntu 24.04.2 LTS\n system   x86_64, linux-gnu\n ui       X11\n language en_CA:en_US:en\n collate  en_CA.UTF-8\n ctype    en_CA.UTF-8\n tz       America/Winnipeg\n date     2025-06-06\n pandoc   3.4 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/x86_64/ (via rmarkdown)\n quarto   1.6.39 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package       * version date (UTC) lib source\n P assertr       * 3.0.1   2023-11-23 [?] CRAN (R 4.5.0)\n P backports       1.5.0   2024-05-23 [?] CRAN (R 4.5.0)\n P bit             4.6.0   2025-03-06 [?] CRAN (R 4.5.0)\n P bit64           4.6.0-1 2025-01-16 [?] CRAN (R 4.5.0)\n P boot            1.3-31  2024-08-28 [?] CRAN (R 4.4.2)\n P broom         * 1.0.8   2025-03-28 [?] CRAN (R 4.5.0)\n P cachem          1.1.0   2024-05-16 [?] CRAN (R 4.5.0)\n P cellranger      1.1.0   2016-07-27 [?] CRAN (R 4.5.0)\n P class           7.3-23  2025-01-01 [?] CRAN (R 4.4.2)\n P classInt        0.4-11  2025-01-08 [?] CRAN (R 4.5.0)\n P cli             3.6.5   2025-04-23 [?] CRAN (R 4.5.0)\n P codetools       0.2-20  2024-03-31 [?] CRAN (R 4.4.0)\n P crayon          1.5.3   2024-06-20 [?] CRAN (R 4.5.0)\n P DBI             1.2.3   2024-06-02 [?] CRAN (R 4.5.0)\n P devtools        2.4.5   2022-10-11 [?] CRAN (R 4.5.0)\n P DHARMa        * 0.4.7   2024-10-18 [?] CRAN (R 4.5.0)\n P digest          0.6.37  2024-08-19 [?] CRAN (R 4.5.0)\n P doParallel      1.0.17  2022-02-07 [?] CRAN (R 4.5.0)\n P dplyr         * 1.1.4   2023-11-17 [?] CRAN (R 4.5.0)\n P e1071           1.7-16  2024-09-16 [?] CRAN (R 4.5.0)\n P ellipsis        0.3.2   2021-04-29 [?] CRAN (R 4.5.0)\n P evaluate        1.0.3   2025-01-10 [?] CRAN (R 4.5.0)\n P farver          2.1.2   2024-05-13 [?] CRAN (R 4.5.0)\n P fastmap         1.2.0   2024-05-15 [?] CRAN (R 4.5.0)\n P foreach         1.5.2   2022-02-02 [?] CRAN (R 4.5.0)\n P fs              1.6.6   2025-04-12 [?] CRAN (R 4.5.0)\n P gamm4           0.2-7   2025-04-22 [?] CRAN (R 4.5.0)\n P gap             1.6     2024-08-27 [?] CRAN (R 4.5.0)\n P gap.datasets    0.0.6   2023-08-25 [?] CRAN (R 4.5.0)\n P generics        0.1.4   2025-05-09 [?] CRAN (R 4.5.0)\n P GGally          2.2.1   2024-02-14 [?] CRAN (R 4.5.0)\n P ggplot2       * 3.5.2   2025-04-09 [?] CRAN (R 4.5.0)\n P ggspatial     * 1.1.9   2023-08-17 [?] CRAN (R 4.5.0)\n P ggstats         0.9.0   2025-03-10 [?] CRAN (R 4.5.0)\n P ggthemes      * 5.1.0   2024-02-10 [?] CRAN (R 4.5.0)\n P glue            1.8.0   2024-09-30 [?] CRAN (R 4.5.0)\n P gridExtra       2.3     2017-09-09 [?] CRAN (R 4.5.0)\n P gt            * 1.0.0   2025-04-05 [?] CRAN (R 4.5.0)\n P gtable          0.3.6   2024-10-25 [?] CRAN (R 4.5.0)\n P hms             1.1.3   2023-03-21 [?] CRAN (R 4.5.0)\n P htmltools       0.5.8.1 2024-04-04 [?] CRAN (R 4.5.0)\n P htmlwidgets     1.6.4   2023-12-06 [?] CRAN (R 4.5.0)\n P httpuv          1.6.16  2025-04-16 [?] CRAN (R 4.5.0)\n P httr            1.4.7   2023-08-15 [?] CRAN (R 4.5.0)\n P iterators       1.0.14  2022-02-05 [?] CRAN (R 4.5.0)\n P jsonlite        2.0.0   2025-03-27 [?] CRAN (R 4.5.0)\n P KernSmooth      2.23-26 2025-01-01 [?] CRAN (R 4.4.2)\n P knitr           1.50    2025-03-16 [?] CRAN (R 4.5.0)\n P labeling        0.4.3   2023-08-29 [?] CRAN (R 4.5.0)\n P later           1.4.2   2025-04-08 [?] CRAN (R 4.5.0)\n P lattice         0.22-5  2023-10-24 [?] CRAN (R 4.3.3)\n P lifecycle       1.0.4   2023-11-07 [?] CRAN (R 4.5.0)\n P lme4            1.1-37  2025-03-26 [?] CRAN (R 4.5.0)\n P lubridate     * 1.9.4   2024-12-08 [?] CRAN (R 4.5.0)\n P magrittr        2.0.3   2022-03-30 [?] CRAN (R 4.5.0)\n P MASS            7.3-65  2025-02-28 [?] CRAN (R 4.4.3)\n P Matrix          1.7-3   2025-03-11 [?] CRAN (R 4.4.3)\n P matrixStats     1.5.0   2025-01-07 [?] CRAN (R 4.5.0)\n P memoise         2.0.1   2021-11-26 [?] CRAN (R 4.5.0)\n P mgcv          * 1.9-3   2025-04-04 [?] CRAN (R 4.5.0)\n P mgcViz        * 0.2.0   2025-04-11 [?] CRAN (R 4.5.0)\n P mime            0.13    2025-03-17 [?] CRAN (R 4.5.0)\n P miniUI          0.1.2   2025-04-17 [?] CRAN (R 4.5.0)\n P minqa           1.2.8   2024-08-17 [?] CRAN (R 4.5.0)\n P moments       * 0.14.1  2022-05-02 [?] CRAN (R 4.5.0)\n P nlme          * 3.1-168 2025-03-31 [?] CRAN (R 4.4.3)\n P nloptr          2.2.1   2025-03-17 [?] CRAN (R 4.5.0)\n P openxlsx      * 4.2.8   2025-01-25 [?] CRAN (R 4.5.0)\n P patchwork     * 1.3.0   2024-09-16 [?] CRAN (R 4.5.0)\n P pillar          1.10.2  2025-04-05 [?] CRAN (R 4.5.0)\n P pkgbuild        1.4.7   2025-03-24 [?] CRAN (R 4.5.0)\n P pkgconfig       2.0.3   2019-09-22 [?] CRAN (R 4.5.0)\n P pkgload         1.4.0   2024-06-28 [?] CRAN (R 4.5.0)\n P plyr            1.8.9   2023-10-02 [?] CRAN (R 4.5.0)\n P profvis         0.4.0   2024-09-20 [?] CRAN (R 4.5.0)\n P promises        1.3.2   2024-11-28 [?] CRAN (R 4.5.0)\n P proxy           0.4-27  2022-06-09 [?] CRAN (R 4.5.0)\n P purrr         * 1.0.4   2025-02-05 [?] CRAN (R 4.5.0)\n P qgam          * 2.0.0   2025-04-10 [?] CRAN (R 4.5.0)\n P R6              2.6.1   2025-02-15 [?] CRAN (R 4.5.0)\n P rbibutils       2.3     2024-10-04 [?] CRAN (R 4.5.0)\n P RColorBrewer    1.1-3   2022-04-03 [?] CRAN (R 4.5.0)\n P Rcpp            1.0.14  2025-01-12 [?] CRAN (R 4.5.0)\n P Rdpack          2.6.4   2025-04-09 [?] CRAN (R 4.5.0)\n P readr         * 2.1.5   2024-01-10 [?] CRAN (R 4.5.0)\n P readxl        * 1.4.5   2025-03-07 [?] CRAN (R 4.5.0)\n P reformulas      0.4.1   2025-04-30 [?] CRAN (R 4.5.0)\n P remotes         2.5.0   2024-03-17 [?] CRAN (R 4.5.0)\n P renv            1.1.4   2025-03-20 [?] CRAN (R 4.5.0)\n P rlang           1.1.6   2025-04-11 [?] CRAN (R 4.5.0)\n P rmarkdown       2.29    2024-11-04 [?] CRAN (R 4.5.0)\n P rnaturalearth * 1.0.1   2023-12-15 [?] CRAN (R 4.5.0)\n P rstudioapi      0.17.1  2024-10-22 [?] CRAN (R 4.5.0)\n P scales          1.4.0   2025-04-24 [?] CRAN (R 4.5.0)\n P sessioninfo     1.2.3   2025-02-05 [?] CRAN (R 4.5.0)\n P sf            * 1.0-20  2025-03-24 [?] CRAN (R 4.5.0)\n P shiny           1.10.0  2024-12-14 [?] CRAN (R 4.5.0)\n P stringi         1.8.7   2025-03-27 [?] CRAN (R 4.5.0)\n P stringr       * 1.5.1   2023-11-14 [?] CRAN (R 4.5.0)\n P terra           1.8-42  2025-04-02 [?] CRAN (R 4.5.0)\n P tibble          3.2.1   2023-03-20 [?] CRAN (R 4.5.0)\n P tidyr         * 1.3.1   2024-01-24 [?] CRAN (R 4.5.0)\n P tidyselect      1.2.1   2024-03-11 [?] CRAN (R 4.5.0)\n P timechange      0.3.0   2024-01-18 [?] CRAN (R 4.5.0)\n P tzdb            0.5.0   2025-03-15 [?] CRAN (R 4.5.0)\n P units           0.8-7   2025-03-11 [?] CRAN (R 4.5.0)\n P urlchecker      1.0.1   2021-11-30 [?] CRAN (R 4.5.0)\n P usethis         3.1.0   2024-11-26 [?] CRAN (R 4.5.0)\n P utf8            1.2.5   2025-05-01 [?] CRAN (R 4.5.0)\n P vctrs           0.6.5   2023-12-01 [?] CRAN (R 4.5.0)\n P viridis         0.6.5   2024-01-29 [?] CRAN (R 4.5.0)\n P viridisLite     0.4.2   2023-05-02 [?] CRAN (R 4.5.0)\n P vroom           1.6.5   2023-12-05 [?] CRAN (R 4.5.0)\n P withr           3.0.2   2024-10-28 [?] CRAN (R 4.5.0)\n P xfun            0.52    2025-04-02 [?] CRAN (R 4.5.0)\n P xml2            1.3.8   2025-03-14 [?] CRAN (R 4.5.0)\n P xtable          1.8-4   2019-04-21 [?] CRAN (R 4.5.0)\n P yaml            2.3.10  2024-07-26 [?] CRAN (R 4.5.0)\n P zip             2.3.2   2025-02-01 [?] CRAN (R 4.5.0)\n\n [1] /home/steffi/Projects/vulture_migration/renv/library/linux-ubuntu-noble/R-4.5/x86_64-pc-linux-gnu\n [2] /home/steffi/.cache/R/renv/sandbox/linux-ubuntu-noble/R-4.5/x86_64-pc-linux-gnu/9a444a72\n\n * ── Packages attached to the search path.\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────",
    "crumbs": [
      "Workflow",
      "Initial Exploration"
    ]
  },
  {
    "objectID": "01_initial_exploration.html#footnotes",
    "href": "01_initial_exploration.html#footnotes",
    "title": "Initial Exploration",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://noamross.github.io/gams-in-r-course/chapter1↩︎\nSimilar to a method I used in a paper with Matt Reudink on Bluebirds and also one on Swifts (companion to the one you referenced). We used this to calculate a threshold for latitude to define the start and end of migration by population postition rather than counts, though.↩︎",
    "crumbs": [
      "Workflow",
      "Initial Exploration"
    ]
  },
  {
    "objectID": "03_analysis.html",
    "href": "03_analysis.html",
    "title": "Analysis",
    "section": "",
    "text": "source(\"XX_functions.R\")  # Custom functions and packages\n\n# Metrics\nv &lt;- read_csv(\"Data/Datasets/vultures_final.csv\") |&gt;\n  # Round non-integer values of population counts\n  mutate(across(c(contains(\"pop\"), contains(\"raw\")), round)) \n\n# Raw counts\nraw &lt;- read_csv(\"Data/Datasets/vultures_clean_2023.csv\")\n\n# Predicted GAM models\npred &lt;- read_csv(\"Data/Datasets/vultures_gams_pred.csv\")\n\nSet plotting defaults\n\ny_breaks &lt;- seq(200, 300, by = 5)",
    "crumbs": [
      "Workflow",
      "Analysis"
    ]
  },
  {
    "objectID": "03_analysis.html#setup",
    "href": "03_analysis.html#setup",
    "title": "Analysis",
    "section": "",
    "text": "source(\"XX_functions.R\")  # Custom functions and packages\n\n# Metrics\nv &lt;- read_csv(\"Data/Datasets/vultures_final.csv\") |&gt;\n  # Round non-integer values of population counts\n  mutate(across(c(contains(\"pop\"), contains(\"raw\")), round)) \n\n# Raw counts\nraw &lt;- read_csv(\"Data/Datasets/vultures_clean_2023.csv\")\n\n# Predicted GAM models\npred &lt;- read_csv(\"Data/Datasets/vultures_gams_pred.csv\")\n\nSet plotting defaults\n\ny_breaks &lt;- seq(200, 300, by = 5)",
    "crumbs": [
      "Workflow",
      "Analysis"
    ]
  },
  {
    "objectID": "03_analysis.html#using-this-report",
    "href": "03_analysis.html#using-this-report",
    "title": "Analysis",
    "section": "Using This Report",
    "text": "Using This Report\nThis report contains all the stats and analyses for the final data set.\nEach section (e.g., “Timing of kettle formation”) contains the analysis for a particular type of data.\nThese analyses have several parts\n\nDescriptive statistics - min, max, median, etc.\nModel results - results of linear or general linear models, depending on the measures\nFigures - visualizing the models on top of the data\nModel Checks - DHARMa plots assessing model fit and assumptions\nSensitivity - Checking influential observations\nFull Model Results - the raw summary output of the models, for completeness\n\n\nModels\nMost models are standard linear regressions.\nSome, involving the count data (number of birds) are either Negative binomial or Poisson generalized linear models.\nThe Patterns of migration (skew and kurtosis) are linear models without predictors (intercept only), which effectively makes them t-tests (they produce the exact same estimates and statistics), but allow us to use DHARMA for model checking, etc.\n\n\nModel Results\nModel results are presented in a tabulated form, which is also saved to CSV as Data/Tables/table_MEASURE.csv.\nThey are also presented as the “Full Model Results”. This is the output of using summary(model) in R and is just a complete version of the output in case anything needs to be inspected at a later date. In short, I don’t think you’ll need it, but I’ve included it just in case.\nThe tabulated results contain both estimates and P values, as well as overall model R2 values for linear models (but not for general linear models, as they don’t really apply). I could also add confidence intervals as well if that’s useful.\nR2 values are presented as regular and adjusted. The adjusted values take into account samples sizes and the number of predictors.\nWe can interpret the Estimates directly, especially in linear models.\nFor example, if there is a significant effect of year on mig_start_doy, and the estimate is 0.176 we can say:\n\n“There was a significant effect of year on the start date of kettle formations, such that the start date increased by 0.176 days per year (t = XXX; P-value = XXX). Thus kettle formations started ~4.4 days later at the end of the study compared to the start.”\n\nConsidering that there are 25 years in this study.\nFor Negative Binomial or Poisson models, we will transform the estimates to “Incident Rate Ratios” by exponentiating the estimate.\nFor example, if there is a significant effect of year on mig_pop_max in a generalized linear model with the negative binomial family, and the estimate is 0.03625499, we can first calculate the exponential:\n\nexp(0.03625499)\n\n[1] 1.03692\n\n\nThen we can say:\n\n“There was a significant effect of year on the maximum number of vultures predicted in a kettle (Est = 0.0363; t = XXX; P-value = XXX), such that the number of birds increased by 3% per year.”\n\nIf our exponentiated result was 3, we would say a “300% increase”, or a “3-fold” increase.\nThese exponentiated estimates are included in the outputs as well as “Estimate Exp”\n\n\nVariables\nAll variables are generally referred to by their name in the data set, e.g., mig_start_doy.\nSee the glossary of variables names in the Calculate Metrics &gt; Data Details section.\n\n\nFigures\nYou can click on any figure expand it out.\nThese figures are here to demostrate the patterns we’re seeing in the data and the stats. I imagine you would want different figures for publication, so just let me know what they should look like!\n\n\nDHARMa plots\nDHARMa is a package for simulating residuals to allow model checking for all types of models (details).\nGenerally these plots can be interpreted similarly to interpreting QQ Normal and Residual plots for linear models (although technically they’re using simulated residuals, not model residuals).\nFor context…\nFor example, we would interpret the following plot as showing decent model fit and assumptions.\nThe QQ plot on the left is nearly straight, there are no tests that flag problems in the distribution (KS test), Dispersion or Outliers. We also see no pattern in the residuals and no problems have been highlighted.\n\n\n\n\n\n\n\n\n\nSimilarly, I wouldn’t be unduly worried about the scary red line in this plot, as I don’t really find the residuals to show much pattern and there are no other problems highlighted.\n\n\n\n\n\n\n\n\n\nThis one starts to look a bit problematic…\n\n\n\n\n\n\n\n\n\nThis one is awful!\n\n\n\n\n\n\n\n\n\n\n\nSensitivity\nThe DHARMa model checks will usually catch outlier problems, but out of an abundance of caution, we’ll do a quick review of any high Cook’s D values and will rerun the models without certain data points to ensure they’re still acceptable.\nThese sections first calculate and show the Cook’s D for all observations in each model. In these tables, values highlighted in red are more than 4/25 (a standard Cook’s D cutoff 4/sample size). These indicate potential influential data points for a particular model.\nThen I run a comparison of the model before and the after removing that point.\nIn every case here the pattern was either strengthened or weakened by the removal, but the overall pattern did not change (no Estimate signs changed, and significance changed from sig to non-sig or vice versa).\n\nStrengthened patterns showed greater Estimates and smaller P values\nWeakened patterns showed smaller Estimates and greater P values\n\nTables with coefficients for both original and new model are shown, with colour intensity showing strength in the pattern.",
    "crumbs": [
      "Workflow",
      "Analysis"
    ]
  },
  {
    "objectID": "03_analysis.html#timing-of-kettle-formation",
    "href": "03_analysis.html#timing-of-kettle-formation",
    "title": "Analysis",
    "section": "Timing of kettle formation",
    "text": "Timing of kettle formation\nWhen do kettles form? Has this timing changed over the years?\n\nLook for changes in the DOY of the 5%, 25%, 50%, 75%, 95%, of passage as well as the DOY with the highest predicted count.\nmig_start_doy, peak_start_doy, p50_doy, peak_end_doy, mig_end_doy, max_doy\n\n\nDescriptive stats\n\n\nCode\nv |&gt; \n  select(contains(\"start_doy\"), contains(\"50_doy\"), contains(\"end_doy\"), \"max_doy\") |&gt;\n  desc_stats()\n\n\n\n\n\n\n\n\nmeasure\nmean\nsd\nmin\nmedian\nmax\nn\n\n\n\n\nmig_start_doy\n259.50\n3.02\n253\n259.5\n265\n24\n\n\npeak_start_doy\n266.88\n2.31\n262\n267.0\n272\n24\n\n\np50_doy\n271.62\n2.32\n268\n271.0\n277\n24\n\n\nmig_end_doy\n283.79\n3.08\n277\n284.0\n289\n24\n\n\npeak_end_doy\n276.42\n2.34\n272\n276.0\n281\n24\n\n\nmax_doy\n271.71\n2.69\n266\n271.0\n277\n24\n\n\n\n\n\n\n\n\nIndividual ModelsSingle ModelFiguresSensitivityProblematic YearsFull Model ResultsModel Checks\n\n\nHere we look at linear regressions of year by kettle timing dates.\n\nm1 &lt;- lm(mig_start_doy ~ year, data = v)\nm2 &lt;- lm(peak_start_doy ~ year, data = v)\nm3 &lt;- lm(p50_doy ~ year, data = v)\nm4 &lt;- lm(peak_end_doy ~ year, data = v)\nm5 &lt;- lm(mig_end_doy ~ year, data = v)\nm6 &lt;- lm(max_doy ~ year, data = v)\n\nmodels &lt;- list(m1, m2, m3, m4, m5, m6)\n\nTabulated Model Results\nAll show a significant increase in doy except mig_end_doy. Data saved to Data/Datasets/table_timing.csv\n\n\nCode\nt1 &lt;- get_table(models)\nwrite_csv(t1, d)\nfmt_table(t1)\n\n\n\n\n\n\n\n\nterm\nestimate\nstd error\nstatistic\np value\nn\n\n\n\n\nmig_start_doy ~ year (R2 = 0.26; R2-adj = 0.23)\n\n\n(Intercept)\n−155.793\n149.151\n−1.045\n0.308\n24\n\n\nyear\n0.206\n0.074\n2.784\n0.011\n24\n\n\npeak_start_doy ~ year (R2 = 0.33; R2-adj = 0.3)\n\n\n(Intercept)\n−91.217\n108.347\n−0.842\n0.409\n24\n\n\nyear\n0.178\n0.054\n3.305\n0.003\n24\n\n\np50_doy ~ year (R2 = 0.28; R2-adj = 0.25)\n\n\n(Intercept)\n−58.259\n112.980\n−0.516\n0.611\n24\n\n\nyear\n0.164\n0.056\n2.920\n0.008\n24\n\n\npeak_end_doy ~ year (R2 = 0.25; R2-adj = 0.21)\n\n\n(Intercept)\n−37.534\n116.392\n−0.322\n0.750\n24\n\n\nyear\n0.156\n0.058\n2.697\n0.013\n24\n\n\nmig_end_doy ~ year (R2 = 0.08; R2-adj = 0.04)\n\n\n(Intercept)\n44.280\n169.169\n0.262\n0.796\n24\n\n\nyear\n0.119\n0.084\n1.416\n0.171\n24\n\n\nmax_doy ~ year (R2 = 0.28; R2-adj = 0.25)\n\n\n(Intercept)\n−110.936\n131.392\n−0.844\n0.408\n24\n\n\nyear\n0.190\n0.065\n2.912\n0.008\n24\n\n\n\n\n\n\n\n\n\nHere we look at a linear regression of year by kettle timing dates. The idea is to explore whether the interaction is significant, which would indicate that the rate of change in migration phenology differs among the seasonal points (start, start of peak, peak, end of peak, end).\nWe look at the effect of year and type of measurement (migration start/end, peak start/end and time of 50% passage) on date (day of year).\n\n\nCode\nv1 &lt;- select(v, ends_with(\"doy\"), year) |&gt;\n  pivot_longer(-year, names_to = \"measure\", values_to = \"doy\")\n\n\nFirst we look at the interaction term to see if the slopes of change among measurements are significantly different from one another.\nThe type III ANOVA shows no significant interaction.\n\n\nCode\nm0 &lt;- lm(doy ~ measure * year, data = v1)\ncar::Anova(m0, type = \"III\")\n\n\nAnova Table (Type III tests)\n\nResponse: doy\n             Sum Sq  Df F value   Pr(&gt;F)   \n(Intercept)    3.90   1  0.6954 0.405829   \nmeasure        7.53   5  0.2681 0.929845   \nyear          46.46   1  8.2739 0.004691 **\nmeasure:year   5.93   5  0.2113 0.957236   \nResiduals    741.14 132                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe’ll look at a post-hoc comparison of the slopes to interpret them individually.\n\nThis is nearly identical to running the models separately, above\n\nAs with the individual models, we can see that each measure shows a significant positive slope with the exception of the End of Migration (mig_end_day).\n\n\nCode\ne &lt;- emmeans::emtrends(m0, ~ measure, var = \"year\") |&gt; \n  emmeans::test()\n\n\nAll show a significant increase in doy except mig_end_doy. Data saved to Data/Datasets/table_timing_combined.csv\n\n\nCode\nt2 &lt;- e |&gt; \n  mutate(\n    model = \"doy ~ measure * year\",\n    n = nrow(v)) |&gt;\n  rename(\n    estimate = year.trend, \n    std_error = SE, \n    \n    statistic = t.ratio, \n    p_value = p.value)\n\nwrite_csv(t2, d)\n\nfmt_table(t2) |&gt;\n  tab_header(title = \"Post-hoc results\") |&gt;\n  tab_footnote(\"Number of years in the data.\", locations = cells_column_labels(n)) |&gt;\n  tab_footnote(\"T statistic.\", locations = cells_column_labels(\"statistic\"))\n\n\n\n\n\n\n\n\nPost-hoc results\n\n\nmeasure\nestimate\nstd error\ndf\nstatistic1\np value\nn2\n\n\n\n\ndoy ~ measure * year\n\n\nmax_doy\n0.190\n0.066\n132\n2.876\n0.005\n24\n\n\nmig_end_doy\n0.119\n0.066\n132\n1.800\n0.074\n24\n\n\nmig_start_doy\n0.206\n0.066\n132\n3.122\n0.002\n24\n\n\np50_doy\n0.164\n0.066\n132\n2.480\n0.014\n24\n\n\npeak_end_doy\n0.156\n0.066\n132\n2.360\n0.020\n24\n\n\npeak_start_doy\n0.178\n0.066\n132\n2.692\n0.008\n24\n\n\n\n1 T statistic.\n\n\n2 Number of years in the data.\n\n\n\n\n\n\n\n\n\n\nNote: Lines without CI ribbons are not significant\n\n\nCode\ndoy_figs &lt;- v |&gt;\n  select(year, contains(\"doy\")) |&gt;\n  pivot_longer(-year, names_to = \"measure\", values_to = \"doy\") |&gt;\n  left_join(mutate(t1, measure = str_trim(str_extract(model, \"^[^~]*\")), sig = p_value &lt;= 0.05) |&gt; \n              filter(term == \"year\") |&gt;\n              select(measure, sig), by = \"measure\")\n\nggplot(doy_figs, aes(x = year, y = doy, group = measure, colour = measure, fill = sig)) +\n  theme_bw() +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  scale_y_continuous(breaks = y_breaks) +\n  scale_colour_viridis_d() +\n  scale_fill_manual(values = c(\"TRUE\" = \"grey60\", \"FALSE\" = \"#FFFFFF00\"), guide = \"none\") +\n  labs(caption = \"Lines without CI Error ribbons represent non-significant models\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nThe DHARMa model checks don’t highlight any particular outlier problems, but out of an abundance of caution, we’ll do a quick review of any high Cook’s D values.\nValues highlighted in red are less than 4/25, a standard Cook’s D cutoff (4/sample size). These indicate potential influential data points for a particular model.\n\n\nCode\nget_cooks(models) |&gt;\n  gt_cooks(width = \"80%\")\n\n\n\n\n\n\n\n\nCook's Distances\n\n\nyear\nmig_start_doy\npeak_start_doy\np50_doy\npeak_end_doy\nmig_end_doy\nmax_doy\n\n\n\n\n1999\n0.25\n0.16\n0.05\n0.01\n0.00\n0.05\n\n\n2000\n0.23\n0.21\n0.07\n0.06\n0.02\n0.01\n\n\n2001\n0.13\n0.00\n0.00\n0.01\n0.06\n0.02\n\n\n2002\n0.00\n0.03\n0.07\n0.00\n0.04\n0.19\n\n\n2003\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n\n\n2004\n0.11\n0.07\n0.08\n0.08\n0.14\n0.02\n\n\n2005\n0.03\n0.01\n0.00\n0.00\n0.00\n0.00\n\n\n2006\n0.01\n0.04\n0.01\n0.00\n0.01\n0.00\n\n\n2008\n0.03\n0.01\n0.01\n0.01\n0.02\n0.00\n\n\n2009\n0.01\n0.00\n0.03\n0.10\n0.12\n0.02\n\n\n2010\n0.02\n0.01\n0.00\n0.00\n0.01\n0.01\n\n\n2011\n0.04\n0.06\n0.11\n0.07\n0.01\n0.12\n\n\n2012\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\n2013\n0.02\n0.02\n0.03\n0.06\n0.02\n0.04\n\n\n2014\n0.00\n0.02\n0.02\n0.06\n0.04\n0.01\n\n\n2015\n0.01\n0.00\n0.01\n0.01\n0.07\n0.03\n\n\n2016\n0.03\n0.07\n0.05\n0.01\n0.00\n0.02\n\n\n2017\n0.00\n0.01\n0.02\n0.05\n0.09\n0.00\n\n\n2018\n0.01\n0.01\n0.01\n0.00\n0.00\n0.08\n\n\n2019\n0.01\n0.02\n0.05\n0.03\n0.04\n0.05\n\n\n2020\n0.06\n0.22\n0.24\n0.15\n0.07\n0.15\n\n\n2021\n0.13\n0.14\n0.09\n0.15\n0.13\n0.09\n\n\n2022\n0.13\n0.11\n0.06\n0.02\n0.00\n0.08\n\n\n2023\n0.13\n0.03\n0.01\n0.00\n0.01\n0.00\n\n\n\n\n\n\n\n\nCheck influential points\nNow let’s see what happens if we were to omit these years from the respective analyses.\nTables with coefficients for both original and new model are shown, with colour intensity showing strength in the pattern.\nFor the start of migration, if we omit 1999, the pattern is stronger, but the same, if we omit 2000, the pattern is weaker but still the same.\n\n\nCode\ncompare(m1, 1999)\n\n\n\n\n\n\n\n\nmodel\nparameter\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\nOriginal - mig_start_doy\n(Intercept)\n-155.793\n149.151\n-1.045\n0.308\n\n\nDrop 1999 - mig_start_doy\n(Intercept)\n-246.743\n152.473\n-1.618\n0.121\n\n\nOriginal - mig_start_doy\nyear\n0.206\n0.074\n2.784\n0.011\n\n\nDrop 1999 - mig_start_doy\nyear\n0.252\n0.076\n3.320\n0.003\n\n\n\n\n\n\n\nCode\ncompare(m1, 2000)\n\n\n\n\n\n\n\n\nmodel\nparameter\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\nOriginal - mig_start_doy\n(Intercept)\n-155.793\n149.151\n-1.045\n0.308\n\n\nDrop 2000 - mig_start_doy\n(Intercept)\n-70.361\n150.113\n-0.469\n0.644\n\n\nOriginal - mig_start_doy\nyear\n0.206\n0.074\n2.784\n0.011\n\n\nDrop 2000 - mig_start_doy\nyear\n0.164\n0.075\n2.199\n0.039\n\n\n\n\n\n\n\nFor the start of peak migration\n\n\nCode\ncompare(m2, 2000)\n\n\n\n\n\n\n\n\nmodel\nparameter\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\nOriginal - peak_start_doy\n(Intercept)\n-91.217\n108.347\n-0.842\n0.409\n\n\nDrop 2000 - peak_start_doy\n(Intercept)\n-32.416\n109.888\n-0.295\n0.771\n\n\nOriginal - peak_start_doy\nyear\n0.178\n0.054\n3.305\n0.003\n\n\nDrop 2000 - peak_start_doy\nyear\n0.149\n0.055\n2.726\n0.013\n\n\n\n\n\n\n\nCode\ncompare(m2, 2020)\n\n\n\n\n\n\n\n\nmodel\nparameter\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\nOriginal - peak_start_doy\n(Intercept)\n-91.217\n108.347\n-0.842\n0.409\n\n\nDrop 2020 - peak_start_doy\n(Intercept)\n-36.595\n104.279\n-0.351\n0.729\n\n\nOriginal - peak_start_doy\nyear\n0.178\n0.054\n3.305\n0.003\n\n\nDrop 2020 - peak_start_doy\nyear\n0.151\n0.052\n2.908\n0.008\n\n\n\n\n\n\n\nThe date of 50% passage\n\n\nCode\ncompare(m3, 2020)\n\n\n\n\n\n\n\n\nmodel\nparameter\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\nOriginal - p50_doy\n(Intercept)\n-58.259\n112.980\n-0.516\n0.611\n\n\nDrop 2020 - p50_doy\n(Intercept)\n2.113\n107.339\n0.020\n0.984\n\n\nOriginal - p50_doy\nyear\n0.164\n0.056\n2.920\n0.008\n\n\nDrop 2020 - p50_doy\nyear\n0.134\n0.053\n2.509\n0.020\n\n\n\n\n\n\n\nAnd the date of maximum counts\n\n\nCode\ncompare(m6, 2002)\n\n\n\n\n\n\n\n\nmodel\nparameter\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\nOriginal - max_doy\n(Intercept)\n-110.936\n131.392\n-0.844\n0.408\n\n\nDrop 2002 - max_doy\n(Intercept)\n-46.968\n128.776\n-0.365\n0.719\n\n\nOriginal - max_doy\nyear\n0.190\n0.065\n2.912\n0.008\n\n\nDrop 2002 - max_doy\nyear\n0.159\n0.064\n2.477\n0.022\n\n\n\n\n\n\n\n\n\n\n\n\n\nTherefore I wouldn’t be concerned\n\n\n\n\n\n\nIn addition to the sensitivity check using cook’s distances, here we will conduct a more targeted exploration of some specific years.\n\nGaps around the peak\nFirst, we consider removing 2011, 2013 as they have sampling gaps around the peak of migration\n\ny &lt;- c(2011, 2013)\nm1a &lt;- update(m1, data = filter(v, !year %in% y))\nm2a &lt;- update(m2, data = filter(v, !year %in% y))\nm3a &lt;- update(m3, data = filter(v, !year %in% y))\nm4a &lt;- update(m4, data = filter(v, !year %in% y))\nm5a &lt;- update(m5, data = filter(v, !year %in% y))\nm6a &lt;- update(m6, data = filter(v, !year %in% y))\n\nmodels_a &lt;- list(m1a, m2a, m3a, m4a, m5a, m6a)\n\nComparing the summary tables shows only superficial differences. If anything, it looks like removing these ‘gappy’ years only strengthens the existing patterns. Those which are significant are slightly more so, and the slopes are slightly greater.\nData saved to Data/Datasets/table_supplemental.csv\nOriginal\n\n\nCode\nfmt_summary(models, intercept = FALSE)\n\n\n\n\n\n\n\n\nmodel\nParameter\nEstimate\nStd. Error\nT\nP\n\n\n\n\nmig_start_doy ~ year\nyear\n0.206\n0.074\n2.784\n0.011\n\n\npeak_start_doy ~ year\nyear\n0.178\n0.054\n3.305\n0.003\n\n\np50_doy ~ year\nyear\n0.164\n0.056\n2.920\n0.008\n\n\npeak_end_doy ~ year\nyear\n0.156\n0.058\n2.697\n0.013\n\n\nmig_end_doy ~ year\nyear\n0.119\n0.084\n1.416\n0.171\n\n\nmax_doy ~ year\nyear\n0.190\n0.065\n2.912\n0.008\n\n\n\n\n\n\n\nCode\nto &lt;- get_table(models) |&gt;\n  select(model, term, estimate, std_error, statistic, p_value) |&gt;\n  mutate(type = \"original\")\nwrite_csv(to, d)\n\n\nYears removed\n\n\nCode\nfmt_summary(models_a, intercept = FALSE)\n\n\n\n\n\n\n\n\nmodel\nParameter\nEstimate\nStd. Error\nT\nP\n\n\n\n\nmig_start_doy ~ year\nyear\n0.204\n0.073\n2.784\n0.011\n\n\npeak_start_doy ~ year\nyear\n0.176\n0.051\n3.416\n0.003\n\n\np50_doy ~ year\nyear\n0.161\n0.050\n3.237\n0.004\n\n\npeak_end_doy ~ year\nyear\n0.151\n0.052\n2.928\n0.008\n\n\nmig_end_doy ~ year\nyear\n0.115\n0.085\n1.352\n0.192\n\n\nmax_doy ~ year\nyear\n0.186\n0.056\n3.324\n0.003\n\n\n\n\n\n\n\nCode\nta &lt;- get_table(models_a) |&gt;\n  select(model, term, estimate, std_error, statistic, p_value) |&gt;\n  mutate(type = \"gaps_removed\")\nwrite_csv(ta, d, append = TRUE)\n\n\n\n\nMissing the end of migration\nSecond, we consider removing 2001, 2006, 2023 as they were three years where it looks like sampling ended earlier than the end of migration.\n\ny &lt;- c(2001, 2006, 2023)\nm1b &lt;- update(m1, data = filter(v, !year %in% y))\nm2b &lt;- update(m2, data = filter(v, !year %in% y))\nm3b &lt;- update(m3, data = filter(v, !year %in% y))\nm4b &lt;- update(m4, data = filter(v, !year %in% y))\nm5b &lt;- update(m5, data = filter(v, !year %in% y))\nm6b &lt;- update(m6, data = filter(v, !year %in% y))\n\nmodels_b &lt;- list(m1b, m2b, m3b, m4b, m5b, m6b)\n\nRemoving these years from the analyses, reduced the strength of the patterns, but not by much. Further, there were few changes to the magnitude of the slopes.\nData saved to Data/Datasets/table_supplemental.csv\nOriginal\n\n\nCode\nfmt_summary(models, intercept = FALSE)\n\n\n\n\n\n\n\n\nmodel\nParameter\nEstimate\nStd. Error\nT\nP\n\n\n\n\nmig_start_doy ~ year\nyear\n0.206\n0.074\n2.784\n0.011\n\n\npeak_start_doy ~ year\nyear\n0.178\n0.054\n3.305\n0.003\n\n\np50_doy ~ year\nyear\n0.164\n0.056\n2.920\n0.008\n\n\npeak_end_doy ~ year\nyear\n0.156\n0.058\n2.697\n0.013\n\n\nmig_end_doy ~ year\nyear\n0.119\n0.084\n1.416\n0.171\n\n\nmax_doy ~ year\nyear\n0.190\n0.065\n2.912\n0.008\n\n\n\n\n\n\n\nYears removed\n\n\nCode\nfmt_summary(models_b, intercept = FALSE)\n\n\n\n\n\n\n\n\nmodel\nParameter\nEstimate\nStd. Error\nT\nP\n\n\n\n\nmig_start_doy ~ year\nyear\n0.198\n0.082\n2.410\n0.026\n\n\npeak_start_doy ~ year\nyear\n0.178\n0.063\n2.825\n0.011\n\n\np50_doy ~ year\nyear\n0.167\n0.068\n2.467\n0.023\n\n\npeak_end_doy ~ year\nyear\n0.147\n0.070\n2.119\n0.048\n\n\nmig_end_doy ~ year\nyear\n0.077\n0.099\n0.781\n0.445\n\n\nmax_doy ~ year\nyear\n0.198\n0.078\n2.530\n0.020\n\n\n\n\n\n\n\nCode\ntb &lt;- get_table(models_b) |&gt;\n  select(model, term, estimate, std_error, statistic, p_value) |&gt;\n  mutate(type = \"end_removed\")\nwrite_csv(tb, d, append = TRUE)\n\n\n\n\nRemove all problematic years\nOut of curiosity, let’s try removing all 5 years. This reduces our data from 25 to 19 samples, so is a substantial change and we may see non-significant results merely because we have removed too many samples.\n\ny &lt;- c(2011, 2013, 2001, 2006, 2023)\nm1c &lt;- update(m1, data = filter(v, !year %in% y))\nm2c &lt;- update(m2, data = filter(v, !year %in% y))\nm3c &lt;- update(m3, data = filter(v, !year %in% y))\nm4c &lt;- update(m4, data = filter(v, !year %in% y))\nm5c &lt;- update(m5, data = filter(v, !year %in% y))\nm6c &lt;- update(m6, data = filter(v, !year %in% y))\n\nmodels_c &lt;- list(m1c, m2c, m3c, m4c, m5c, m6c)\n\nRemoving these years from the analyses, reduced the strength of the patterns, but not by much. Further, there are few changes to the magnitude of the slopes.\nData saved to Data/Datasets/table_supplemental.csv\nOriginal\n\n\nCode\nfmt_summary(models, intercept = FALSE)\n\n\n\n\n\n\n\n\nmodel\nParameter\nEstimate\nStd. Error\nT\nP\n\n\n\n\nmig_start_doy ~ year\nyear\n0.206\n0.074\n2.784\n0.011\n\n\npeak_start_doy ~ year\nyear\n0.178\n0.054\n3.305\n0.003\n\n\np50_doy ~ year\nyear\n0.164\n0.056\n2.920\n0.008\n\n\npeak_end_doy ~ year\nyear\n0.156\n0.058\n2.697\n0.013\n\n\nmig_end_doy ~ year\nyear\n0.119\n0.084\n1.416\n0.171\n\n\nmax_doy ~ year\nyear\n0.190\n0.065\n2.912\n0.008\n\n\n\n\n\n\n\nYears removed\n\n\nCode\nfmt_summary(models_c, intercept = FALSE)\n\n\n\n\n\n\n\n\nmodel\nParameter\nEstimate\nStd. Error\nT\nP\n\n\n\n\nmig_start_doy ~ year\nyear\n0.196\n0.082\n2.387\n0.029\n\n\npeak_start_doy ~ year\nyear\n0.176\n0.061\n2.893\n0.010\n\n\np50_doy ~ year\nyear\n0.165\n0.061\n2.715\n0.015\n\n\npeak_end_doy ~ year\nyear\n0.143\n0.063\n2.270\n0.036\n\n\nmig_end_doy ~ year\nyear\n0.073\n0.101\n0.722\n0.480\n\n\nmax_doy ~ year\nyear\n0.195\n0.067\n2.901\n0.010\n\n\n\n\n\n\n\nCode\ntc &lt;- get_table(models_c) |&gt;\n  select(model, term, estimate, std_error, statistic, p_value) |&gt;\n  mutate(type = \"all_removed\")\nwrite_csv(tc, d, append = TRUE)\n\n\n\n\nConclusions\nI do not believe that these potentially problematic sampling years are driving the patterns we see in this data.\nAt worst, by removing these years we increase the error around a slope. But the magnitudes of the slopes do not change very much and even the significance is only ever reduced only a little.\n\n\n\n\n\nCode\nmodels &lt;- append(list(m0), models)\nmap(models, summary)\n\n\n[[1]]\n\nCall:\nlm(formula = doy ~ measure * year, data = v1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.5336 -1.5933 -0.3033  1.6795  5.3234 \n\nCoefficients:\n                             Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)                -110.93571  133.02821  -0.834  0.40583   \nmeasuremig_end_doy          155.21571  188.13030   0.825  0.41084   \nmeasuremig_start_doy        -44.85714  188.13030  -0.238  0.81191   \nmeasurep50_doy               52.67714  188.13030   0.280  0.77991   \nmeasurepeak_end_doy          73.40143  188.13030   0.390  0.69705   \nmeasurepeak_start_doy        19.71857  188.13030   0.105  0.91668   \nyear                          0.19026    0.06614   2.876  0.00469 **\nmeasuremig_end_doy:year      -0.07117    0.09354  -0.761  0.44812   \nmeasuremig_start_doy:year     0.01623    0.09354   0.174  0.86249   \nmeasurep50_doy:year          -0.02623    0.09354  -0.280  0.77957   \nmeasurepeak_end_doy:year     -0.03416    0.09354  -0.365  0.71559   \nmeasurepeak_start_doy:year   -0.01221    0.09354  -0.131  0.89637   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.37 on 132 degrees of freedom\nMultiple R-squared:  0.9189,    Adjusted R-squared:  0.9122 \nF-statistic:   136 on 11 and 132 DF,  p-value: &lt; 2.2e-16\n\n\n[[2]]\n\nCall:\nlm(formula = mig_start_doy ~ year, data = v)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.1942 -2.2945 -0.3461  2.2601  4.0123 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) -155.79286  149.15112  -1.045   0.3076  \nyear           0.20649    0.07416   2.784   0.0108 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.657 on 22 degrees of freedom\nMultiple R-squared:  0.2606,    Adjusted R-squared:  0.227 \nF-statistic: 7.753 on 1 and 22 DF,  p-value: 0.01081\n\n\n[[3]]\n\nCall:\nlm(formula = peak_start_doy ~ year, data = v)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.8868 -1.2496 -0.5234  1.6650  3.5522 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept) -91.21714  108.34732  -0.842  0.40891   \nyear          0.17805    0.05387   3.305  0.00322 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.93 on 22 degrees of freedom\nMultiple R-squared:  0.3318,    Adjusted R-squared:  0.3014 \nF-statistic: 10.92 on 1 and 22 DF,  p-value: 0.003223\n\n\n[[4]]\n\nCall:\nlm(formula = p50_doy ~ year, data = v)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.4178 -1.6347 -0.5897  1.4275  4.4023 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept) -58.25857  112.98029  -0.516  0.61124   \nyear          0.16403    0.05618   2.920  0.00794 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.012 on 22 degrees of freedom\nMultiple R-squared:  0.2793,    Adjusted R-squared:  0.2465 \nF-statistic: 8.526 on 1 and 22 DF,  p-value: 0.007935\n\n\n[[5]]\n\nCall:\nlm(formula = peak_end_doy ~ year, data = v)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.0784 -1.0541 -0.3590  0.8667  3.6094 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept) -37.53429  116.39244  -0.322   0.7501  \nyear          0.15610    0.05787   2.697   0.0132 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.073 on 22 degrees of freedom\nMultiple R-squared:  0.2485,    Adjusted R-squared:  0.2144 \nF-statistic: 7.276 on 1 and 22 DF,  p-value: 0.01316\n\n\n[[6]]\n\nCall:\nlm(formula = mig_end_doy ~ year, data = v)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.5336 -1.8432  0.0259  2.2461  5.0618 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  44.28000  169.16915   0.262    0.796\nyear          0.11909    0.08411   1.416    0.171\n\nResidual standard error: 3.013 on 22 degrees of freedom\nMultiple R-squared:  0.08351,   Adjusted R-squared:  0.04185 \nF-statistic: 2.005 on 1 and 22 DF,  p-value: 0.1708\n\n\n[[7]]\n\nCall:\nlm(formula = max_doy ~ year, data = v)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.9643 -1.7706 -0.3205  1.6187  5.3234 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept) -110.93571  131.39209  -0.844  0.40758   \nyear           0.19026    0.06533   2.912  0.00808 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.34 on 22 degrees of freedom\nMultiple R-squared:  0.2782,    Adjusted R-squared:  0.2454 \nF-statistic: 8.481 on 1 and 22 DF,  p-value: 0.008075\n\n\n\n\n\n\nCode\nmodel_check_figs(models)",
    "crumbs": [
      "Workflow",
      "Analysis"
    ]
  },
  {
    "objectID": "03_analysis.html#duration-of-migration",
    "href": "03_analysis.html#duration-of-migration",
    "title": "Analysis",
    "section": "Duration of migration",
    "text": "Duration of migration\nHow long is migration? Has it changed in length? \n\nLook for changes in the number of days over which migration and peak migration occur\nmig_dur_days, peak_dur_days\n\n\nDescriptive stats\n\n\nCode\nv |&gt; \n  select(contains(\"dur\")) |&gt;\n  desc_stats()\n\n\n\n\n\n\n\n\nmeasure\nmean\nsd\nmin\nmedian\nmax\nn\n\n\n\n\nmig_dur_days\n24.29\n3.43\n16\n25\n30\n24\n\n\npeak_dur_days\n9.54\n1.32\n6\n10\n11\n24\n\n\n\n\n\n\n\n\nModelsFiguresModel ChecksSensitivityProblematic YearsFull Model Results\n\n\n\nm1 &lt;- lm(mig_dur_days ~ year, data = v)\nm2 &lt;- lm(peak_dur_days ~ year, data = v)\n\nmodels &lt;- list(m1, m2)\n\nTabulated Results\nNo significant results\n\n\nCode\nt &lt;- get_table(models)\nwrite_csv(t, \"Data/Datasets/table_duration.csv\")\nfmt_table(t)\n\n\n\n\n\n\n\n\nterm\nestimate\nstd error\nstatistic\np value\nn\n\n\n\n\nmig_dur_days ~ year (R2 = 0.04; R2-adj = -0.01)\n\n\n(Intercept)\n200.073\n193.428\n1.034\n0.312\n24\n\n\nyear\n−0.087\n0.096\n−0.909\n0.373\n24\n\n\npeak_dur_days ~ year (R2 = 0.02; R2-adj = -0.03)\n\n\n(Intercept)\n53.683\n75.074\n0.715\n0.482\n24\n\n\nyear\n−0.022\n0.037\n−0.588\n0.563\n24\n\n\n\n\n\n\n\n\n\nNote: Lines without Std Error ribbons are not significant\n\n\nCode\ndur_figs &lt;- v |&gt;\n  select(year, contains(\"dur\")) |&gt;\n  pivot_longer(-year, names_to = \"measure\", values_to = \"dur\")\n\nggplot(dur_figs, aes(x = year, y = dur, group = measure, colour = measure)) +\n  theme_bw() +\n  geom_point() +\n  stat_smooth(method = \"lm\", se = FALSE) +\n  #scale_y_continuous(breaks = y_breaks) +\n  scale_colour_viridis_d()+\n  labs(caption = \"Lines without Std Error ribbons represent non-significant models\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nSome pattern to the upper quandrant residuals, but I think the more reflects what we’re seeing, that year is a poor predictor of migration duration.\n\n\nCode\nmodel_check_figs(models)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe DHARMa model checks don’t highlight any particular outlier problems, but out of an abundance of caution, we’ll do a quick review of any high Cook’s D values.\nValues highlighted in red are less than 4/25, a standard Cook’s D cutoff (4/sample size). These indicate potential influential data points for a particular model.\n\n\nCode\nget_cooks(models) |&gt;\n  gt_cooks()\n\n\n\n\n\n\n\n\nCook's Distances\n\n\nyear\nmig_dur_days\npeak_dur_days\n\n\n\n\n1999\n0.18\n0.20\n\n\n2000\n0.06\n0.08\n\n\n2001\n0.00\n0.03\n\n\n2002\n0.05\n0.06\n\n\n2003\n0.00\n0.00\n\n\n2004\n0.00\n0.00\n\n\n2005\n0.02\n0.00\n\n\n2006\n0.00\n0.04\n\n\n2008\n0.00\n0.00\n\n\n2009\n0.15\n0.18\n\n\n2010\n0.04\n0.03\n\n\n2011\n0.00\n0.00\n\n\n2012\n0.00\n0.00\n\n\n2013\n0.00\n0.03\n\n\n2014\n0.03\n0.03\n\n\n2015\n0.09\n0.00\n\n\n2016\n0.03\n0.05\n\n\n2017\n0.08\n0.04\n\n\n2018\n0.01\n0.07\n\n\n2019\n0.06\n0.00\n\n\n2020\n0.00\n0.00\n\n\n2021\n0.00\n0.00\n\n\n2022\n0.08\n0.08\n\n\n2023\n0.12\n0.03\n\n\n\n\n\n\n\n\nCheck influential points\nNow let’s see what happens if we were to omit these years from the respective analyses.\nTables with coefficients for both original and new model are shown, with colour intensity showing strength in the pattern.\nFor the number of days in peak migration, if we omit 1999, the pattern is stronger, but the same (i.e. not significant), we see the same for the number of days in peak migration for 1999 and 2009.\n\n\nCode\ncompare(m1, 1999)\n\n\n\n\n\n\n\n\nmodel\nparameter\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\nOriginal - mig_dur_days\n(Intercept)\n200.073\n193.428\n1.034\n0.312\n\n\nDrop 1999 - mig_dur_days\n(Intercept)\n298.792\n201.845\n1.480\n0.154\n\n\nOriginal - mig_dur_days\nyear\n-0.087\n0.096\n-0.909\n0.373\n\n\nDrop 1999 - mig_dur_days\nyear\n-0.136\n0.100\n-1.359\n0.188\n\n\n\n\n\n\n\nCode\ncompare(m2, 1999)\n\n\n\n\n\n\n\n\nmodel\nparameter\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\nOriginal - peak_dur_days\n(Intercept)\n53.683\n75.074\n0.715\n0.482\n\n\nDrop 1999 - peak_dur_days\n(Intercept)\n94.682\n77.803\n1.217\n0.237\n\n\nOriginal - peak_dur_days\nyear\n-0.022\n0.037\n-0.588\n0.563\n\n\nDrop 1999 - peak_dur_days\nyear\n-0.042\n0.039\n-1.093\n0.287\n\n\n\n\n\n\n\nCode\ncompare(m2, 2009)\n\n\n\n\n\n\n\n\nmodel\nparameter\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\nOriginal - peak_dur_days\n(Intercept)\n53.683\n75.074\n0.715\n0.482\n\n\nDrop 2009 - peak_dur_days\n(Intercept)\n66.605\n62.405\n1.067\n0.298\n\n\nOriginal - peak_dur_days\nyear\n-0.022\n0.037\n-0.588\n0.563\n\n\nDrop 2009 - peak_dur_days\nyear\n-0.028\n0.031\n-0.912\n0.372\n\n\n\n\n\n\n\n\n\n\n\n\n\nTherefore I wouldn’t be concerned\n\n\n\n\n\n\nIn addition to the sensitivity check using cook’s distances, here we will conduct a more targeted exploration of some specific years.\n\nGaps around the peak\nFirst, we consider removing 2011, 2013 as they have sampling gaps around the peak of migration\n\ny &lt;- c(2011, 2013)\nm1a &lt;- update(m1, data = filter(v, !year %in% y))\nm2a &lt;- update(m2, data = filter(v, !year %in% y))\nmodels_a &lt;- list(m1a, m2a)\n\nComparing the summary tables shows only superficial differences.\nData saved to Data/Datasets/table_supplemental.csv\nOriginal\nfmt_summary(models, intercept = FALSE)\nto &lt;- get_table(models) |&gt;\n  select(model, term, estimate, std_error, statistic, p_value) |&gt;\n  mutate(type = \"original\")\nwrite_csv(to, d, append = TRUE)\n\n\n\n\n\n\n\n\nmodel\nParameter\nEstimate\nStd. Error\nT\nP\n\n\n\n\nmig_dur_days ~ year\nyear\n−0.087\n0.096\n−0.909\n0.373\n\n\npeak_dur_days ~ year\nyear\n−0.022\n0.037\n−0.588\n0.563\n\n\n\n\n\n\n\nYears removed\nfmt_summary(models_a, intercept = FALSE)\nta &lt;- get_table(models_a) |&gt;\n  select(model, term, estimate, std_error, statistic, p_value) |&gt;\n  mutate(type = \"gaps_removed\")\nwrite_csv(ta, d, append = TRUE)\n\n\n\n\n\n\n\n\nmodel\nParameter\nEstimate\nStd. Error\nT\nP\n\n\n\n\nmig_dur_days ~ year\nyear\n−0.089\n0.101\n−0.883\n0.388\n\n\npeak_dur_days ~ year\nyear\n−0.024\n0.038\n−0.638\n0.531\n\n\n\n\n\n\n\n\n\nMissing the end of migration\nSecond, we consider removing 2001, 2006, 2023 as they were three years where it looks like sampling ended earlier than the end of migration.\n\ny &lt;- c(2001, 2006, 2023)\nm1b &lt;- update(m1, data = filter(v, !year %in% y))\nm2b &lt;- update(m2, data = filter(v, !year %in% y))\nmodels_b &lt;- list(m1b, m2b)\n\nComparing the summary tables shows only superficial differences.\nData saved to Data/Datasets/table_supplemental.csv\nOriginal\nfmt_summary(models, intercept = FALSE)\n\n\n\n\n\n\n\n\nmodel\nParameter\nEstimate\nStd. Error\nT\nP\n\n\n\n\nmig_dur_days ~ year\nyear\n−0.087\n0.096\n−0.909\n0.373\n\n\npeak_dur_days ~ year\nyear\n−0.022\n0.037\n−0.588\n0.563\n\n\n\n\n\n\n\nYears removed\nfmt_summary(models_b, intercept = FALSE)\ntb &lt;- get_table(models_b) |&gt;\n  select(model, term, estimate, std_error, statistic, p_value) |&gt;\n  mutate(type = \"end_removed\")\nwrite_csv(tb, d, append = TRUE)\n\n\n\n\n\n\n\n\nmodel\nParameter\nEstimate\nStd. Error\nT\nP\n\n\n\n\nmig_dur_days ~ year\nyear\n−0.121\n0.113\n−1.072\n0.297\n\n\npeak_dur_days ~ year\nyear\n−0.031\n0.043\n−0.713\n0.485\n\n\n\n\n\n\n\n\n\nRemove all problematic years\nOut of curiosity, let’s try removing all 5 years. This reduces our data from 25 to 19 samples, so is a substantial change and we may see non-significant results merely because we have removed too many samples.\n\ny &lt;- c(2011, 2013, 2001, 2006, 2023)\nm1c &lt;- update(m1, data = filter(v, !year %in% y))\nm2c &lt;- update(m2, data = filter(v, !year %in% y))\nmodels_c &lt;- list(m1c, m2c)\n\nComparing the summary tables shows only superficial differences.\nData saved to Data/Datasets/table_supplemental.csv\nOriginal\nfmt_summary(models, intercept = FALSE)\n\n\n\n\n\n\n\n\nmodel\nParameter\nEstimate\nStd. Error\nT\nP\n\n\n\n\nmig_dur_days ~ year\nyear\n−0.087\n0.096\n−0.909\n0.373\n\n\npeak_dur_days ~ year\nyear\n−0.022\n0.037\n−0.588\n0.563\n\n\n\n\n\n\n\nYears removed\nfmt_summary(models_c, intercept = FALSE)\ntc &lt;- get_table(models_c) |&gt;\n  select(model, term, estimate, std_error, statistic, p_value) |&gt;\n  mutate(type = \"all_removed\")\nwrite_csv(tc, d, append = TRUE)\n\n\n\n\n\n\n\n\nmodel\nParameter\nEstimate\nStd. Error\nT\nP\n\n\n\n\nmig_dur_days ~ year\nyear\n−0.123\n0.119\n−1.037\n0.314\n\n\npeak_dur_days ~ year\nyear\n−0.034\n0.044\n−0.761\n0.457\n\n\n\n\n\n\n\n\n\nConclusions\nI do not believe that these potentially problematic sampling years are driving the patterns we see in this data.\n\n\n\n\n\nCode\nmap(models, summary)\n\n\n[[1]]\n\nCall:\nlm(formula = mig_dur_days ~ year, data = v)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.4810 -1.8159  0.6308  2.3101  6.0434 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) 200.07286  193.42774   1.034    0.312\nyear         -0.08740    0.09618  -0.909    0.373\n\nResidual standard error: 3.445 on 22 degrees of freedom\nMultiple R-squared:  0.03618,   Adjusted R-squared:  -0.007629 \nF-statistic: 0.8259 on 1 and 22 DF,  p-value: 0.3733\n\n\n[[2]]\n\nCall:\nlm(formula = peak_dur_days ~ year, data = v)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.5892 -0.5837  0.2901  1.2242  1.6083 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) 53.68286   75.07369   0.715    0.482\nyear        -0.02195    0.03733  -0.588    0.563\n\nResidual standard error: 1.337 on 22 degrees of freedom\nMultiple R-squared:  0.01547,   Adjusted R-squared:  -0.02928 \nF-statistic: 0.3457 on 1 and 22 DF,  p-value: 0.5625",
    "crumbs": [
      "Workflow",
      "Analysis"
    ]
  },
  {
    "objectID": "03_analysis.html#number-of-birds-in-kettles",
    "href": "03_analysis.html#number-of-birds-in-kettles",
    "title": "Analysis",
    "section": "Number of birds in kettles",
    "text": "Number of birds in kettles\nWhat is the maximum kettle count? Has this changed?\n\nLook for changes in the maximum predicted count and in the maximum observed count\nvariables mig_pop_max, mig_raw_max\nIn both cases, these values represent the maximum population counts for the migration period (25%-75%), but also correspond the the maximum population count overall.\n\n\nDescriptive stats\n\n\nCode\nv |&gt; \n  select(\"mig_pop_max\", \"mig_raw_max\") |&gt;\n  desc_stats()\n\n\n\n\n\n\n\n\nmeasure\nmean\nsd\nmin\nmedian\nmax\nn\n\n\n\n\nmig_pop_max\n442.38\n236.76\n141\n351\n1304\n24\n\n\nmig_raw_max\n831.00\n471.99\n310\n665\n2375\n24\n\n\n\n\n\n\n\n\nModelsFiguresModel ChecksSensitivityProblematic YearsFull Model Results\n\n\n\n\n\n\n\n\nHere we use a Negative Binomial Generalized Linear Model.\n\nThe linear regression was a bit iffy (especially for the raw counts which are more volatile), although a log transformation helped.\nHowever, since the data is technically count data (i.e. kettle counts, predicted and raw), theoretically is it Poisson distributed data.\nSo I tried Poisson models, but they were overdispersed\nTherefore, I use a Negative Binomial distribution to model the dispersion directly\nThis ends up with very similar results to log-transformed linear regression, but is a more appropriate given the type of data.\n\nHowever we can easily present log10 models if you think that’s simpler to present.\n\n\n\n\nNegative-BinomialAlternative Distributions\n\n\n\nI also ran a third NB model (m2b), which omitted some of the really large raw kettle counts (see the Figure), because these really large counts resulted in some problems in the model residual figure (see Model Checks)\nRemoving the counts resulted in better model fit, and didn’t really change the interpretations (see the results under mig_raw_max where the sample size is only 23.\nTherefore, I think I wouldn’t worry about the model checks and would keep model m2 (the results saved to file only include the first two models).\n\n\nm1 &lt;- MASS::glm.nb(mig_pop_max ~ year, data = v)\nm2 &lt;- MASS::glm.nb(mig_raw_max ~ year, data = v)\nm2b &lt;- MASS::glm.nb(mig_raw_max ~ year, data = filter(v, mig_raw_max &lt; 1500))\n\nmodels &lt;- list(m1, m2, m2b)\n\n\n\nNot used, but included for completeness\n\nx1 &lt;- lm(log10(mig_pop_max) ~ year, data = v)\nx2 &lt;- lm(log10(mig_raw_max) ~ year, data = v)\n\ny1 &lt;- glm(mig_pop_max ~ year, family = \"poisson\", data = v)\ny2 &lt;- glm(mig_raw_max ~ year, family = \"poisson\", data = v)\n\ns &lt;- simulateResiduals(x1, plot = TRUE)\ntitle(\"log10 - mig_pop_max ~ year\")\n\n\n\n\n\n\n\ns &lt;- simulateResiduals(x2, plot = TRUE)\ntitle(\"log10 - mig_raw_max ~ year\")\n\n\n\n\n\n\n\ns &lt;- simulateResiduals(y1, plot = TRUE)\ntitle(\"Poisson - mig_pop_max ~ year\")\n\n\n\n\n\n\n\ns &lt;- simulateResiduals(y2, plot = TRUE)\ntitle(\"Poisson - mig_raw_max ~ year\")\n\n\n\n\n\n\n\n\n\n\n\nTabulated Results\nAll the measures are significant. There was an increase in the maximum number of birds in kettles in both the raw counts as well as the predicted max.\n\n\nCode\nt &lt;- get_table(models)\nwrite_csv(filter(t, n != 23), \"Data/Datasets/table_pop_mig.csv\")\nfmt_table(t)\n\n\n\n\n\n\n\n\nterm\nestimate\nestimate exp\nstd error\nstatistic\np value\nn\n\n\n\n\nmig_pop_max ~ year\n\n\n(Intercept)\n−61.797\n0.000\n21.547\n−2.868\n0.004\n24\n\n\nyear\n0.034\n1.034\n0.011\n3.149\n0.002\n24\n\n\nmig_raw_max ~ year\n\n\n(Intercept)\n−76.141\n0.000\n21.342\n−3.568\n0.000\n24\n\n\nyear\n0.041\n1.042\n0.011\n3.880\n0.000\n24\n\n\n(Intercept)\n−45.812\n0.000\n21.627\n−2.118\n0.034\n22\n\n\nyear\n0.026\n1.026\n0.011\n2.422\n0.015\n22\n\n\n\n\n\n\n\n\n\n\n\nCode\npop_figs &lt;- v |&gt;\n  select(year, mig_pop_max, mig_raw_max) |&gt;\n  pivot_longer(-year, names_to = \"measure\", values_to = \"pop\")\n\nggplot(pop_figs, aes(x = year, y = pop, group = measure, colour = measure)) +\n  theme_bw() +\n  geom_point() +\n  stat_smooth(method = MASS::glm.nb) +\n  scale_colour_viridis_d(end = 0.8)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nmodel_check_figs(models)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe DHARMa model checks don’t highlight any particular outlier problems, but out of an abundance of caution, we’ll do a quick review of any high Cook’s D values.\nValues highlighted in red are less than 4/25, a standard Cook’s D cutoff (4/sample size). These indicate potential influential data points for a particular model.\n\n\nCode\nget_cooks(models[-3]) |&gt;\n  gt_cooks()\n\n\n\n\n\n\n\n\nCook's Distances\n\n\nyear\nmig_pop_max\nmig_raw_max\n\n\n\n\n1999\n0.97\n1.28\n\n\n2000\n0.17\n0.06\n\n\n2001\n0.06\n0.09\n\n\n2002\n0.02\n0.01\n\n\n2003\n0.00\n0.00\n\n\n2004\n0.13\n0.03\n\n\n2005\n0.01\n0.02\n\n\n2006\n0.00\n0.02\n\n\n2008\n0.02\n0.00\n\n\n2009\n0.00\n0.00\n\n\n2010\n0.04\n0.10\n\n\n2011\n0.00\n0.01\n\n\n2012\n0.01\n0.03\n\n\n2013\n0.02\n0.02\n\n\n2014\n0.02\n0.00\n\n\n2015\n0.02\n0.00\n\n\n2016\n0.00\n0.00\n\n\n2017\n0.00\n0.05\n\n\n2018\n0.05\n0.04\n\n\n2019\n0.04\n0.03\n\n\n2020\n0.02\n0.00\n\n\n2021\n0.00\n0.00\n\n\n2022\n0.76\n0.53\n\n\n2023\n0.00\n0.08\n\n\n\n\n\n\n\n\nCheck influential points\nNow let’s see what happens if we were to omit these years from the respective analyses.\nTables with coefficients for both original and new model are shown, with colour intensity showing strength in the pattern.\nFor the maximum population recorded during migration based on both predicted counts and raw counts, if we omit 1999, the patterns are stronger, but the same.\n\n\nCode\ncompare(m1, 1999)\n\n\n\n\n\n\n\n\nmodel\nparameter\nEstimate\nStd. Error\nz value\nPr(&gt;|z|)\n\n\n\n\nOriginal - mig_pop_max\n(Intercept)\n-61.797\n21.547\n-2.868\n0.004\n\n\nDrop 1999 - mig_pop_max\n(Intercept)\n-88.105\n19.999\n-4.405\n0.000\n\n\nOriginal - mig_pop_max\nyear\n0.034\n0.011\n3.149\n0.002\n\n\nDrop 1999 - mig_pop_max\nyear\n0.047\n0.010\n4.707\n0.000\n\n\n\n\n\n\n\nCode\ncompare(m2, 1999)\n\n\n\n\n\n\n\n\nmodel\nparameter\nEstimate\nStd. Error\nz value\nPr(&gt;|z|)\n\n\n\n\nOriginal - mig_raw_max\n(Intercept)\n-76.141\n21.342\n-3.568\n0\n\n\nDrop 1999 - mig_raw_max\n(Intercept)\n-106.053\n18.756\n-5.654\n0\n\n\nOriginal - mig_raw_max\nyear\n0.041\n0.011\n3.880\n0\n\n\nDrop 1999 - mig_raw_max\nyear\n0.056\n0.009\n6.008\n0\n\n\n\n\n\n\n\nIf we omit 2000 or 2022, the patterns are weaker, but still the same.\n\n\nCode\ncompare(m1, 2000)\n\n\n\n\n\n\n\n\nmodel\nparameter\nEstimate\nStd. Error\nz value\nPr(&gt;|z|)\n\n\n\n\nOriginal - mig_pop_max\n(Intercept)\n-61.797\n21.547\n-2.868\n0.004\n\n\nDrop 2000 - mig_pop_max\n(Intercept)\n-53.042\n21.731\n-2.441\n0.015\n\n\nOriginal - mig_pop_max\nyear\n0.034\n0.011\n3.149\n0.002\n\n\nDrop 2000 - mig_pop_max\nyear\n0.029\n0.011\n2.721\n0.006\n\n\n\n\n\n\n\nCode\ncompare(m1, 2022)\n\n\n\n\n\n\n\n\nmodel\nparameter\nEstimate\nStd. Error\nz value\nPr(&gt;|z|)\n\n\n\n\nOriginal - mig_pop_max\n(Intercept)\n-61.797\n21.547\n-2.868\n0.004\n\n\nDrop 2022 - mig_pop_max\n(Intercept)\n-40.321\n19.944\n-2.022\n0.043\n\n\nOriginal - mig_pop_max\nyear\n0.034\n0.011\n3.149\n0.002\n\n\nDrop 2022 - mig_pop_max\nyear\n0.023\n0.010\n2.322\n0.020\n\n\n\n\n\n\n\nCode\ncompare(m2, 2022)\n\n\n\n\n\n\n\n\nmodel\nparameter\nEstimate\nStd. Error\nz value\nPr(&gt;|z|)\n\n\n\n\nOriginal - mig_raw_max\n(Intercept)\n-76.141\n21.342\n-3.568\n0.000\n\n\nDrop 2022 - mig_raw_max\n(Intercept)\n-59.119\n20.655\n-2.862\n0.004\n\n\nOriginal - mig_raw_max\nyear\n0.041\n0.011\n3.880\n0.000\n\n\nDrop 2022 - mig_raw_max\nyear\n0.033\n0.010\n3.182\n0.001\n\n\n\n\n\n\n\n\n\n\n\n\n\nTherefore I wouldn’t be concerned\n\n\n\n\n\n\nIn addition to the sensitivity check using cook’s distances, here we will conduct a more targeted exploration of some specific years.\n\nGaps around the peak\nFirst, we consider removing 2011 and 2013 as they have sampling gaps around the peak of migration\n\ny &lt;- c(2011, 2013)\nm1a &lt;- update(m1, data = filter(v, !year %in% y))\nm2a &lt;- update(m2, data = filter(v, !year %in% y))\nmodels_a &lt;- list(m1a, m2a)\n\nComparing the summary tables shows only superficial differences.\nData saved to Data/Datasets/table_supplemental.csv\nOriginal\nfmt_summary(models[1:2], intercept = FALSE)\nto &lt;- get_table(models[1:2]) |&gt;\n  select(model, term, estimate, std_error, statistic, p_value) |&gt;\n  mutate(type = \"original\")\nwrite_csv(to, d, append = TRUE)\n\n\n\n\n\n\n\n\nmodel\nParameter\nEstimate\nStd. Error\nZ\nP\n\n\n\n\nmig_pop_max ~ year\nyear\n0.034\n0.011\n3.149\n0.002\n\n\nmig_raw_max ~ year\nyear\n0.041\n0.011\n3.880\n0.000\n\n\n\n\n\n\n\nYears removed\nfmt_summary(models_a, intercept = FALSE)\nta &lt;- get_table(models_a) |&gt;\n  select(model, term, estimate, std_error, statistic, p_value) |&gt;\n  mutate(type = \"gaps_removed\")\nwrite_csv(ta, d, append = TRUE)\n\n\n\n\n\n\n\n\nmodel\nParameter\nEstimate\nStd. Error\nZ\nP\n\n\n\n\nmig_pop_max ~ year\nyear\n0.034\n0.011\n3.117\n0.002\n\n\nmig_raw_max ~ year\nyear\n0.042\n0.011\n3.842\n0.000\n\n\n\n\n\n\n\n\n\nMissing the end of migration\nSecond, we consider removing 2001, 2006, 2023 as they were three years where it looks like sampling ended earlier than the end of migration.\n\ny &lt;- c(2001, 2006, 2023)\nm1b &lt;- update(m1, data = filter(v, !year %in% y))\nm2b &lt;- update(m2, data = filter(v, !year %in% y))\nmodels_b &lt;- list(m1b, m2b)\n\nComparing the summary tables shows only superficial differences.\nData saved to Data/Datasets/table_supplemental.csv\nOriginal\nfmt_summary(models[1:2], intercept = FALSE)\n\n\n\n\n\n\n\n\nmodel\nParameter\nEstimate\nStd. Error\nZ\nP\n\n\n\n\nmig_pop_max ~ year\nyear\n0.034\n0.011\n3.149\n0.002\n\n\nmig_raw_max ~ year\nyear\n0.041\n0.011\n3.880\n0.000\n\n\n\n\n\n\n\nYears removed\nfmt_summary(models_b, intercept = FALSE)\ntb &lt;- get_table(models_b) |&gt;\n  select(model, term, estimate, std_error, statistic, p_value) |&gt;\n  mutate(type = \"end_removed\")\nwrite_csv(tb, d, append = TRUE)\n\n\n\n\n\n\n\n\nmodel\nParameter\nEstimate\nStd. Error\nZ\nP\n\n\n\n\nmig_pop_max ~ year\nyear\n0.031\n0.013\n2.439\n0.015\n\n\nmig_raw_max ~ year\nyear\n0.033\n0.012\n2.811\n0.005\n\n\n\n\n\n\n\n\n\nRemove all problematic years\nOut of curiosity, let’s try removing all 5 years. This reduces our data from 25 to 19 samples, so is a substantial change and we may see non-significant results merely because we have removed too many samples.\n\ny &lt;- c(2011, 2013, 2001, 2006, 2023)\nm1c &lt;- update(m1, data = filter(v, !year %in% y))\nm2c &lt;- update(m2, data = filter(v, !year %in% y))\nmodels_c &lt;- list(m1c, m2c)\n\nComparing the summary tables shows only superficial differences.\nData saved to Data/Datasets/table_supplemental.csv\nOriginal\nfmt_summary(models[1:2], intercept = FALSE)\n\n\n\n\n\n\n\n\nmodel\nParameter\nEstimate\nStd. Error\nZ\nP\n\n\n\n\nmig_pop_max ~ year\nyear\n0.034\n0.011\n3.149\n0.002\n\n\nmig_raw_max ~ year\nyear\n0.041\n0.011\n3.880\n0.000\n\n\n\n\n\n\n\nYears removed\nfmt_summary(models_c, intercept = FALSE)\ntc &lt;- get_table(models_c) |&gt;\n  select(model, term, estimate, std_error, statistic, p_value) |&gt;\n  mutate(type = \"all_removed\")\nwrite_csv(tc, d, append = TRUE)\n\n\n\n\n\n\n\n\nmodel\nParameter\nEstimate\nStd. Error\nZ\nP\n\n\n\n\nmig_pop_max ~ year\nyear\n0.031\n0.013\n2.411\n0.016\n\n\nmig_raw_max ~ year\nyear\n0.034\n0.012\n2.790\n0.005\n\n\n\n\n\n\n\n\n\nConclusions\nI do not believe that these potentially problematic sampling years are driving the patterns we see in this data.\n\n\n\n\n\nCode\nmap(models, summary)\n\n\n[[1]]\n\nCall:\nMASS::glm.nb(formula = mig_pop_max ~ year, data = v, init.theta = 6.906339287, \n    link = log)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept) -61.79691   21.54677  -2.868  0.00413 **\nyear          0.03374    0.01071   3.149  0.00164 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(6.9063) family taken to be 1)\n\n    Null deviance: 35.662  on 23  degrees of freedom\nResidual deviance: 24.554  on 22  degrees of freedom\nAIC: 316.49\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  6.91 \n          Std. Err.:  1.98 \n\n 2 x log-likelihood:  -310.494 \n\n[[2]]\n\nCall:\nMASS::glm.nb(formula = mig_raw_max ~ year, data = v, init.theta = 6.987056274, \n    link = log)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -76.14124   21.34238  -3.568 0.000360 ***\nyear          0.04118    0.01061   3.880 0.000104 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(6.9871) family taken to be 1)\n\n    Null deviance: 41.646  on 23  degrees of freedom\nResidual deviance: 24.554  on 22  degrees of freedom\nAIC: 345.5\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  6.99 \n          Std. Err.:  1.99 \n\n 2 x log-likelihood:  -339.505 \n\n[[3]]\n\nCall:\nMASS::glm.nb(formula = mig_raw_max ~ year, data = filter(v, mig_raw_max &lt; \n    1500), init.theta = 8.726319225, link = log)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept) -45.81205   21.62744  -2.118   0.0342 *\nyear          0.02606    0.01076   2.422   0.0154 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(8.7263) family taken to be 1)\n\n    Null deviance: 28.568  on 21  degrees of freedom\nResidual deviance: 22.414  on 20  degrees of freedom\nAIC: 308.17\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  8.73 \n          Std. Err.:  2.62 \n\n 2 x log-likelihood:  -302.166",
    "crumbs": [
      "Workflow",
      "Analysis"
    ]
  },
  {
    "objectID": "03_analysis.html#number-of-residents",
    "href": "03_analysis.html#number-of-residents",
    "title": "Analysis",
    "section": "Number of residents",
    "text": "Number of residents\nHow many resident vultures are there? Has this changed?\n\nLook for changes in the median number of residents\nres_pop_median, res_raw_median\n\n\nDescriptive stats\n\n\nCode\nv |&gt; \n  select(\"res_pop_median\", \"res_raw_median\") |&gt;\n  desc_stats()\n\n\n\n\n\n\n\n\nmeasure\nmean\nsd\nmin\nmedian\nmax\nn\n\n\n\n\nres_pop_median\n5.62\n2.68\n2\n5.0\n12\n24\n\n\nres_raw_median\n4.96\n2.14\n1\n4.5\n8\n24\n\n\n\n\n\n\n\n\nModelsFiguresModel ChecksSensitivityFull Model Results\n\n\n\n\n\n\n\n\nHere we use a Poisson Generalized Linear Model.\n\nThis yields similar results to a linear regression with log transformed values.\nHowever, since the data is technically count data (i.e. kettle counts, predicted and raw), theoretically is it Poisson distributed data.\nIt is not overdispersed so we stick with Poisson (rather than Negative Binomial as we did above)\n\n\n\n\n\nPoissonAlternative distributions\n\n\n\nm1 &lt;- glm(res_pop_median ~ year, family = \"poisson\", data = v)\nm2 &lt;- glm(res_raw_median ~ year, family = \"poisson\", data = v)\n\nmodels &lt;- list(m1, m2)\n\n\n\nNot used, but included for completeness\n\nx1 &lt;- lm(log10(res_pop_median) ~ year, data = v)\nx2 &lt;- lm(log10(res_raw_median) ~ year, data = v)\n\ns &lt;- simulateResiduals(x1, plot = TRUE)\n\nWarning in newton(lsp = lsp, X = G$X, y = G$y, Eb = G$Eb, UrS = G$UrS, L = G$L,\n: Fitting terminated with step failure - check results carefully\n\ntitle(\"log10 - res_pop_median ~ year\")\n\n\n\n\n\n\n\ns &lt;- simulateResiduals(x2, plot = TRUE)\ntitle(\"log10 - res_raw_median ~ year\")\n\n\n\n\n\n\n\n\n\n\n\nTabulated Results\nAll measures are significant, there is an increase in the number of resident birds over the years.\n\n\nCode\nt &lt;- get_table(models)\nwrite_csv(t, \"Data/Datasets/table_pop_res.csv\")\nfmt_table(t)\n\n\n\n\n\n\n\n\nterm\nestimate\nestimate exp\nstd error\nstatistic\np value\nn\n\n\n\n\nres_pop_median ~ year\n\n\n(Intercept)\n−79.712\n0.000\n24.549\n−3.247\n0.001\n24\n\n\nyear\n0.040\n1.041\n0.012\n3.319\n0.001\n24\n\n\nres_raw_median ~ year\n\n\n(Intercept)\n−60.538\n0.000\n25.804\n−2.346\n0.019\n24\n\n\nyear\n0.031\n1.031\n0.013\n2.409\n0.016\n24\n\n\n\n\n\n\n\n\n\n\n\nCode\nres_figs &lt;- v |&gt;\n  select(year, res_pop_median, res_raw_median) |&gt;\n  pivot_longer(-year, names_to = \"measure\", values_to = \"pop\")\n\nggplot(res_figs, aes(x = year, y = pop, group = measure, colour = measure)) +\n  theme_bw() +\n  geom_point() +\n  geom_smooth(method = \"glm\", method.args = list(family = \"poisson\")) +\n  scale_colour_viridis_d(end = 0.8)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nmodel_check_figs(models)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe DHARMa model checks don’t highlight any particular outlier problems, but out of an abundance of caution, we’ll do a quick review of any high Cook’s D values.\nValues highlighted in red are less than 4/25, a standard Cook’s D cutoff (4/sample size). These indicate potential influential data points for a particular model.\n\n\nCode\nget_cooks(models) |&gt;\n  gt_cooks()\n\n\n\n\n\n\n\n\nCook's Distances\n\n\nyear\nres_pop_median\nres_raw_median\n\n\n\n\n1999\n0.07\n0.07\n\n\n2000\n0.04\n0.05\n\n\n2001\n0.22\n0.39\n\n\n2002\n0.00\n0.00\n\n\n2003\n0.02\n0.00\n\n\n2004\n0.00\n0.00\n\n\n2005\n0.00\n0.00\n\n\n2006\n0.02\n0.04\n\n\n2008\n0.05\n0.08\n\n\n2009\n0.02\n0.01\n\n\n2010\n0.00\n0.00\n\n\n2011\n0.01\n0.00\n\n\n2012\n0.01\n0.00\n\n\n2013\n0.00\n0.00\n\n\n2014\n0.04\n0.02\n\n\n2015\n0.01\n0.01\n\n\n2016\n0.03\n0.03\n\n\n2017\n0.02\n0.00\n\n\n2018\n0.10\n0.03\n\n\n2019\n0.00\n0.03\n\n\n2020\n0.16\n0.03\n\n\n2021\n0.01\n0.03\n\n\n2022\n0.02\n0.01\n\n\n2023\n0.05\n0.02\n\n\n\n\n\n\n\n\nCheck influential points\nNow let’s see what happens if we were to omit these years from the respective analyses.\nTables with coefficients for both original and new model are shown, with colour intensity showing strength in the pattern.\nFor the daily median number of resident birds based on both predicted counts and raw counts, if we omit 2001, the patterns are stronger, but the same.\n\n\nCode\ncompare(m1, 2001)\n\n\n\n\n\n\n\n\nmodel\nparameter\nEstimate\nStd. Error\nz value\nPr(&gt;|z|)\n\n\n\n\nOriginal - res_pop_median\n(Intercept)\n-79.712\n24.549\n-3.247\n0.001\n\n\nDrop 2001 - res_pop_median\n(Intercept)\n-94.397\n26.293\n-3.590\n0.000\n\n\nOriginal - res_pop_median\nyear\n0.040\n0.012\n3.319\n0.001\n\n\nDrop 2001 - res_pop_median\nyear\n0.048\n0.013\n3.658\n0.000\n\n\n\n\n\n\n\nCode\ncompare(m2, 2001)\n\n\n\n\n\n\n\n\nmodel\nparameter\nEstimate\nStd. Error\nz value\nPr(&gt;|z|)\n\n\n\n\nOriginal - res_raw_median\n(Intercept)\n-60.538\n25.804\n-2.346\n0.019\n\n\nDrop 2001 - res_raw_median\n(Intercept)\n-81.261\n27.918\n-2.911\n0.004\n\n\nOriginal - res_raw_median\nyear\n0.031\n0.013\n2.409\n0.016\n\n\nDrop 2001 - res_raw_median\nyear\n0.041\n0.014\n2.969\n0.003\n\n\n\n\n\n\n\nIf we omit 2020, the patterns are weaker, but still the same.\n\n\nCode\ncompare(m1, 2020)\n\n\n\n\n\n\n\n\nmodel\nparameter\nEstimate\nStd. Error\nz value\nPr(&gt;|z|)\n\n\n\n\nOriginal - res_pop_median\n(Intercept)\n-79.712\n24.549\n-3.247\n0.001\n\n\nDrop 2020 - res_pop_median\n(Intercept)\n-69.757\n25.544\n-2.731\n0.006\n\n\nOriginal - res_pop_median\nyear\n0.040\n0.012\n3.319\n0.001\n\n\nDrop 2020 - res_pop_median\nyear\n0.036\n0.013\n2.798\n0.005\n\n\n\n\n\n\n\n\n\n\n\n\n\nTherefore I wouldn’t be concerned\n\n\n\n\n\n\n\n\nCode\nmap(models, summary)\n\n\n[[1]]\n\nCall:\nglm(formula = res_pop_median ~ year, family = \"poisson\", data = v)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -79.71246   24.54876  -3.247 0.001166 ** \nyear          0.04047    0.01219   3.319 0.000903 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 28.293  on 23  degrees of freedom\nResidual deviance: 16.914  on 22  degrees of freedom\nAIC: 104.74\n\nNumber of Fisher Scoring iterations: 4\n\n\n[[2]]\n\nCall:\nglm(formula = res_raw_median ~ year, family = \"poisson\", data = v)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept) -60.53757   25.80385  -2.346    0.019 *\nyear          0.03088    0.01282   2.409    0.016 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 22.313  on 23  degrees of freedom\nResidual deviance: 16.391  on 22  degrees of freedom\nAIC: 101.33\n\nNumber of Fisher Scoring iterations: 4",
    "crumbs": [
      "Workflow",
      "Analysis"
    ]
  },
  {
    "objectID": "03_analysis.html#patterns-of-migration",
    "href": "03_analysis.html#patterns-of-migration",
    "title": "Analysis",
    "section": "Patterns of migration",
    "text": "Patterns of migration\nIs the timing of migration skewed to earlier or later in the season? Is this distribution of migration flattened and wide (low kurtosis) or peaked (high kurtosis) Are these patterns consistent over the years?\nKurtosis can be used to indicates if all the birds pass through in a relatively quick ‘clump’ (thick-tailed, lower kurtosis), or whether the migration stretches out over a longer period of time (long-tailed, higher kurtosis). Possible implications for conservation or future survey designs?\n\nLook for significant skew and kurtosis in migration (5-95%) and peak migration (25-75%)\nmig_skew, peak_skew, mig_kurt, peak_kurt\n\n\nDescriptive stats\n\n\nCode\nv |&gt; \n  select(contains(\"skew\"), contains(\"kurt\")) |&gt;\n  desc_stats()\n\n\n\n\n\n\n\n\nmeasure\nmean\nsd\nmin\nmedian\nmax\nn\n\n\n\n\nmig_skew\n0.00\n0.21\n-0.4442868\n-0.03392198\n0.5067799\n24\n\n\npeak_skew\n0.00\n0.09\n-0.1296424\n-0.02295204\n0.2880288\n24\n\n\nall_skew\n−0.06\n0.20\n-0.4654119\n-0.05317408\n0.3816406\n24\n\n\nmig_kurt\n−0.67\n0.17\n-0.9329652\n-0.71138435\n-0.1280714\n24\n\n\npeak_kurt\n−1.12\n0.03\n-1.1883554\n-1.12398983\n-1.0337500\n24\n\n\nall_kurt\n0.27\n0.58\n-0.3319748\n0.20767601\n2.5554292\n24\n\n\n\n\n\n\n\n\nModelsFiguresModel ChecksSensitivityFull Model Results\n\n\nHere we look at skew and excess kurtosis only against the intercept. This allows us to test for differences from 0.\n\nSkew of 0 is normal, but below or above are considered left- and right-skewed\nA normal distribution has an excess kurtosis of 0.\n\n\n\n\n\n\n\nSo here, we are not asking if skew or kurtosis changes over the years, just whether it is different from normal (0).\n\n\n\n\nm1 &lt;- lm(mig_skew ~ 1, data = v)\nm2 &lt;- lm(peak_skew ~ 1, data = v)\nm3 &lt;- lm(all_skew ~ 1, data = v)\nm4 &lt;- lm(mig_kurt ~ 1, data = v)\nm5 &lt;- lm(peak_kurt ~ 1, data = v)\nm6 &lt;- lm(all_kurt ~ 1, data = v)\n\nmodels &lt;- list(m1, m2, m3, m4, m5, m6)\n\nTabulated Results\nSkew is not significantly different from zero in any period.\nKurtosis is significantly different from zero in all periods.\nIn the migration and peak migration periods, Kurtosis is negative, corresponding to thick-tailed distributions, squat distributions (closer to a uniform distribution).\nHowever, in the ‘all’ period, kurtosis is actually significantly positive, corresponding to relatively more narrow, thin-tailed distributions.\n\n\nCode\nt &lt;- get_table(models)\nwrite_csv(t, \"Data/Datasets/table_skew.csv\")\nfmt_table(t)\n\n\n\n\n\n\n\n\nterm\nestimate\nstd error\nstatistic\np value\nn\n\n\n\n\nmig_skew ~ 1 (R2 = 0; R2-adj = 0)\n\n\n(Intercept)\n−0.003\n0.042\n−0.067\n0.947\n24\n\n\npeak_skew ~ 1 (R2 = 0; R2-adj = 0)\n\n\n(Intercept)\n−0.003\n0.018\n−0.166\n0.869\n24\n\n\nall_skew ~ 1 (R2 = 0; R2-adj = 0)\n\n\n(Intercept)\n−0.063\n0.041\n−1.531\n0.139\n24\n\n\nmig_kurt ~ 1 (R2 = 0; R2-adj = 0)\n\n\n(Intercept)\n−0.674\n0.034\n−19.567\n0.000\n24\n\n\npeak_kurt ~ 1 (R2 = 0; R2-adj = 0)\n\n\n(Intercept)\n−1.124\n0.006\n−174.493\n0.000\n24\n\n\nall_kurt ~ 1 (R2 = 0; R2-adj = 0)\n\n\n(Intercept)\n0.267\n0.119\n2.246\n0.035\n24\n\n\n\n\n\n\n\nThe periods\nThe “all” period is defined individually as being the dates centred on the date at the 50% percentile passage is reached and includes all dates from this date to the end of the year, and that same number of dates prior to the p50 date. See the Skewness & Kurtosis section in the last report.\nBy definition, the migration period (5-95%) and the peak migration period (25-75%) are truncated distributions, so I don’t think we can really interpret kurtosis for these subsets. With the “all” period, I have attempted to subset the data to get a good measure of the migration period only, but honestly, kurtosis depends so much on where I define the cutoffs I’m not really sure that we can accurately measure it. Looking at the figures, I’d say they’re narrower than normal (so positive kurtosis, which does align with the stats for the “all” period), but that’s just a visual estimate.\n\n\nThis is figure showing the statistics.\n\n\nCode\ndist_figs &lt;- v |&gt;\n  select(year, contains(\"skew\"), contains(\"kurt\")) |&gt;\n  pivot_longer(cols = -year, names_to = \"measure\", values_to = \"statistic\")\n\nggplot(filter(dist_figs, year != 2001), aes(x = statistic, y = measure, fill = measure)) +\n  theme_bw() +\n  geom_vline(xintercept = 0, colour = \"grey20\", linetype = \"dotted\") +\n  geom_boxplot() +\n  scale_fill_viridis_d()\n\n\n\n\n\n\n\n\n\nThis figure shows the distributions in each year over top a normal distribution, for illustration.\nI think this shows positive kurtosis. The black lines are simulated normal distributions simulated using the mean, SD and counts from the data.\n\n\nCode\nnormal &lt;- v |&gt;\n  select(year, max_doy, mig_pop_total) |&gt;\n  left_join(summarize(filter(pred, doy &gt; 240), sd = sd(rep(doy, round(count))), .by = \"year\"), \n            by = \"year\") |&gt;\n  mutate(doy = pmap(list(max_doy, sd, mig_pop_total),\n                    \\(x, y, z) as.integer(rnorm(mean = x, sd = y, n = z)))) |&gt;\n  unnest(doy) |&gt;\n  count(year, doy, name = \"count\")\n\nggplot(raw, aes(x = doy, y = count)) + \n  theme_bw() +\n  geom_smooth(data = normal, method = \"gam\", colour = \"black\", linewidth = 1.5) +\n  geom_point(size = 1, aes(colour = year), na.rm = TRUE) + \n  geom_line(data = pred, linewidth = 1, aes(group = year, colour = year), na.rm = TRUE) +\n  scale_colour_viridis_c() +\n  facet_wrap(~year, scales = \"free\")\n\n\n`geom_smooth()` using formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\nHere is a scaled view (counts are divided by the yearly standard deviation) of the patterns of migration using our GAM models from the “Calculate Metrics” step.\n\n\nCode\nraw_scaled &lt;- raw |&gt;\n  mutate(across(c(count), \\(x) scale(x, center = FALSE)),\n         .by = \"year\")\n\npred_scaled &lt;- pred |&gt;\n  mutate(across(c(count, ci99_upper, ci99_lower), \\(x) scale(x, center = FALSE)),\n         .by = \"year\")\n           \nggplot(raw_scaled, aes(x = doy, y = count)) + \n  theme_bw() +\n  geom_point(aes(colour = year), na.rm = TRUE) +\n  geom_line(data = pred_scaled, aes(colour = year, group = year), \n            size = 1, na.rm = TRUE) +\n  scale_colour_viridis_c()\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nmodel_check_figs(models)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe DHARMa model checks don’t highlight any particular outlier problems, but out of an abundance of caution, we’ll do a quick review of any high Cook’s D values.\nValues highlighted in red are less than 4/25, a standard Cook’s D cutoff (4/sample size). These indicate potential influential data points for a particular model.\n\n\nCode\nget_cooks(models) |&gt;\n  gt_cooks()\n\n\n\n\n\n\n\n\nCook's Distances\n\n\nyear\nmig_skew ~ 1\npeak_skew ~ 1\nall_skew ~ 1\nmig_kurt ~ 1\npeak_kurt ~ 1\nall_kurt ~ 1\n\n\n\n\n1999\n0.01\n0.00\n0.01\n0.00\n0.00\n0.04\n\n\n2000\n0.02\n0.03\n0.02\n0.00\n0.00\n0.00\n\n\n2001\n0.21\n0.05\n0.18\n0.17\n0.00\n0.00\n\n\n2002\n0.16\n0.47\n0.05\n0.00\n0.37\n0.01\n\n\n2003\n0.02\n0.05\n0.02\n0.03\n0.03\n0.01\n\n\n2004\n0.15\n0.09\n0.21\n0.09\n0.08\n0.00\n\n\n2005\n0.00\n0.00\n0.00\n0.00\n0.01\n0.00\n\n\n2006\n0.00\n0.00\n0.00\n0.03\n0.00\n0.02\n\n\n2008\n0.05\n0.02\n0.09\n0.00\n0.00\n0.00\n\n\n2009\n0.00\n0.00\n0.02\n0.00\n0.08\n0.70\n\n\n2010\n0.00\n0.04\n0.01\n0.01\n0.01\n0.01\n\n\n2011\n0.04\n0.09\n0.01\n0.02\n0.00\n0.05\n\n\n2012\n0.00\n0.00\n0.01\n0.01\n0.00\n0.00\n\n\n2013\n0.00\n0.00\n0.00\n0.02\n0.00\n0.04\n\n\n2014\n0.00\n0.00\n0.00\n0.00\n0.03\n0.03\n\n\n2015\n0.28\n0.10\n0.22\n0.47\n0.13\n0.00\n\n\n2016\n0.02\n0.00\n0.03\n0.00\n0.00\n0.00\n\n\n2017\n0.03\n0.03\n0.05\n0.00\n0.00\n0.03\n\n\n2018\n0.00\n0.01\n0.01\n0.11\n0.19\n0.04\n\n\n2019\n0.01\n0.01\n0.04\n0.03\n0.01\n0.01\n\n\n2020\n0.01\n0.01\n0.00\n0.00\n0.01\n0.03\n\n\n2021\n0.02\n0.01\n0.02\n0.02\n0.07\n0.00\n\n\n2022\n0.01\n0.03\n0.02\n0.01\n0.04\n0.00\n\n\n2023\n0.00\n0.01\n0.00\n0.00\n0.00\n0.02\n\n\n\n\n\n\n\n\nCheck influential points\nNow let’s see what happens if we were to omit these years from the respective analyses.\nTables with coefficients for both original and new model are shown, with colour intensity showing strength in the pattern.\nFor 2001, the patterns shift but do not change.\n\n\nCode\ncompare(m1, 2001)\n\n\n\n\n\n\n\n\nmodel\nparameter\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\nOriginal - mig_skew ~ 1\n(Intercept)\n-0.003\n0.042\n-0.067\n0.947\n\n\nDrop 2001 - mig_skew ~ 1\n(Intercept)\n0.016\n0.039\n0.418\n0.680\n\n\n\n\n\n\n\nCode\ncompare(m3, 2001)\n\n\n\n\n\n\n\n\nmodel\nparameter\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\nOriginal - all_skew ~ 1\n(Intercept)\n-0.063\n0.041\n-1.531\n0.139\n\n\nDrop 2001 - all_skew ~ 1\n(Intercept)\n-0.045\n0.039\n-1.169\n0.255\n\n\n\n\n\n\n\nCode\ncompare(m4, 2001)\n\n\n\n\n\n\n\n\nmodel\nparameter\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\nOriginal - mig_kurt ~ 1\n(Intercept)\n-0.674\n0.034\n-19.567\n0\n\n\nDrop 2001 - mig_kurt ~ 1\n(Intercept)\n-0.688\n0.033\n-21.016\n0\n\n\n\n\n\n\n\nFor 2002, the patterns are stronger.\n\n\nCode\ncompare(m2, 2002)\n\n\n\n\n\n\n\n\nmodel\nparameter\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\nOriginal - peak_skew ~ 1\n(Intercept)\n-0.003\n0.018\n-0.166\n0.869\n\n\nDrop 2002 - peak_skew ~ 1\n(Intercept)\n-0.016\n0.014\n-1.121\n0.275\n\n\n\n\n\n\n\nCode\ncompare(m5, 2002)\n\n\n\n\n\n\n\n\nmodel\nparameter\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\nOriginal - peak_kurt ~ 1\n(Intercept)\n-1.124\n0.006\n-174.493\n0\n\n\nDrop 2002 - peak_kurt ~ 1\n(Intercept)\n-1.128\n0.005\n-211.632\n0\n\n\n\n\n\n\n\nFor skew of the counts during the “all” period, if we omit 2004, the patterns are stronger, and nearly show a trend, but are still non-significant.\n\n\nCode\ncompare(m3, 2004)\n\n\n\n\n\n\n\n\nmodel\nparameter\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\nOriginal - all_skew ~ 1\n(Intercept)\n-0.063\n0.041\n-1.531\n0.139\n\n\nDrop 2004 - all_skew ~ 1\n(Intercept)\n-0.082\n0.038\n-2.147\n0.043\n\n\n\n\n\n\n\nFor kurtosis during the “all” migration period, if we omit 2009, the patterns are less strong, but the same.\n\n\nCode\ncompare(m6, 2009)\n\n\n\n\n\n\n\n\nmodel\nparameter\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\nOriginal - all_kurt ~ 1\n(Intercept)\n0.267\n0.119\n2.246\n0.035\n\n\nDrop 2009 - all_kurt ~ 1\n(Intercept)\n0.167\n0.068\n2.467\n0.022\n\n\n\n\n\n\n\nFor 2015, the patterns are stronger, but the same (i.e., not significant).\n\n\nCode\ncompare(m1, 2015)\n\n\n\n\n\n\n\n\nmodel\nparameter\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\nOriginal - mig_skew ~ 1\n(Intercept)\n-0.003\n0.042\n-0.067\n0.947\n\n\nDrop 2015 - mig_skew ~ 1\n(Intercept)\n-0.025\n0.037\n-0.667\n0.512\n\n\n\n\n\n\n\nCode\ncompare(m3, 2015)\n\n\n\n\n\n\n\n\nmodel\nparameter\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\nOriginal - all_skew ~ 1\n(Intercept)\n-0.063\n0.041\n-1.531\n0.139\n\n\nDrop 2015 - all_skew ~ 1\n(Intercept)\n-0.082\n0.038\n-2.172\n0.041\n\n\n\n\n\n\n\nCode\ncompare(m4, 2015)\n\n\n\n\n\n\n\n\nmodel\nparameter\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\nOriginal - mig_kurt ~ 1\n(Intercept)\n-0.674\n0.034\n-19.567\n0\n\n\nDrop 2015 - mig_kurt ~ 1\n(Intercept)\n-0.697\n0.026\n-26.758\n0\n\n\n\n\n\n\n\nFor kurtosis during the peak migration period, if we omit 2018, the patterns don’t really change.\n\n\nCode\ncompare(m5, 2018)\n\n\n\n\n\n\n\n\nmodel\nparameter\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\nOriginal - peak_kurt ~ 1\n(Intercept)\n-1.124\n0.006\n-174.493\n0\n\n\nDrop 2018 - peak_kurt ~ 1\n(Intercept)\n-1.121\n0.006\n-184.887\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\nTherefore I wouldn’t be concerned\n\n\n\n\n\n\n\n\nCode\nmap(models, summary)\n\n\n[[1]]\n\nCall:\nlm(formula = mig_skew ~ 1, data = v)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.44147 -0.11385 -0.03111  0.07105  0.50959 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) -0.002813   0.042126  -0.067    0.947\n\nResidual standard error: 0.2064 on 23 degrees of freedom\n\n\n[[2]]\n\nCall:\nlm(formula = peak_skew ~ 1, data = v)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.12657 -0.06225 -0.01988  0.02682  0.29110 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) -0.003073   0.018460  -0.166    0.869\n\nResidual standard error: 0.09044 on 23 degrees of freedom\n\n\n[[3]]\n\nCall:\nlm(formula = all_skew ~ 1, data = v)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.40252 -0.13270  0.00971  0.08692  0.44453 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) -0.06289    0.04108  -1.531    0.139\n\nResidual standard error: 0.2012 on 23 degrees of freedom\n\n\n[[4]]\n\nCall:\nlm(formula = mig_kurt ~ 1, data = v)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.25928 -0.10693 -0.03770  0.04197  0.54561 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.67368    0.03443  -19.57 7.79e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1687 on 23 degrees of freedom\n\n\n[[5]]\n\nCall:\nlm(formula = peak_kurt ~ 1, data = v)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.064172 -0.013194  0.000194  0.006965  0.090433 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.124183   0.006443  -174.5   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.03156 on 23 degrees of freedom\n\n\n[[6]]\n\nCall:\nlm(formula = all_kurt ~ 1, data = v)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.59881 -0.36626 -0.05916  0.15063  2.28859 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)   0.2668     0.1188   2.246   0.0346 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5821 on 23 degrees of freedom",
    "crumbs": [
      "Workflow",
      "Analysis"
    ]
  },
  {
    "objectID": "03_analysis.html#reproducibility",
    "href": "03_analysis.html#reproducibility",
    "title": "Analysis",
    "section": "Reproducibility",
    "text": "Reproducibility\n\n\n\n\n\n\nSession Info\n\n\n\n\n\n\n\nCode\ndevtools::session_info()\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.5.0 (2025-04-11)\n os       Ubuntu 24.04.2 LTS\n system   x86_64, linux-gnu\n ui       X11\n language en_CA:en_US:en\n collate  en_CA.UTF-8\n ctype    en_CA.UTF-8\n tz       America/Winnipeg\n date     2025-06-06\n pandoc   3.4 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/x86_64/ (via rmarkdown)\n quarto   1.6.39 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package       * version date (UTC) lib source\n P abind           1.4-8   2024-09-12 [?] CRAN (R 4.5.0)\n P assertr       * 3.0.1   2023-11-23 [?] CRAN (R 4.5.0)\n P backports       1.5.0   2024-05-23 [?] CRAN (R 4.5.0)\n P bit             4.6.0   2025-03-06 [?] CRAN (R 4.5.0)\n P bit64           4.6.0-1 2025-01-16 [?] CRAN (R 4.5.0)\n P boot            1.3-31  2024-08-28 [?] CRAN (R 4.4.2)\n P broom         * 1.0.8   2025-03-28 [?] CRAN (R 4.5.0)\n P cachem          1.1.0   2024-05-16 [?] CRAN (R 4.5.0)\n P car             3.1-3   2024-09-27 [?] CRAN (R 4.5.0)\n P carData         3.0-5   2022-01-06 [?] CRAN (R 4.5.0)\n P cellranger      1.1.0   2016-07-27 [?] CRAN (R 4.5.0)\n P class           7.3-23  2025-01-01 [?] CRAN (R 4.4.2)\n P classInt        0.4-11  2025-01-08 [?] CRAN (R 4.5.0)\n P cli             3.6.5   2025-04-23 [?] CRAN (R 4.5.0)\n P codetools       0.2-20  2024-03-31 [?] CRAN (R 4.4.0)\n P crayon          1.5.3   2024-06-20 [?] CRAN (R 4.5.0)\n P DBI             1.2.3   2024-06-02 [?] CRAN (R 4.5.0)\n P devtools        2.4.5   2022-10-11 [?] CRAN (R 4.5.0)\n P DHARMa        * 0.4.7   2024-10-18 [?] CRAN (R 4.5.0)\n P digest          0.6.37  2024-08-19 [?] CRAN (R 4.5.0)\n P doParallel      1.0.17  2022-02-07 [?] CRAN (R 4.5.0)\n P dplyr         * 1.1.4   2023-11-17 [?] CRAN (R 4.5.0)\n P e1071           1.7-16  2024-09-16 [?] CRAN (R 4.5.0)\n P ellipsis        0.3.2   2021-04-29 [?] CRAN (R 4.5.0)\n P emmeans         1.11.1  2025-05-04 [?] CRAN (R 4.5.0)\n P estimability    1.5.1   2024-05-12 [?] CRAN (R 4.5.0)\n P evaluate        1.0.3   2025-01-10 [?] CRAN (R 4.5.0)\n P farver          2.1.2   2024-05-13 [?] CRAN (R 4.5.0)\n P fastmap         1.2.0   2024-05-15 [?] CRAN (R 4.5.0)\n P foreach         1.5.2   2022-02-02 [?] CRAN (R 4.5.0)\n P Formula         1.2-5   2023-02-24 [?] CRAN (R 4.5.0)\n P fs              1.6.6   2025-04-12 [?] CRAN (R 4.5.0)\n P gamm4           0.2-7   2025-04-22 [?] CRAN (R 4.5.0)\n P gap             1.6     2024-08-27 [?] CRAN (R 4.5.0)\n P gap.datasets    0.0.6   2023-08-25 [?] CRAN (R 4.5.0)\n P generics        0.1.4   2025-05-09 [?] CRAN (R 4.5.0)\n P GGally          2.2.1   2024-02-14 [?] CRAN (R 4.5.0)\n P ggplot2       * 3.5.2   2025-04-09 [?] CRAN (R 4.5.0)\n P ggspatial     * 1.1.9   2023-08-17 [?] CRAN (R 4.5.0)\n P ggstats         0.9.0   2025-03-10 [?] CRAN (R 4.5.0)\n P ggthemes      * 5.1.0   2024-02-10 [?] CRAN (R 4.5.0)\n P glue            1.8.0   2024-09-30 [?] CRAN (R 4.5.0)\n P gridExtra       2.3     2017-09-09 [?] CRAN (R 4.5.0)\n P gt            * 1.0.0   2025-04-05 [?] CRAN (R 4.5.0)\n P gtable          0.3.6   2024-10-25 [?] CRAN (R 4.5.0)\n P hms             1.1.3   2023-03-21 [?] CRAN (R 4.5.0)\n P htmltools       0.5.8.1 2024-04-04 [?] CRAN (R 4.5.0)\n P htmlwidgets     1.6.4   2023-12-06 [?] CRAN (R 4.5.0)\n P httpuv          1.6.16  2025-04-16 [?] CRAN (R 4.5.0)\n P httr            1.4.7   2023-08-15 [?] CRAN (R 4.5.0)\n P iterators       1.0.14  2022-02-05 [?] CRAN (R 4.5.0)\n P jsonlite        2.0.0   2025-03-27 [?] CRAN (R 4.5.0)\n P KernSmooth      2.23-26 2025-01-01 [?] CRAN (R 4.4.2)\n P knitr           1.50    2025-03-16 [?] CRAN (R 4.5.0)\n P labeling        0.4.3   2023-08-29 [?] CRAN (R 4.5.0)\n P later           1.4.2   2025-04-08 [?] CRAN (R 4.5.0)\n P lattice         0.22-7  2025-04-02 [?] CRAN (R 4.5.0)\n P lifecycle       1.0.4   2023-11-07 [?] CRAN (R 4.5.0)\n P lme4            1.1-37  2025-03-26 [?] CRAN (R 4.5.0)\n P lubridate     * 1.9.4   2024-12-08 [?] CRAN (R 4.5.0)\n P magrittr        2.0.3   2022-03-30 [?] CRAN (R 4.5.0)\n P MASS            7.3-65  2025-02-28 [?] CRAN (R 4.4.3)\n P Matrix          1.7-3   2025-03-11 [?] CRAN (R 4.4.3)\n P matrixStats     1.5.0   2025-01-07 [?] CRAN (R 4.5.0)\n P memoise         2.0.1   2021-11-26 [?] CRAN (R 4.5.0)\n P mgcv          * 1.9-3   2025-04-04 [?] CRAN (R 4.5.0)\n P mgcViz        * 0.2.0   2025-04-11 [?] CRAN (R 4.5.0)\n P mime            0.13    2025-03-17 [?] CRAN (R 4.5.0)\n P miniUI          0.1.2   2025-04-17 [?] CRAN (R 4.5.0)\n P minqa           1.2.8   2024-08-17 [?] CRAN (R 4.5.0)\n P moments       * 0.14.1  2022-05-02 [?] CRAN (R 4.5.0)\n P mvtnorm         1.3-3   2025-01-10 [?] CRAN (R 4.5.0)\n P nlme          * 3.1-168 2025-03-31 [?] CRAN (R 4.4.3)\n P nloptr          2.2.1   2025-03-17 [?] CRAN (R 4.5.0)\n P openxlsx      * 4.2.8   2025-01-25 [?] CRAN (R 4.5.0)\n P patchwork     * 1.3.0   2024-09-16 [?] CRAN (R 4.5.0)\n P pillar          1.10.2  2025-04-05 [?] CRAN (R 4.5.0)\n P pkgbuild        1.4.8   2025-05-26 [?] CRAN (R 4.5.0)\n P pkgconfig       2.0.3   2019-09-22 [?] CRAN (R 4.5.0)\n P pkgload         1.4.0   2024-06-28 [?] CRAN (R 4.5.0)\n P plyr            1.8.9   2023-10-02 [?] CRAN (R 4.5.0)\n P profvis         0.4.0   2024-09-20 [?] CRAN (R 4.5.0)\n P promises        1.3.3   2025-05-29 [?] CRAN (R 4.5.0)\n P proxy           0.4-27  2022-06-09 [?] CRAN (R 4.5.0)\n P purrr         * 1.0.4   2025-02-05 [?] CRAN (R 4.5.0)\n P qgam          * 2.0.0   2025-04-10 [?] CRAN (R 4.5.0)\n P R6              2.6.1   2025-02-15 [?] CRAN (R 4.5.0)\n P rbibutils       2.3     2024-10-04 [?] CRAN (R 4.5.0)\n P RColorBrewer    1.1-3   2022-04-03 [?] CRAN (R 4.5.0)\n P Rcpp            1.0.14  2025-01-12 [?] CRAN (R 4.5.0)\n P Rdpack          2.6.4   2025-04-09 [?] CRAN (R 4.5.0)\n P readr         * 2.1.5   2024-01-10 [?] CRAN (R 4.5.0)\n P readxl        * 1.4.5   2025-03-07 [?] CRAN (R 4.5.0)\n P reformulas      0.4.1   2025-04-30 [?] CRAN (R 4.5.0)\n P remotes         2.5.0   2024-03-17 [?] CRAN (R 4.5.0)\n P renv            1.1.4   2025-03-20 [?] CRAN (R 4.5.0)\n P rlang           1.1.6   2025-04-11 [?] CRAN (R 4.5.0)\n P rmarkdown       2.29    2024-11-04 [?] CRAN (R 4.5.0)\n P rnaturalearth * 1.0.1   2023-12-15 [?] CRAN (R 4.5.0)\n P rstudioapi      0.17.1  2024-10-22 [?] CRAN (R 4.5.0)\n P sass            0.4.10  2025-04-11 [?] CRAN (R 4.5.0)\n P scales          1.4.0   2025-04-24 [?] CRAN (R 4.5.0)\n P sessioninfo     1.2.3   2025-02-05 [?] CRAN (R 4.5.0)\n P sf            * 1.0-21  2025-05-15 [?] CRAN (R 4.5.0)\n P shiny           1.10.0  2024-12-14 [?] CRAN (R 4.5.0)\n P stringi         1.8.7   2025-03-27 [?] CRAN (R 4.5.0)\n P stringr       * 1.5.1   2023-11-14 [?] CRAN (R 4.5.0)\n P terra           1.8-54  2025-06-01 [?] CRAN (R 4.5.0)\n P tibble          3.2.1   2023-03-20 [?] CRAN (R 4.5.0)\n P tidyr         * 1.3.1   2024-01-24 [?] CRAN (R 4.5.0)\n P tidyselect      1.2.1   2024-03-11 [?] CRAN (R 4.5.0)\n P timechange      0.3.0   2024-01-18 [?] CRAN (R 4.5.0)\n P tzdb            0.5.0   2025-03-15 [?] CRAN (R 4.5.0)\n P units           0.8-7   2025-03-11 [?] CRAN (R 4.5.0)\n P urlchecker      1.0.1   2021-11-30 [?] CRAN (R 4.5.0)\n P usethis         3.1.0   2024-11-26 [?] CRAN (R 4.5.0)\n P vctrs           0.6.5   2023-12-01 [?] CRAN (R 4.5.0)\n P viridis         0.6.5   2024-01-29 [?] CRAN (R 4.5.0)\n P viridisLite     0.4.2   2023-05-02 [?] CRAN (R 4.5.0)\n P vroom           1.6.5   2023-12-05 [?] CRAN (R 4.5.0)\n P withr           3.0.2   2024-10-28 [?] CRAN (R 4.5.0)\n P xfun            0.52    2025-04-02 [?] CRAN (R 4.5.0)\n P xml2            1.3.8   2025-03-14 [?] CRAN (R 4.5.0)\n P xtable          1.8-4   2019-04-21 [?] CRAN (R 4.5.0)\n P yaml            2.3.10  2024-07-26 [?] CRAN (R 4.5.0)\n P zip             2.3.3   2025-05-13 [?] CRAN (R 4.5.0)\n\n [1] /home/steffi/Projects/vulture_migration/renv/library/linux-ubuntu-noble/R-4.5/x86_64-pc-linux-gnu\n [2] /home/steffi/.cache/R/renv/sandbox/linux-ubuntu-noble/R-4.5/x86_64-pc-linux-gnu/9a444a72\n\n * ── Packages attached to the search path.\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────",
    "crumbs": [
      "Workflow",
      "Analysis"
    ]
  },
  {
    "objectID": "XX_citations.html",
    "href": "XX_citations.html",
    "title": "Citations",
    "section": "",
    "text": "You probably only need to cite\n\nmcgv for gam models\nDHARMa for model diagnostics\nMASS for negative binomial GLM models\npossibly the tidyverse for general data manipulation (dplyr, tidyr, ggplot2, readr, stringr)\n\nbut you should keep track of the packages used and their versions (see General Info, but note that if an workflow step was updated separately, see the most up-to-date package versions under that pages “General Info”).\nNote: There are several papers for citing mgcv, but they recommend the 2017 book for an overview, which I think makes the most sense.\n\n\nWood SN (2017). Generalized Additive Models: An Introduction with R, 2 edition. Chapman and Hall/CRC.\n\n\n\nHartig F (2024). DHARMa: Residual Diagnostics for Hierarchical (Multi-Level / Mixed) Regression Models. doi:10.32614/CRAN.package.DHARMa https://doi.org/10.32614/CRAN.package.DHARMa, R package version 0.4.7, https://CRAN.R-project.org/package=DHARMa.\n\n\n\nVenables WN, Ripley BD (2002). Modern Applied Statistics with S, Fourth edition. Springer, New York. ISBN 0-387-95457-0, https://www.stats.ox.ac.uk/pub/MASS4/.\n\n\n\nWickham H, Averick M, Bryan J, Chang W, McGowan LD, François R, Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V, Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). “Welcome to the tidyverse.” Journal of Open Source Software, 4(43), 1686. doi:10.21105/joss.01686.",
    "crumbs": [
      "Appendicies",
      "Citations"
    ]
  },
  {
    "objectID": "XX_citations.html#reproducibility",
    "href": "XX_citations.html#reproducibility",
    "title": "Citations",
    "section": "Reproducibility",
    "text": "Reproducibility\n\n\n\n\n\n\nSession Info\n\n\n\n\n\n\ndevtools::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.5.0 (2025-04-11)\n os       Ubuntu 24.04.2 LTS\n system   x86_64, linux-gnu\n ui       X11\n language en_CA:en_US:en\n collate  en_CA.UTF-8\n ctype    en_CA.UTF-8\n tz       America/Winnipeg\n date     2025-06-06\n pandoc   3.4 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/x86_64/ (via rmarkdown)\n quarto   1.6.39 @ /usr/local/bin/quarto\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package     * version date (UTC) lib source\n P cachem        1.1.0   2024-05-16 [?] CRAN (R 4.5.0)\n P cli           3.6.5   2025-04-23 [?] CRAN (R 4.5.0)\n P devtools      2.4.5   2022-10-11 [?] CRAN (R 4.5.0)\n P digest        0.6.37  2024-08-19 [?] CRAN (R 4.5.0)\n P ellipsis      0.3.2   2021-04-29 [?] CRAN (R 4.5.0)\n P evaluate      1.0.3   2025-01-10 [?] CRAN (R 4.5.0)\n P fastmap       1.2.0   2024-05-15 [?] CRAN (R 4.5.0)\n P fs            1.6.6   2025-04-12 [?] CRAN (R 4.5.0)\n P glue          1.8.0   2024-09-30 [?] CRAN (R 4.5.0)\n P htmltools     0.5.8.1 2024-04-04 [?] CRAN (R 4.5.0)\n P htmlwidgets   1.6.4   2023-12-06 [?] CRAN (R 4.5.0)\n P httpuv        1.6.16  2025-04-16 [?] CRAN (R 4.5.0)\n P jsonlite      2.0.0   2025-03-27 [?] CRAN (R 4.5.0)\n P knitr         1.50    2025-03-16 [?] CRAN (R 4.5.0)\n P later         1.4.2   2025-04-08 [?] CRAN (R 4.5.0)\n P lifecycle     1.0.4   2023-11-07 [?] CRAN (R 4.5.0)\n P magrittr      2.0.3   2022-03-30 [?] CRAN (R 4.5.0)\n P memoise       2.0.1   2021-11-26 [?] CRAN (R 4.5.0)\n P mime          0.13    2025-03-17 [?] CRAN (R 4.5.0)\n P miniUI        0.1.2   2025-04-17 [?] CRAN (R 4.5.0)\n P pkgbuild      1.4.7   2025-03-24 [?] CRAN (R 4.5.0)\n P pkgload       1.4.0   2024-06-28 [?] CRAN (R 4.5.0)\n P profvis       0.4.0   2024-09-20 [?] CRAN (R 4.5.0)\n P promises      1.3.2   2024-11-28 [?] CRAN (R 4.5.0)\n P purrr         1.0.4   2025-02-05 [?] CRAN (R 4.5.0)\n P R6            2.6.1   2025-02-15 [?] CRAN (R 4.5.0)\n P Rcpp          1.0.14  2025-01-12 [?] CRAN (R 4.5.0)\n P remotes       2.5.0   2024-03-17 [?] CRAN (R 4.5.0)\n P renv          1.1.4   2025-03-20 [?] CRAN (R 4.5.0)\n P rlang         1.1.6   2025-04-11 [?] CRAN (R 4.5.0)\n P rmarkdown     2.29    2024-11-04 [?] CRAN (R 4.5.0)\n P rstudioapi    0.17.1  2024-10-22 [?] CRAN (R 4.5.0)\n P sessioninfo   1.2.3   2025-02-05 [?] CRAN (R 4.5.0)\n P shiny         1.10.0  2024-12-14 [?] CRAN (R 4.5.0)\n P urlchecker    1.0.1   2021-11-30 [?] CRAN (R 4.5.0)\n P usethis       3.1.0   2024-11-26 [?] CRAN (R 4.5.0)\n P vctrs         0.6.5   2023-12-01 [?] CRAN (R 4.5.0)\n P xfun          0.52    2025-04-02 [?] CRAN (R 4.5.0)\n P xtable        1.8-4   2019-04-21 [?] CRAN (R 4.5.0)\n P yaml          2.3.10  2024-07-26 [?] CRAN (R 4.5.0)\n\n [1] /home/steffi/Projects/vulture_migration/renv/library/linux-ubuntu-noble/R-4.5/x86_64-pc-linux-gnu\n [2] /home/steffi/.cache/R/renv/sandbox/linux-ubuntu-noble/R-4.5/x86_64-pc-linux-gnu/9a444a72\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────",
    "crumbs": [
      "Appendicies",
      "Citations"
    ]
  },
  {
    "objectID": "XX_setup.html",
    "href": "XX_setup.html",
    "title": "Setup",
    "section": "",
    "text": "These functions are sourced from XX_functions.R\n\n# Data manipulation\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)\nlibrary(purrr)\nlibrary(stringr)\nlibrary(lubridate) # Dates/times\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nlibrary(assertr)   # Data checks\nlibrary(broom)     # Data tidying\n\n# Data reading/saving\nlibrary(openxlsx) # Custom saving with formating\nlibrary(readxl)   # Quick reading\nlibrary(readr)    # Quick reading\n\n# Figures & Tables\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(ggthemes)\nlibrary(gt)\n\n# Spatial\nlibrary(ggspatial)\nlibrary(sf)\n\nLinking to GEOS 3.12.1, GDAL 3.8.4, PROJ 9.4.0; sf_use_s2() is TRUE\n\nlibrary(rnaturalearth)\n\n# Models & Stats\nlibrary(mgcv)    # GAM\n\nLoading required package: nlme\n\n\n\nAttaching package: 'nlme'\n\n\nThe following object is masked from 'package:dplyr':\n\n    collapse\n\n\nThis is mgcv 1.9-3. For overview type 'help(\"mgcv-package\")'.\n\nlibrary(DHARMa)  # Model checks\n\nThis is DHARMa 0.4.7. For overview type '?DHARMa'. For recent changes, type news(package = 'DHARMa')\n\nlibrary(mgcViz) # For DHARMa to use GAM\n\nLoading required package: qgam\n\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\n\nRegistered S3 method overwritten by 'mgcViz':\n  method from  \n  +.gg   GGally\n\n\n\nAttaching package: 'mgcViz'\n\n\nThe following objects are masked from 'package:stats':\n\n    qqline, qqnorm, qqplot\n\nlibrary(moments) # Skew etc.\n\n\ncalc_dates &lt;- function(x) {\n  x |&gt;\n    summarize(p05y1 = 0.05 * max(count_sum),\n              p05x = doy[count_sum &gt;= p05y1][1],\n              p05y = count[count_sum &gt;= p05y1][1],\n              \n              p25y1 = 0.25 * max(count_sum),\n              p25x = doy[count_sum &gt;= p25y1][1],\n              p25y = count[count_sum &gt;= p25y1][1],\n              \n              p50y1 = 0.5 * max(count_sum),\n              p50x = doy[count_sum &gt;= p50y1][1],\n              p50y = count[count_sum &gt;= p50y1][1],\n              \n              p75y1 = 0.75 * max(count_sum),\n              p75x = doy[count_sum &gt;= p75y1][1],\n              p75y = count[count_sum &gt;= p75y1][1],\n              \n              p95y1 = 0.95 * max(count_sum),\n              p95x = doy[count_sum &gt;= p95y1][1],\n              p95y = count[count_sum &gt;= p95y1][1]) |&gt;\n    pivot_longer(-any_of(\"year\"), names_to = c(\"perc\", \"dim\"), names_sep = 3) |&gt;\n    pivot_wider(names_from = \"dim\") |&gt;\n    rename(\"doy_passage\" = \"x\", \"count_thresh\" = \"y1\", \"count_pred\" = \"y\")\n}\n\nplot_model &lt;- function(raw, pred, final = NULL) {\n  ymax &lt;- max(c(raw$count, pred$ci99_upper), na.rm = TRUE)\n  \n  g &lt;- ggplot(data = pred, mapping = aes(x = doy, y = count)) +\n    theme_bw() +\n    geom_ribbon(aes(ymin = ci99_lower, ymax = ci99_upper), fill = \"grey50\", alpha = 0.5) +\n    geom_point(data = raw, na.rm = TRUE) +\n    geom_line() +\n    scale_x_continuous(name = \"Day of Year\", limits = c(203, 295)) +\n    labs(y = \"Count\")\n  \n  if(!is.null(final)) g &lt;- g + annotate(geom = \"text\", x = -Inf, y = +Inf, hjust = -0.1, vjust = 1.1, label = paste0(\"n days = \", final$n_dates_obs))\n  g\n}\n\nplot_cum &lt;- function(d_sum, dts) {\n  ggplot(data = d_sum, aes(x = doy, y = count_sum)) +\n    theme_bw() +\n    geom_bar(stat = \"identity\") +\n    geom_hline(yintercept = c(0.05, 0.25, 0.5, 0.75, 0.95) * max(d_sum$count_sum, na.rm = TRUE), \n               linetype = \"dotted\") +\n    geom_point(data = dts, aes(x = doy_passage, y = count_thresh), colour = \"red\", size = 3) +\n    scale_x_continuous(name = \"Day of Year\", limits = c(203, 295)) +\n    labs(y = \"Cumulative count\")\n}\n\nplot_model_explore &lt;- function(d_raw, d_pred, dts, residents, resident_date) {\n  ymax &lt;- max(c(d_raw$count, d_pred$ci99_upper), na.rm = TRUE)\n  \n  ggplot(data = d_pred, mapping = aes(x = doy, y = count)) +\n    theme_bw() +\n    annotate(geom = \"rect\", xmin = dts$doy_passage[1], xmax = dts$doy_passage[5], ymin = 0, ymax = Inf, \n             colour = NA, fill = \"red\", alpha = 0.1) +\n    annotate(geom = \"rect\", xmin = dts$doy_passage[2], xmax = dts$doy_passage[4], ymin = 0, ymax = Inf, \n             colour = NA, fill = \"red\", alpha = 0.3) +\n    annotate(geom = \"text\", \n             x = dts$doy_passage[1] * 1.015, y = ymax * 0.9,\n             label = \"5%-95%\") +\n    annotate(geom = \"text\", \n             x = dts$doy_passage[2] + (dts$doy_passage[4] - dts$doy_passage[2])/2, y = ymax * 0.9,\n             label = \"25%-75%\") +\n    geom_ribbon(aes(ymin = ci99_lower, ymax = ci99_upper), fill = \"grey50\", alpha = 0.5) +\n    geom_point(data = d_raw, na.rm = TRUE) +\n    geom_line() +\n    geom_point(data = dts, aes(x = doy_passage, y = count_pred), colour = \"red\", size = 3) +\n    annotate(geom = \"segment\", x = min(d_pred$doy), xend = resident_date, \n             y = 50, yend = 50, \n             arrow = arrow(ends = \"both\", length = unit(2, \"mm\"))) +\n    annotate(geom = \"text\", label = \"Resident Count\", fontface = \"bold\", \n             x = 220, y = max(d_raw$count, na.rm = TRUE) * 0.85,\n             hjust = \"left\", vjust = \"top\") +\n    annotate(geom = \"text\", \n             label = glue::glue_data(\n               residents, \n               \"min: {res_pop_min}\\nmax: {res_pop_max}\\nmedian: {res_pop_median}\\nmean: {res_pop_mean}\"), \n             hjust = \"left\", vjust = \"top\", x = 220, y = max(d_raw$count, na.rm = TRUE)*0.7)  +\n    scale_x_continuous(name = \"Day of Year\", limits = c(203, 295)) +\n    labs(y = \"Count\")\n}\n\nplot_cum_explore &lt;- function(d_sum, dts) {\n  ggplot(data = d_sum, aes(x = doy, y = count_sum)) +\n    theme_bw() +\n    geom_bar(stat = \"identity\") +\n    geom_hline(yintercept = c(0.05, 0.25, 0.5, 0.75, 0.95) * max(d_sum$count_sum, na.rm = TRUE), \n               linetype = \"dotted\") +\n    geom_point(data = dts, aes(x = doy_passage, y = count_thresh), colour = \"red\", size = 3) +\n    scale_x_continuous(name = \"Day of Year\", limits = c(203, 295)) +\n    labs(y = \"Cumulative count\")\n}\n\ngam_check &lt;- function(g, year) {\n  \n  if(!dir.exists(d &lt;- \"Figures/GAM checks/\")) dir.create(d, recursive = TRUE)\n  f &lt;- file.path(d, paste0(\"gam_check_\", year, \".png\"))\n  png(file = f, width = 1000, height = 400, res = 100)\n  p0 &lt;- par(mfrow = c(2,2), mar = c(2,2,2,2))\n  o &lt;- capture.output(gam.check(g, k.rep = 1000))\n  par(p0)\n  dev.off()\n  \n  t &lt;- str_split(o[12:13], \" +\", simplify = TRUE)\n  t &lt;- t[, -6]\n  t[1,1] &lt;- \"param\"\n  colnames(t) &lt;- as.vector(t[1,])\n  t &lt;- as_tibble(t)\n  t &lt;- slice(t, -1) |&gt;\n    mutate(convergence = str_subset(o, \"convergence\"),\n           full_convergence = str_detect(convergence, \"full\")) |&gt;\n    janitor::clean_names()\n  \n  list(plot = f, checks = t)\n}\n\nlm_coefs &lt;- function(x) {\n  bind_cols(model = as.character(x$call)[2], \n            tidy(x),\n            select(glance(x), n = nobs, contains(\"r.squ\"), \n                   model_f = statistic, model_p = p.value))\n}\n\nglm_coefs &lt;- function(x) {\n  bind_cols(model = as.character(x$call)[2], \n            tidy(x),\n            select(glance(x), n = nobs)) |&gt;\n    mutate(estimate_exp = exp(estimate)) |&gt;\n    relocate(estimate_exp, .after = estimate)\n}\n\nget_table &lt;- function(models) {\n  if(inherits(models[[1]], \"glm\")) f &lt;- glm_coefs else f &lt;- lm_coefs\n  map(models, f) |&gt;\n    bind_rows() |&gt;\n    rename_with(\\(x) str_replace_all(x, \"\\\\.\", \"_\")) |&gt;\n    rename_with(\\(x) str_replace(x, \"^(r_|adj_r)\", \"model_\\\\1\")) \n}\n\nfmt_table &lt;- function(t) {\n  ft &lt;- t |&gt;\n    mutate(\n      #model_p = round(model_p, 3),\n      #model_p = if_else(model_p &lt;= 0.05, paste0(\"&lt;strong&gt;\", model_p, \"&lt;/strong&gt;\"), as.character(model_p)),\n      row_group = md(paste0(\"&lt;strong&gt;\", model, \"&lt;/strong&gt;\")))\n  \n  if(\"model_r_squared\" %in% names(t)) {\n    ft &lt;- ft |&gt;\n      mutate(row_group = md(paste0(row_group, \n                                   \" &lt;small&gt;(\",\n                                   #\"&lt;br&gt;&lt;small&gt;(F = \", round(model_f, 2), \"; \",\n                                   #\"P = \", model_p, \"; \",\n                                   \"R2 = \", round(model_r_squared, 2), \"; \",\n                                   \"R2-adj = \", round(model_adj_r_squared, 2), \n                                   \")&lt;/small&gt;\")))\n  }\n  \n  ft |&gt; \n    select(-starts_with(\"model\")) |&gt;\n    group_by(row_group) |&gt;\n    rename_with(\\(x) str_replace_all(x, \"\\\\.|\\\\_\", \" \")) |&gt;\n    gt() |&gt;\n    gt_theme() |&gt;\n    tab_style(cell_text(transform = \"capitalize\", weight = \"bold\"), \n              locations = cells_column_labels()) |&gt;\n    text_transform(locations = cells_row_groups(), fn = \\(x) map(x, html)) |&gt;\n    fmt_number(columns = -1, decimals = 3) |&gt;\n    fmt_number(columns = any_of(c(\"n\", \"df\")), decimals = 0) |&gt;\n    tab_style(style = cell_text(weight = \"bold\"), \n              locations = cells_body(\n                columns = `p value`, rows = `p value` &lt;= 0.05))\n}\n\ngt_theme &lt;- function(data, ...) {\n  data |&gt;\n    tab_options(\n      table.border.top.width = px(3), \n      table.width = \"80%\",\n      column_labels.font.weight = \"bold\",\n      column_labels.border.bottom.width = px(3),\n      data_row.padding = px(7),\n      ...\n    )\n}\n\ngt_cooks &lt;- function(cooks, width = \"40%\") {\n  col &lt;- names(cooks)[-1]\n  g &lt;- gt(cooks) |&gt;\n    fmt_number(-\"year\", decimals = 2)\n  \n  for(i in col) {\n    g &lt;- tab_style(g, style = list(cell_fill(color = \"#F9E3D6\")),\n                   cells_body(columns = contains(i), \n                              rows = .data[[i]] &gt; 4/25))\n  }\n\n  g |&gt;\n    gt_theme() |&gt;\n    tab_header(\"Cook's Distances\") |&gt;\n    tab_options(table.width = width)\n}\n\n\nget_cooks &lt;- function(models) {\n  cbind(\n    year = v$year,\n    map(models, \\(x) {\n      data.frame(cooks = cooks.distance(x)) |&gt;\n        rename_with(~ str_remove(as.character(x$call)[2], \" ~ year\"))\n    }) |&gt;list_cbind()\n  )\n}\n\ncompare &lt;- function(model, y) {\n  \n  pal &lt;- RColorBrewer::brewer.pal(n = 9, \"Greens\")[-c(1,2,3,9)]\n  \n  m &lt;- paste0(c(\"Original - \", paste0(\"Drop \", y, \" - \")), \n              str_remove(as.character(model$call)[2], \" ~ year\"))\n  c &lt;- rbind(\n    as.data.frame(coef(summary(model)))|&gt;\n      tibble::rownames_to_column(\"parameter\"),\n    as.data.frame(coef(summary(update(model, data = filter(v, year != y))))) |&gt;\n      tibble::rownames_to_column(\"parameter\")\n  ) |&gt;\n    mutate(across(-\"parameter\", \\(x) round(x, digits = 3)))\n  cbind(model = sort(rep(m, nrow(c)/2), decreasing = TRUE), c) |&gt;\n    arrange(parameter) |&gt;\n    gt() |&gt;\n    gt_theme() |&gt;\n    data_color(columns = matches(\"Estimate\"),\n               rows = c(last_col() - 1, last_col()),\n               palette = pal, reverse = c$Estimate[nrow(c)] &lt; 0) |&gt;\n    data_color(columns = matches(\"Pr\"),\n               rows = c(last_col() - 1, last_col()), reverse = TRUE,\n               palette = pal)\n}\n\n\n\nmodel_check_figs &lt;- function(models) {\n  for(i in models) {\n    p0 &lt;- par(mar = c(4, 4, 4, 0))\n    t &lt;- as.character(i$call)[2]\n    simulateResiduals(i, plot = TRUE)\n    mtext(t, line = 1, at = -0.1)\n    par(p0)\n  }\n}\n\ndesc_stats &lt;- function(data) {\n  data |&gt;\n    pivot_longer(cols = everything(), names_to = \"measure\") |&gt;\n    summarize(mean = mean(value), \n              sd = sd(value),\n              min = min(value),\n              median = median(value),\n              max = max(value),\n              n = n(),\n              .by = \"measure\") |&gt;\n    gt() |&gt;\n    gt_theme() |&gt;\n    fmt_number(columns = c(mean, sd), decimals = 2)\n}\n  \nfmt_anova &lt;- function(m) {\n  car::Anova(m, type = \"III\") |&gt;\n    as_tibble(rownames = \"Parameter\") |&gt;\n    rename(\"P\" = \"Pr(&gt;F)\", \"F\" = \"F value\") |&gt;\n    gt() |&gt;\n    tab_style(style = cell_text(weight = \"bold\"), \n              locations = cells_body(\n                columns = P, rows = P &lt;= 0.05)) |&gt;\n    fmt_number(columns = -Df, decimals = 3)\n}\n\nfmt_prep &lt;- function(m) {\n  summary(m) |&gt;\n    coef() |&gt; \n    as_tibble(rownames = \"Parameter\") |&gt;\n    mutate(model = as.character(m$call)[2]) |&gt;\n    rename(\"P\" = starts_with(\"Pr\"),\n           \"T\" = any_of(\"t value\"),\n           \"Z\" = any_of(\"z value\")) |&gt;\n    relocate(model)\n}\n\nfmt_summary &lt;- function(m, intercept = TRUE) {\n  if(inherits(m, \"lm\")) {\n    t &lt;- fmt_prep(m) \n  } else {\n    t &lt;- map(m, fmt_prep) |&gt; \n      bind_rows()\n  }\n  if(!intercept) t &lt;- filter(t, !str_detect(Parameter, \"(I|i)ntercept\"))\n  t |&gt;\n    #group_by(pick(any_of(\"model\"))) |&gt;\n    gt() |&gt;\n    tab_style(style = cell_text(weight = \"bold\"), \n              locations = cells_body(\n                columns = P, rows = P &lt;= 0.05)) |&gt;\n    tab_style(style = cell_text(weight = \"bold\"), \n              locations = cells_body(columns = model)) |&gt;\n    fmt_number(decimals = 3)\n}\n\nfmt_emmeans &lt;- function(m, adjust = \"FDR\") {\n  emmeans::emtrends(m, ~ measure, var = \"year\") |&gt; \n    emmeans::test(adjust = adjust) |&gt;\n    as_tibble() |&gt;\n    rename(slope = year.trend, P = p.value, `T` = t.ratio) |&gt;\n    rename_with(tools::toTitleCase) |&gt;\n    gt() |&gt;\n    tab_style(style = cell_text(weight = \"bold\"), \n              locations = cells_body(\n                columns = P, rows = P &lt;= 0.05)) |&gt;\n    fmt_number(columns = -Df, decimals = 3) |&gt;\n    tab_footnote(paste0(adjust, \" P-value adjustment\"))\n}",
    "crumbs": [
      "Appendicies",
      "Setup"
    ]
  },
  {
    "objectID": "XX_setup.html#functions",
    "href": "XX_setup.html#functions",
    "title": "Setup",
    "section": "",
    "text": "These functions are sourced from XX_functions.R\n\n# Data manipulation\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)\nlibrary(purrr)\nlibrary(stringr)\nlibrary(lubridate) # Dates/times\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nlibrary(assertr)   # Data checks\nlibrary(broom)     # Data tidying\n\n# Data reading/saving\nlibrary(openxlsx) # Custom saving with formating\nlibrary(readxl)   # Quick reading\nlibrary(readr)    # Quick reading\n\n# Figures & Tables\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(ggthemes)\nlibrary(gt)\n\n# Spatial\nlibrary(ggspatial)\nlibrary(sf)\n\nLinking to GEOS 3.12.1, GDAL 3.8.4, PROJ 9.4.0; sf_use_s2() is TRUE\n\nlibrary(rnaturalearth)\n\n# Models & Stats\nlibrary(mgcv)    # GAM\n\nLoading required package: nlme\n\n\n\nAttaching package: 'nlme'\n\n\nThe following object is masked from 'package:dplyr':\n\n    collapse\n\n\nThis is mgcv 1.9-3. For overview type 'help(\"mgcv-package\")'.\n\nlibrary(DHARMa)  # Model checks\n\nThis is DHARMa 0.4.7. For overview type '?DHARMa'. For recent changes, type news(package = 'DHARMa')\n\nlibrary(mgcViz) # For DHARMa to use GAM\n\nLoading required package: qgam\n\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\n\nRegistered S3 method overwritten by 'mgcViz':\n  method from  \n  +.gg   GGally\n\n\n\nAttaching package: 'mgcViz'\n\n\nThe following objects are masked from 'package:stats':\n\n    qqline, qqnorm, qqplot\n\nlibrary(moments) # Skew etc.\n\n\ncalc_dates &lt;- function(x) {\n  x |&gt;\n    summarize(p05y1 = 0.05 * max(count_sum),\n              p05x = doy[count_sum &gt;= p05y1][1],\n              p05y = count[count_sum &gt;= p05y1][1],\n              \n              p25y1 = 0.25 * max(count_sum),\n              p25x = doy[count_sum &gt;= p25y1][1],\n              p25y = count[count_sum &gt;= p25y1][1],\n              \n              p50y1 = 0.5 * max(count_sum),\n              p50x = doy[count_sum &gt;= p50y1][1],\n              p50y = count[count_sum &gt;= p50y1][1],\n              \n              p75y1 = 0.75 * max(count_sum),\n              p75x = doy[count_sum &gt;= p75y1][1],\n              p75y = count[count_sum &gt;= p75y1][1],\n              \n              p95y1 = 0.95 * max(count_sum),\n              p95x = doy[count_sum &gt;= p95y1][1],\n              p95y = count[count_sum &gt;= p95y1][1]) |&gt;\n    pivot_longer(-any_of(\"year\"), names_to = c(\"perc\", \"dim\"), names_sep = 3) |&gt;\n    pivot_wider(names_from = \"dim\") |&gt;\n    rename(\"doy_passage\" = \"x\", \"count_thresh\" = \"y1\", \"count_pred\" = \"y\")\n}\n\nplot_model &lt;- function(raw, pred, final = NULL) {\n  ymax &lt;- max(c(raw$count, pred$ci99_upper), na.rm = TRUE)\n  \n  g &lt;- ggplot(data = pred, mapping = aes(x = doy, y = count)) +\n    theme_bw() +\n    geom_ribbon(aes(ymin = ci99_lower, ymax = ci99_upper), fill = \"grey50\", alpha = 0.5) +\n    geom_point(data = raw, na.rm = TRUE) +\n    geom_line() +\n    scale_x_continuous(name = \"Day of Year\", limits = c(203, 295)) +\n    labs(y = \"Count\")\n  \n  if(!is.null(final)) g &lt;- g + annotate(geom = \"text\", x = -Inf, y = +Inf, hjust = -0.1, vjust = 1.1, label = paste0(\"n days = \", final$n_dates_obs))\n  g\n}\n\nplot_cum &lt;- function(d_sum, dts) {\n  ggplot(data = d_sum, aes(x = doy, y = count_sum)) +\n    theme_bw() +\n    geom_bar(stat = \"identity\") +\n    geom_hline(yintercept = c(0.05, 0.25, 0.5, 0.75, 0.95) * max(d_sum$count_sum, na.rm = TRUE), \n               linetype = \"dotted\") +\n    geom_point(data = dts, aes(x = doy_passage, y = count_thresh), colour = \"red\", size = 3) +\n    scale_x_continuous(name = \"Day of Year\", limits = c(203, 295)) +\n    labs(y = \"Cumulative count\")\n}\n\nplot_model_explore &lt;- function(d_raw, d_pred, dts, residents, resident_date) {\n  ymax &lt;- max(c(d_raw$count, d_pred$ci99_upper), na.rm = TRUE)\n  \n  ggplot(data = d_pred, mapping = aes(x = doy, y = count)) +\n    theme_bw() +\n    annotate(geom = \"rect\", xmin = dts$doy_passage[1], xmax = dts$doy_passage[5], ymin = 0, ymax = Inf, \n             colour = NA, fill = \"red\", alpha = 0.1) +\n    annotate(geom = \"rect\", xmin = dts$doy_passage[2], xmax = dts$doy_passage[4], ymin = 0, ymax = Inf, \n             colour = NA, fill = \"red\", alpha = 0.3) +\n    annotate(geom = \"text\", \n             x = dts$doy_passage[1] * 1.015, y = ymax * 0.9,\n             label = \"5%-95%\") +\n    annotate(geom = \"text\", \n             x = dts$doy_passage[2] + (dts$doy_passage[4] - dts$doy_passage[2])/2, y = ymax * 0.9,\n             label = \"25%-75%\") +\n    geom_ribbon(aes(ymin = ci99_lower, ymax = ci99_upper), fill = \"grey50\", alpha = 0.5) +\n    geom_point(data = d_raw, na.rm = TRUE) +\n    geom_line() +\n    geom_point(data = dts, aes(x = doy_passage, y = count_pred), colour = \"red\", size = 3) +\n    annotate(geom = \"segment\", x = min(d_pred$doy), xend = resident_date, \n             y = 50, yend = 50, \n             arrow = arrow(ends = \"both\", length = unit(2, \"mm\"))) +\n    annotate(geom = \"text\", label = \"Resident Count\", fontface = \"bold\", \n             x = 220, y = max(d_raw$count, na.rm = TRUE) * 0.85,\n             hjust = \"left\", vjust = \"top\") +\n    annotate(geom = \"text\", \n             label = glue::glue_data(\n               residents, \n               \"min: {res_pop_min}\\nmax: {res_pop_max}\\nmedian: {res_pop_median}\\nmean: {res_pop_mean}\"), \n             hjust = \"left\", vjust = \"top\", x = 220, y = max(d_raw$count, na.rm = TRUE)*0.7)  +\n    scale_x_continuous(name = \"Day of Year\", limits = c(203, 295)) +\n    labs(y = \"Count\")\n}\n\nplot_cum_explore &lt;- function(d_sum, dts) {\n  ggplot(data = d_sum, aes(x = doy, y = count_sum)) +\n    theme_bw() +\n    geom_bar(stat = \"identity\") +\n    geom_hline(yintercept = c(0.05, 0.25, 0.5, 0.75, 0.95) * max(d_sum$count_sum, na.rm = TRUE), \n               linetype = \"dotted\") +\n    geom_point(data = dts, aes(x = doy_passage, y = count_thresh), colour = \"red\", size = 3) +\n    scale_x_continuous(name = \"Day of Year\", limits = c(203, 295)) +\n    labs(y = \"Cumulative count\")\n}\n\ngam_check &lt;- function(g, year) {\n  \n  if(!dir.exists(d &lt;- \"Figures/GAM checks/\")) dir.create(d, recursive = TRUE)\n  f &lt;- file.path(d, paste0(\"gam_check_\", year, \".png\"))\n  png(file = f, width = 1000, height = 400, res = 100)\n  p0 &lt;- par(mfrow = c(2,2), mar = c(2,2,2,2))\n  o &lt;- capture.output(gam.check(g, k.rep = 1000))\n  par(p0)\n  dev.off()\n  \n  t &lt;- str_split(o[12:13], \" +\", simplify = TRUE)\n  t &lt;- t[, -6]\n  t[1,1] &lt;- \"param\"\n  colnames(t) &lt;- as.vector(t[1,])\n  t &lt;- as_tibble(t)\n  t &lt;- slice(t, -1) |&gt;\n    mutate(convergence = str_subset(o, \"convergence\"),\n           full_convergence = str_detect(convergence, \"full\")) |&gt;\n    janitor::clean_names()\n  \n  list(plot = f, checks = t)\n}\n\nlm_coefs &lt;- function(x) {\n  bind_cols(model = as.character(x$call)[2], \n            tidy(x),\n            select(glance(x), n = nobs, contains(\"r.squ\"), \n                   model_f = statistic, model_p = p.value))\n}\n\nglm_coefs &lt;- function(x) {\n  bind_cols(model = as.character(x$call)[2], \n            tidy(x),\n            select(glance(x), n = nobs)) |&gt;\n    mutate(estimate_exp = exp(estimate)) |&gt;\n    relocate(estimate_exp, .after = estimate)\n}\n\nget_table &lt;- function(models) {\n  if(inherits(models[[1]], \"glm\")) f &lt;- glm_coefs else f &lt;- lm_coefs\n  map(models, f) |&gt;\n    bind_rows() |&gt;\n    rename_with(\\(x) str_replace_all(x, \"\\\\.\", \"_\")) |&gt;\n    rename_with(\\(x) str_replace(x, \"^(r_|adj_r)\", \"model_\\\\1\")) \n}\n\nfmt_table &lt;- function(t) {\n  ft &lt;- t |&gt;\n    mutate(\n      #model_p = round(model_p, 3),\n      #model_p = if_else(model_p &lt;= 0.05, paste0(\"&lt;strong&gt;\", model_p, \"&lt;/strong&gt;\"), as.character(model_p)),\n      row_group = md(paste0(\"&lt;strong&gt;\", model, \"&lt;/strong&gt;\")))\n  \n  if(\"model_r_squared\" %in% names(t)) {\n    ft &lt;- ft |&gt;\n      mutate(row_group = md(paste0(row_group, \n                                   \" &lt;small&gt;(\",\n                                   #\"&lt;br&gt;&lt;small&gt;(F = \", round(model_f, 2), \"; \",\n                                   #\"P = \", model_p, \"; \",\n                                   \"R2 = \", round(model_r_squared, 2), \"; \",\n                                   \"R2-adj = \", round(model_adj_r_squared, 2), \n                                   \")&lt;/small&gt;\")))\n  }\n  \n  ft |&gt; \n    select(-starts_with(\"model\")) |&gt;\n    group_by(row_group) |&gt;\n    rename_with(\\(x) str_replace_all(x, \"\\\\.|\\\\_\", \" \")) |&gt;\n    gt() |&gt;\n    gt_theme() |&gt;\n    tab_style(cell_text(transform = \"capitalize\", weight = \"bold\"), \n              locations = cells_column_labels()) |&gt;\n    text_transform(locations = cells_row_groups(), fn = \\(x) map(x, html)) |&gt;\n    fmt_number(columns = -1, decimals = 3) |&gt;\n    fmt_number(columns = any_of(c(\"n\", \"df\")), decimals = 0) |&gt;\n    tab_style(style = cell_text(weight = \"bold\"), \n              locations = cells_body(\n                columns = `p value`, rows = `p value` &lt;= 0.05))\n}\n\ngt_theme &lt;- function(data, ...) {\n  data |&gt;\n    tab_options(\n      table.border.top.width = px(3), \n      table.width = \"80%\",\n      column_labels.font.weight = \"bold\",\n      column_labels.border.bottom.width = px(3),\n      data_row.padding = px(7),\n      ...\n    )\n}\n\ngt_cooks &lt;- function(cooks, width = \"40%\") {\n  col &lt;- names(cooks)[-1]\n  g &lt;- gt(cooks) |&gt;\n    fmt_number(-\"year\", decimals = 2)\n  \n  for(i in col) {\n    g &lt;- tab_style(g, style = list(cell_fill(color = \"#F9E3D6\")),\n                   cells_body(columns = contains(i), \n                              rows = .data[[i]] &gt; 4/25))\n  }\n\n  g |&gt;\n    gt_theme() |&gt;\n    tab_header(\"Cook's Distances\") |&gt;\n    tab_options(table.width = width)\n}\n\n\nget_cooks &lt;- function(models) {\n  cbind(\n    year = v$year,\n    map(models, \\(x) {\n      data.frame(cooks = cooks.distance(x)) |&gt;\n        rename_with(~ str_remove(as.character(x$call)[2], \" ~ year\"))\n    }) |&gt;list_cbind()\n  )\n}\n\ncompare &lt;- function(model, y) {\n  \n  pal &lt;- RColorBrewer::brewer.pal(n = 9, \"Greens\")[-c(1,2,3,9)]\n  \n  m &lt;- paste0(c(\"Original - \", paste0(\"Drop \", y, \" - \")), \n              str_remove(as.character(model$call)[2], \" ~ year\"))\n  c &lt;- rbind(\n    as.data.frame(coef(summary(model)))|&gt;\n      tibble::rownames_to_column(\"parameter\"),\n    as.data.frame(coef(summary(update(model, data = filter(v, year != y))))) |&gt;\n      tibble::rownames_to_column(\"parameter\")\n  ) |&gt;\n    mutate(across(-\"parameter\", \\(x) round(x, digits = 3)))\n  cbind(model = sort(rep(m, nrow(c)/2), decreasing = TRUE), c) |&gt;\n    arrange(parameter) |&gt;\n    gt() |&gt;\n    gt_theme() |&gt;\n    data_color(columns = matches(\"Estimate\"),\n               rows = c(last_col() - 1, last_col()),\n               palette = pal, reverse = c$Estimate[nrow(c)] &lt; 0) |&gt;\n    data_color(columns = matches(\"Pr\"),\n               rows = c(last_col() - 1, last_col()), reverse = TRUE,\n               palette = pal)\n}\n\n\n\nmodel_check_figs &lt;- function(models) {\n  for(i in models) {\n    p0 &lt;- par(mar = c(4, 4, 4, 0))\n    t &lt;- as.character(i$call)[2]\n    simulateResiduals(i, plot = TRUE)\n    mtext(t, line = 1, at = -0.1)\n    par(p0)\n  }\n}\n\ndesc_stats &lt;- function(data) {\n  data |&gt;\n    pivot_longer(cols = everything(), names_to = \"measure\") |&gt;\n    summarize(mean = mean(value), \n              sd = sd(value),\n              min = min(value),\n              median = median(value),\n              max = max(value),\n              n = n(),\n              .by = \"measure\") |&gt;\n    gt() |&gt;\n    gt_theme() |&gt;\n    fmt_number(columns = c(mean, sd), decimals = 2)\n}\n  \nfmt_anova &lt;- function(m) {\n  car::Anova(m, type = \"III\") |&gt;\n    as_tibble(rownames = \"Parameter\") |&gt;\n    rename(\"P\" = \"Pr(&gt;F)\", \"F\" = \"F value\") |&gt;\n    gt() |&gt;\n    tab_style(style = cell_text(weight = \"bold\"), \n              locations = cells_body(\n                columns = P, rows = P &lt;= 0.05)) |&gt;\n    fmt_number(columns = -Df, decimals = 3)\n}\n\nfmt_prep &lt;- function(m) {\n  summary(m) |&gt;\n    coef() |&gt; \n    as_tibble(rownames = \"Parameter\") |&gt;\n    mutate(model = as.character(m$call)[2]) |&gt;\n    rename(\"P\" = starts_with(\"Pr\"),\n           \"T\" = any_of(\"t value\"),\n           \"Z\" = any_of(\"z value\")) |&gt;\n    relocate(model)\n}\n\nfmt_summary &lt;- function(m, intercept = TRUE) {\n  if(inherits(m, \"lm\")) {\n    t &lt;- fmt_prep(m) \n  } else {\n    t &lt;- map(m, fmt_prep) |&gt; \n      bind_rows()\n  }\n  if(!intercept) t &lt;- filter(t, !str_detect(Parameter, \"(I|i)ntercept\"))\n  t |&gt;\n    #group_by(pick(any_of(\"model\"))) |&gt;\n    gt() |&gt;\n    tab_style(style = cell_text(weight = \"bold\"), \n              locations = cells_body(\n                columns = P, rows = P &lt;= 0.05)) |&gt;\n    tab_style(style = cell_text(weight = \"bold\"), \n              locations = cells_body(columns = model)) |&gt;\n    fmt_number(decimals = 3)\n}\n\nfmt_emmeans &lt;- function(m, adjust = \"FDR\") {\n  emmeans::emtrends(m, ~ measure, var = \"year\") |&gt; \n    emmeans::test(adjust = adjust) |&gt;\n    as_tibble() |&gt;\n    rename(slope = year.trend, P = p.value, `T` = t.ratio) |&gt;\n    rename_with(tools::toTitleCase) |&gt;\n    gt() |&gt;\n    tab_style(style = cell_text(weight = \"bold\"), \n              locations = cells_body(\n                columns = P, rows = P &lt;= 0.05)) |&gt;\n    fmt_number(columns = -Df, decimals = 3) |&gt;\n    tab_footnote(paste0(adjust, \" P-value adjustment\"))\n}",
    "crumbs": [
      "Appendicies",
      "Setup"
    ]
  }
]